Summary - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/run-azure-databricks-notebooks-azure-data-factory/08-summary
Summary
1 minute

Notebooks are one of the most common ways that data engineers and data analysts implement data ingestion and processing logic in Azure Databricks. Using Azure Data Factory to run notebooks in a pipeline enables you to create data processing solutions that can be run on-demand, at scheduled intervals, or in response to a specific event.

In this module, you learned how to:

Describe how Azure Databricks notebooks can be run in a pipeline.
Create an Azure Data Factory linked service for Azure Databricks.
Use a Notebook activity in a pipeline.
Pass parameters to a notebook.




Knowledge check - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/run-azure-databricks-notebooks-azure-data-factory/07-knowledge-check
Knowledge check
3 minutes
1. 

You want to connect to an Azure Databricks workspace from Azure Data Factory. What must you define in Azure Data Factory?

 

A global parameter

A linked service

A customer managed key

2. 

You need to run a notebook in the Azure Databricks workspace referenced by a linked service. What type of activity should you add to a pipeline?

 

Notebook

Python

Jar

3. 

You need to use a parameter in a notebook. Which library should you use to define parameters with default values and get parameter values that are passed to the notebook?

 

notebook

argparse

dbutils.widget

Check your answers




Exercise - Run an Azure Databricks Notebook with Azure Data Factory - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/run-azure-databricks-notebooks-azure-data-factory/06-exercise-databricks-factory
Exercise - Run an Azure Databricks Notebook with Azure Data Factory
40 minutes

Now it's your chance to explore how to use Azure Data Factory to run a notebook in Azure Databricks for yourself.

 Note

To complete this lab, you will need an Azure subscription in which you have administrative access.

Launch the exercise and follow the instructions.




Use parameters in a notebook - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/run-azure-databricks-notebooks-azure-data-factory/05-notebook-parameters
Use parameters in a notebook
3 minutes

You can use parameters to pass variable values to a notebook from the pipeline. Parameterization enables greater flexibility than using hard-coded values in the notebook code.

Using parameters in a notebook

To define and use parameters in a notebook, use the dbutils.widgets library in your notebook code.

For example, the following Python code defines a variable named folder and assigns a default value of data:

dbutils.widgets.text("folder", "data")


To retrieve a parameter value, use the get function, like this:

folder = dbutils.widgets.get("folder")


The get function will retrieve the value for the specific parameter that was passed to the notebook. If no such parameter was passed, it will get the default value of the variable you declared previously.

Passing output values

In addition to using parameters that can be passed in to a notebook, you can pass values out to the calling application by using the notebook.exit function, as shown here:

path = "dbfs:/{0}/products.csv".format(folder)
dbutils.notebook.exit(path)

Setting parameter values in a pipeline

To pass parameter values to a Notebook activity, add each parameter to the activity's Base parameters, as shown here:

In this example, the parameter value is explicitly specified as a property of the Notebook activity. You could also define a pipeline parameter and assign its value dynamically to the Notebook activity's base parameter; adding a further level of abstraction.

 Tip

For more information about using parameters in Azure Data Factory, see How to use parameters, expressions and functions in Azure Data Factory in the Azure Data Factory documentation.




Use a Notebook activity in a pipeline - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/run-azure-databricks-notebooks-azure-data-factory/04-notebook-activity
Use a Notebook activity in a pipeline
5 minutes

After you've created a linked service in Azure Data Factory for your Azure Databricks workspace, you can use it to define the connection for a Notebook activity in a pipeline.

To use a Notebook activity, create a pipeline and from the Databricks category, add a Notebook activity to the pipeline designer surface.

Use the following properties of the Notebook activity to configure it:

Expand table
Category	Setting	Descriptions
General	Name	A unique name for the activity.
	Description	A meaningful description.
	Timeout	How long the activity should run before automatically canceling.
	Retries	How many times should Azure Data Factory try before failing.
	Retry interval	How long to wait before retrying.
	Secure input and output	Determines if input and output values are logged.
Azure Databricks	Azure Databricks linked service	The linked service for the Azure Databricks workspace containing the notebook.
Settings	Notebook path	The path to the notebook file in the Workspace.
	Base parameters	Used to pass parameters to the notebook.
	Append libraries	Required code libraries that aren't installed by default.
User properties		Custom user-defined properties.
Running a pipeline

When the pipeline containing the Notebook activity is published, you can run it by defining a trigger. You can then monitor pipeline runs in the Monitor section of Azure Data Factory Studio.




Create a linked service for Azure Databricks - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/run-azure-databricks-notebooks-azure-data-factory/03-linked-service
Create a linked service for Azure Databricks
5 minutes

To run notebooks in an Azure Databricks workspace, the Azure Data Factory pipeline must be able to connect to the workspace; which requires authentication. To enable this authenticated connection, you must perform two configuration tasks:

Generate an access token for your Azure Databricks workspace.
Create a linked service in your Azure Data Factory resource that uses the access token to connect to Azure Databricks.
Generating an access token

An access token provides an authentication method for Azure Databricks as an alternative to credentials on the form of a user name and password. You can generate access tokens for applications, specifying an expiration period after which the token must be regenerated and updated in the client applications.

To create an Access token, use the Generate new token option on the Developer tab of the User Settings page in Azure Databricks portal.

Creating a linked service

To connect to Azure Databricks from Azure Data Factory, you need to create a linked service for Azure Databricks compute. You can create a linked service in the Linked services page in the Manage section of Azure Data Factory Studio.

When you create an Azure Databricks linked service, you must specify the following configuration settings:

Expand table
Setting	Description
Name	A unique name for the linked service
Description	A meaningful description
Integration runtime	The integration runtime used to run activities in this linked service. See Integration runtime in Azure Data Factory for more details.
Azure subscription	The Azure subscription in which Azure Databricks is provisioned
Databricks workspace	The Azure Databricks workspace
Cluster	The Spark cluster on which activity code will be run. You can have Azure Databricks dynamically provision a job cluster on-demand or you can specify an existing cluster in the workspace.
Authentication type	How the linked connection will be authenticated by Azure Databricks. For example, using an access token (in which case, you need to specify the access token you generated for your workspace).
Cluster configuration	The Databricks runtime version, Python version, worker node type, and number of worker nodes for your cluster.




Understand Azure Databricks notebooks and pipelines - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/run-azure-databricks-notebooks-azure-data-factory/02-databricks-notebooks
Understand Azure Databricks notebooks and pipelines
3 minutes

In Azure Databricks, you can use notebooks to run code written in Python, Scala, SQL, and other languages to ingest and process data. Notebooks provide an interactive interface in which you can run individual code cells and use Markdown to include notes and annotations.

In many data engineering solutions, code that is written and tested interactively can later be incorporated into an automated data processing workload. On Azure, such workloads are often implemented as pipelines in Azure Data Factory, in which one or more activities are used to orchestrate a series of tasks that can be run on-demand, at scheduled intervals, or in response to an event (such as new data being loaded into a folder in a data lake). Azure Data Factory supports a Notebook activity that can be used to automate the unattended execution of a notebook in an Azure Databricks workspace.

 Note

The same Notebook activity is available in pipelines built in Azure Synapse Analytics.




Introduction - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/run-azure-databricks-notebooks-azure-data-factory/01-introduction
Introduction
1 minute

Azure Databricks enables data engineers to use code in notebooks to ingest and process data. While notebooks are designed to be used interactively, you can use them to encapsulate activities in a data ingestion or processing pipeline that is orchestrated using Azure Data Factory.

In this module, you'll learn how to:

Describe how Azure Databricks notebooks can be run in a pipeline.
Create an Azure Data Factory linked service for Azure Databricks.
Use a Notebook activity in a pipeline.
Pass parameters to a notebook.

 Note

While this module focuses on using Azure Databricks notebooks in an Azure Data Factory pipeline, the same principles and techniques apply when using an Azure Synapse Analytics pipeline.




Run Azure Databricks Notebooks with Azure Data Factory - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/run-azure-databricks-notebooks-azure-data-factory/

Run Azure Databricks Notebooks with Azure Data Factory
Module
8 Units
Feedback
Intermediate
Data Engineer
Azure Data Factory
Azure Databricks

Using pipelines in Azure Data Factory to run notebooks in Azure Databricks enables you to automate data engineering processes at cloud scale.

Learning objectives

In this module, you'll learn how to:

Describe how Azure Databricks notebooks can be run in a pipeline.
Create an Azure Data Factory linked service for Azure Databricks.
Use a Notebook activity in a pipeline.
Pass parameters to a notebook.
Add
Prerequisites

Before starting this module, you should have a basic knowledge of Azure Databricks. Consider completing the previous modules in the Data Engineering with Azure Databricks learning path before this one.

Introduction
min
Understand Azure Databricks notebooks and pipelines
min
Create a linked service for Azure Databricks
min
Use a Notebook activity in a pipeline
min
Use parameters in a notebook
min
Exercise - Run an Azure Databricks Notebook with Azure Data Factory
min
Knowledge check
min
Summary
min


Summary - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/use-sql-warehouses-azure-databricks/07-summary
Summary
1 minute

Azure Databricks SQL enables you to create a scalable relational SQL Warehouse in Azure Databricks. Data analysts can use the SQL Warehouse to query tables and create dashboards of data visualizations.

In this module, you learned how to:

Create and configure SQL Warehouses in Azure Databricks.
Create databases and tables.
Create queries and dashboards.




Knowledge check - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/use-sql-warehouses-azure-databricks/06-knowledge-check
Knowledge check
3 minutes
1. 

Which of the following workloads is best suited for Azure Databricks SQL?

 

Running Scala code in notebooks to transform data.

Querying and visualizing data in relational tables.

Training and deploying machine learning models.

2. 

Which statement should you use to create a database in a SQL warehouse?

 

CREATE VIEW

CREATE SCHEMA

CREATE GROUP

3. 

You need to share data visualizations, including charts and tables of data, with users in your organization. What should you create?

 

A table

A query

A dashboard

Check your answers




Exercise - Use a SQL Warehouse in Azure Databricks - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/use-sql-warehouses-azure-databricks/05-exercise-databricks-sql
Exercise - Use a SQL Warehouse in Azure Databricks
30 minutes

Now it's your chance to explore Azure Databricks SQL for yourself. In this exercise, you'll use a SQL Warehouse in Azure Databricks to query tables and create a dashboard.

 Note

To complete this lab, you will need an Azure subscription in which you have administrative access.

Launch the exercise and follow the instructions.




Create queries and dashboards - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/use-sql-warehouses-azure-databricks/04-queries-dashboards
Create queries and dashboards
3 minutes

Azure Databricks SQL is primarily designed for data analytics and visualization workloads. To support these workloads, users can create queries to retrieve and summarize data from tables, and dashboards to share visualizations of the data.

Queries

You can use the SQL Editor in the Azure Databricks portal to create a query based on any valid SQL SELECT statement, and then save the query with a meaningful name to be retrieved and run later.

After saving the query, you can schedule it to be run automatically at regular intervals to refresh the data, or you can open it and run it interactively.

Dashboards

Dashboards enable you to display the results of queries, either as tables of data or as graphical visualizations.

You can create multiple visualizations in a dashboard and share it with users in your organization. As with individual queries, you can schedule the dashboard to refresh is data periodically, and notify subscribers by email that new data is available.




Create databases and tables - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/use-sql-warehouses-azure-databricks/03-databases-tables
Create databases and tables
3 minutes

After creating and starting a SQL Warehouse, you can start to work with data in tables.

Database schema

All SQL Warehouses contain a default database schema named default. You can use create tables in this schema in order to analyze data. However, if you need to work with multiple tables in a relational schema, or you have multiple analytical workloads where you want to manage the data (and access to it) separately, you can create custom database schema. To create a database, use the SQL editor to run a CREATE DATABASE or CREATE SCHEMA SQL statement. These statements are equivalent, but CREATE SCHEMA is preferred, as shown in this example:

CREATE SCHEMA salesdata;


 Tip

For more information, see CREATE SCHEMA in the Azure Databricks documentation.

Tables

You can use the user interface in the Azure Databricks portal to upload delimited data, or import data from a wide range of common data sources. The imported data is stored in files in Databricks File System (DBFS) storage, and a Delta table is defined for it in the Hive metastore.

If the data files already exist in storage, or you need to define an explicit schema for the table, you can use a CREATE TABLE SQL statement. For example, the following code creates a table named salesorders in the salesdata database, based on the /data/sales/ folder in DBFS storage.

CREATE TABLE salesdata.salesorders
(
    orderid INT,
    orderdate DATE,
    customerid INT,
    ordertotal DECIMAL
)
USING DELTA
LOCATION '/data/sales/';


 Tip

For more information, see CREATE TABLE in the Azure Databricks documentation.




Get started with SQL Warehouses - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/use-sql-warehouses-azure-databricks/02-sql-warehouses
Get started with SQL Warehouses
3 minutes

SQL Warehouses (formerly known as SQL Endpoints) provide a relational database interface for data in Azure Databricks. The data is stored in files that are abstracted by Delta tables in a hive metastore, but from the perspective of the user or client application, the SQL Warehouse behaves like a relational database.

Creating a SQL Warehouse

When you create a premium-tier Azure Databricks workspace, it includes a default SQL Warehouse named Starter Warehouse, which you can use to explore sample data and get started with SQL-based data analytics in Azure Databricks. You can modify the configuration of the default SQL Warehouse to suit your needs, or you can create more SQL Warehouses in your workspace.

You can manage the SQL Warehouses in your Azure Databricks workspace by using the Azure Databricks portal in the SQL persona view.

SQL Warehouse configuration settings

When you create or configure a SQL Warehouse, you can specify the following settings:

Name: A name used to identify the SQL Warehouse.
Cluster size: Choose from a range of standard sizes to control the number and size of compute resources used to support the SQL Warehouse. Available sizes range from 2X-Small (a single worker node) to 4X-Large (256 worker nodes). For more information, see Cluster size in the Azure Databricks documentation.
Auto Stop: The amount of time the cluster will remain running when idle before being stopped. Idle clusters continue to incur charges when running.
Scaling: The minimum and maximum number of clusters used to distribute query processing.
Type: You can create a SQL Warehouse that uses serverless compute for fast, cost-effective on-demand provisioning. Alternatively, you can create a Pro or Classic SQL warehouse.

 Note

You can create a SQL Warehouse with any available size, but if you have insufficient quota for the number of cores required to support your choice in the region where Azure Databricks is provisioned, the SQL Warehouse will fail to start.




Introduction - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/use-sql-warehouses-azure-databricks/01-introduction
Introduction
1 minute

Data analysts often use SQL to query relational data and create reports and dashboards. Azure Databricks provides support for SQL-based data analytics through SQL Warehouses and the SQL persona.

In this module, you'll learn how to:

Create and configure SQL Warehouses in Azure Databricks.
Create databases and tables.
Create queries and dashboards.

 Note

SQL Warehouses and the SQL persona are available in premium-tier Azure Databricks workspaces.




Use SQL Warehouses in Azure Databricks - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/use-sql-warehouses-azure-databricks/

Use SQL Warehouses in Azure Databricks
Module
7 Units
Feedback
Intermediate
Data Engineer
Azure Databricks

Azure Databricks provides SQL Warehouses that enable data analysts to work with data using familiar relational SQL queries.

Learning objectives

In this module, you'll learn how to:

Create and configure SQL Warehouses in Azure Databricks.
Create databases and tables.
Create queries and dashboards.
Add
Prerequisites

Before starting this module, you should have a basic knowledge of Azure Databricks. Consider completing the previous modules in the Data Engineering with Azure Databricks learning path before this one.

Introduction
min
Get started with SQL Warehouses
min
Create databases and tables
min
Create queries and dashboards
min
Exercise - Use a SQL Warehouse in Azure Databricks
min
Knowledge check
min
Summary
min


Summary - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/use-delta-lake-azure-databricks/08-summary
Summary
1 minute

Delta Lake is an increasingly used technology for large-scale data analytics where you need to combine the flexibility and scalability of a data lake with the transactional consistency and structure of a relational database.

In this module, you learned how to:

Describe core features and capabilities of Delta Lake.
Create and use Delta Lake tables in Azure Databricks.
Create Spark catalog tables for Delta Lake data.
Use Delta Lake tables for streaming data.




Knowledge check - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/use-delta-lake-azure-databricks/07-knowledge-check
Knowledge check
3 minutes
1. 

Which of the following descriptions best fits Delta Lake?

 

A Spark API for exporting data from a relational database into CSV files.

A relational storage layer for Spark that supports tables based on Parquet files.

A synchronization solution that replicates data between SQL Server and Spark clusters.

2. 

You've loaded a Spark dataframe with data, that you now want to use in a Delta Lake table. What format should you use to write the dataframe to storage?

 

CSV

PARQUET

DELTA

3. 

What feature of Delta Lake enables you to retrieve data from previous versions of a table?

 

Spark Structured Streaming

Time Travel

Catalog Tables

4. 

You have a managed catalog table that contains Delta Lake data. If you drop the table, what will happen?

 

The table metadata and data files will be deleted.

The table metadata will be removed from the catalog, but the data files will remain intact.

The table metadata will remain in the catalog, but the data files will be deleted.

5. 

When using Spark Structured Streaming, a Delta Lake table can be which of the following?

 

Only a source

Only a sink

Either a source or a sink

Check your answers




Exercise - Use Delta Lake in Azure Databricks - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/use-delta-lake-azure-databricks/06-exercise-use-delta-lake
Exercise - Use Delta Lake in Azure Databricks
40 minutes

Now it's your chance to explore Delta Lake for yourself. In this exercise, you'll use a Spark cluster in Azure Databricks to create and query Delta Lake tables.

 Note

To complete this lab, you will need an Azure subscription in which you have administrative access.

Launch the exercise and follow the instructions.




Use Delta Lake for streaming data - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/use-delta-lake-azure-databricks/05-use-delta-lake-streaming-data
Use Delta Lake for streaming data
5 minutes

All of the data we've explored up to this point has been static data in files. However, many data analytics scenarios involve streaming data that must be processed in near real time. For example, you might need to capture readings emitted by internet-of-things (IoT) devices and store them in a table as they occur.

Spark Structured Streaming

A typical stream processing solution involves constantly reading a stream of data from a source, optionally processing it to select specific fields, aggregate and group values, or otherwise manipulate the data, and writing the results to a sink.

Spark includes native support for streaming data through Spark Structured Streaming, an API that is based on a boundless dataframe in which streaming data is captured for processing. A Spark Structured Streaming dataframe can read data from many different kinds of streaming source, including network ports, real time message brokering services such as Azure Event Hubs or Kafka, or file system locations.

 Tip

For more information about Spark Structured Streaming, see Structured Streaming Programming Guide in the Spark documentation.

Streaming with Delta Lake tables

You can use a Delta Lake table as a source or a sink for Spark Structured Streaming. For example, you could capture a stream of real time data from an IoT device and write the stream directly to a Delta Lake table as a sink - enabling you to query the table to see the latest streamed data. Or, you could read a Delta Table as a streaming source, enabling you to constantly report new data as it is added to the table.

Using a Delta Lake table as a streaming source

In the following PySpark example, a Delta Lake table is used to store details of Internet sales orders. A stream is created that reads data from the Delta Lake table folder as new data is appended.

from pyspark.sql.types import *
from pyspark.sql.functions import *

# Load a streaming dataframe from the Delta Table
stream_df = spark.readStream.format("delta") \
    .option("ignoreChanges", "true") \
    .load("/delta/internetorders")

# Now you can process the streaming data in the dataframe
# for example, show it:
stream_df.show()


 Note

When using a Delta Lake table as a streaming source, only append operations can be included in the stream. Data modifications will cause an error unless you specify the ignoreChanges or ignoreDeletes option.

After reading the data from the Delta Lake table into a streaming dataframe, you can use the Spark Structured Streaming API to process it. In the example above, the dataframe is simply displayed; but you could use Spark Structured Streaming to aggregate the data over temporal windows (for example to count the number of orders placed every minute) and send the aggregated results to a downstream process for near-real-time visualization.

Using a Delta Lake table as a streaming sink

In the following PySpark example, a stream of data is read from JSON files in a folder. The JSON data in each file contains the status for an IoT device in the format {"device":"Dev1","status":"ok"} New data is added to the stream whenever a file is added to the folder. The input stream is a boundless dataframe, which is then written in delta format to a folder location for a Delta Lake table.

from pyspark.sql.types import *
from pyspark.sql.functions import *

# Create a stream that reads JSON data from a folder
streamFolder = '/streamingdata/'
jsonSchema = StructType([
    StructField("device", StringType(), False),
    StructField("status", StringType(), False)
])
stream_df = spark.readStream.schema(jsonSchema).option("maxFilesPerTrigger", 1).json(inputPath)

# Write the stream to a delta table
table_path = '/delta/devicetable'
checkpoint_path = '/delta/checkpoint'
delta_stream = stream_df.writeStream.format("delta").option("checkpointLocation", checkpoint_path).start(table_path)


 Note

The checkpointLocation option is used to write a checkpoint file that tracks the state of the stream processing. This file enables you to recover from failure at the point where stream processing left off.

After the streaming process has started, you can query the Delta Lake table to which the streaming output is being written to see the latest data. For example, the following code creates a catalog table for the Delta Lake table folder and queries it:

%sql

CREATE TABLE DeviceTable
USING DELTA
LOCATION '/delta/devicetable';

SELECT device, status
FROM DeviceTable;


To stop the stream of data being written to the Delta Lake table, you can use the stop method of the streaming query:

delta_stream.stop()


 Tip

For more information about using Delta Lake tables for streaming data, see Table streaming reads and writes in the Delta Lake documentation.




Create and query catalog tables - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/use-delta-lake-azure-databricks/04-catalog-tables
Create and query catalog tables
5 minutes

So far we've considered Delta Lake table instances created from dataframes and modified through the Delta Lake API. You can also define Delta Lake tables as catalog tables in the Hive metastore for your Spark cluster, and work with them using SQL.

External vs managed tables

Tables in a Spark catalog, including Delta Lake tables, can be managed or external; and it's important to understand the distinction between these kinds of table.

A managed table is defined without a specified location, and the data files are stored within the storage used by the metastore. Dropping the table not only removes its metadata from the catalog, but also deletes the folder in which its data files are stored.
An external table is defined for a custom file location, where the data for the table is stored. The metadata for the table is defined in the Spark catalog. Dropping the table deletes the metadata from the catalog, but doesn't affect the data files.
Creating catalog tables

There are several ways to create catalog tables.

Creating a catalog table from a dataframe

You can create managed tables by writing a dataframe using the saveAsTable operation as shown in the following examples:

# Save a dataframe as a managed table
df.write.format("delta").saveAsTable("MyManagedTable")

## specify a path option to save as an external table
df.write.format("delta").option("path", "/mydata").saveAsTable("MyExternalTable")

Creating a catalog table using SQL

You can also create a catalog table by using the CREATE TABLE SQL statement with the USING DELTA clause, and an optional LOCATION parameter for external tables. You can run the statement using the SparkSQL API, like the following example:

spark.sql("CREATE TABLE MyExternalTable USING DELTA LOCATION '/mydata'")


Alternatively you can use the native SQL support in Spark to run the statement:

%sql

CREATE TABLE MyExternalTable
USING DELTA
LOCATION '/mydata'


 Tip

The CREATE TABLE statement returns an error if a table with the specified name already exists in the catalog. To mitigate this behavior, you can use a CREATE TABLE IF NOT EXISTS statement or the CREATE OR REPLACE TABLE statement.

Defining the table schema

In all of the examples so far, the table is created without an explicit schema. In the case of tables created by writing a dataframe, the table schema is inherited from the dataframe. When creating an external table, the schema is inherited from any files that are currently stored in the table location. However, when creating a new managed table, or an external table with a currently empty location, you define the table schema by specifying the column names, types, and nullability as part of the CREATE TABLE statement; as shown in the following example:

%sql

CREATE TABLE ManagedSalesOrders
(
    Orderid INT NOT NULL,
    OrderDate TIMESTAMP NOT NULL,
    CustomerName STRING,
    SalesTotal FLOAT NOT NULL
)
USING DELTA


When using Delta Lake, table schemas are enforced - all inserts and updates must comply with the specified column nullability and data types.

Using the DeltaTableBuilder API

You can use the DeltaTableBuilder API (part of the Delta Lake API) to create a catalog table, as shown in the following example:

from delta.tables import *

DeltaTable.create(spark) \
  .tableName("default.ManagedProducts") \
  .addColumn("Productid", "INT") \
  .addColumn("ProductName", "STRING") \
  .addColumn("Category", "STRING") \
  .addColumn("Price", "FLOAT") \
  .execute()


Similarly to the CREATE TABLE SQL statement, the create method returns an error if a table with the specified name already exists. You can mitigate this behavior by using the createIfNotExists or createOrReplace method.

Using catalog tables

You can use catalog tables like tables in any SQL-based relational database, querying and manipulating them by using standard SQL statements. For example, the following code example uses a SELECT statement to query the ManagedSalesOrders table:

%sql

SELECT orderid, salestotal
FROM ManagedSalesOrders


 Tip

For more information about working with Delta Lake, see Table batch reads and writes in the Delta Lake documentation.




Create Delta Lake tables - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/use-delta-lake-azure-databricks/03-create-delta-tables
Create Delta Lake tables
5 minutes

Delta lake is built on tables, which provide a relational storage abstraction over files in a data lake.

Creating a Delta Lake table from a dataframe

One of the easiest ways to create a Delta Lake table is to save a dataframe in the delta format, specifying a path where the data files and related metadata information for the table should be stored.

For example, the following PySpark code loads a dataframe with data from an existing file, and then saves that dataframe to a new folder location in delta format:

# Load a file into a dataframe
df = spark.read.load('/data/mydata.csv', format='csv', header=True)

# Save the dataframe as a delta table
delta_table_path = "/delta/mydata"
df.write.format("delta").save(delta_table_path)


After saving the delta table, the path location you specified includes parquet files for the data (regardless of the format of the source file you loaded into the dataframe) and a _delta_log folder containing the transaction log for the table.

 Note

The transaction log records all data modifications to the table. By logging each modification, transactional consistency can be enforced and versioning information for the table can be retained.

You can replace an existing Delta Lake table with the contents of a dataframe by using the overwrite mode, as shown here:

new_df.write.format("delta").mode("overwrite").save(delta_table_path)


You can also add rows from a dataframe to an existing table by using the append mode:

new_rows_df.write.format("delta").mode("append").save(delta_table_path)

Making conditional updates

While you can make data modifications in a dataframe and then replace a Delta Lake table by overwriting it, a more common pattern in a database is to insert, update or delete rows in an existing table as discrete transactional operations. To make such modifications to a Delta Lake table, you can use the DeltaTable object in the Delta Lake API, which supports update, delete, and merge operations. For example, you could use the following code to update the price column for all rows with a category column value of "Accessories":

from delta.tables import *
from pyspark.sql.functions import *

# Create a deltaTable object
deltaTable = DeltaTable.forPath(spark, delta_table_path)

# Update the table (reduce price of accessories by 10%)
deltaTable.update(
    condition = "Category == 'Accessories'",
    set = { "Price": "Price * 0.9" })


The data modifications are recorded in the transaction log, and new parquet files are created in the table folder as required.

 Tip

For more information about using the Data Lake API, see the Delta Lake API documentation.

Querying a previous version of a table

Delta Lake tables support versioning through the transaction log. The transaction log records modifications made to the table, noting the timestamp and version number for each transaction. You can use this logged version data to view previous versions of the table - a feature known as time travel.

You can retrieve data from a specific version of a Delta Lake table by reading the data from the delta table location into a dataframe, specifying the version required as a versionAsOf option:

df = spark.read.format("delta").option("versionAsOf", 0).load(delta_table_path)


Alternatively, you can specify a timestamp by using the timestampAsOf option:

df = spark.read.format("delta").option("timestampAsOf", '2022-01-01').load(delta_table_path)





Get Started with Delta Lake - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/use-delta-lake-azure-databricks/02-understand-delta-lake
Get Started with Delta Lake
3 minutes

Delta Lake is an open-source storage layer that adds relational database semantics to Spark-based data lake processing. Delta Lake is supported in Azure Synapse Analytics Spark pools for PySpark, Scala, and .NET code.

The benefits of using Delta Lake in Azure Databricks include:

Relational tables that support querying and data modification. With Delta Lake, you can store data in tables that support CRUD (create, read, update, and delete) operations. In other words, you can select, insert, update, and delete rows of data in the same way you would in a relational database system.
Support for ACID transactions. Relational databases are designed to support transactional data modifications that provide atomicity (transactions complete as a single unit of work), consistency (transactions leave the database in a consistent state), isolation (in-process transactions can't interfere with one another), and durability (when a transaction completes, the changes it made are persisted). Delta Lake brings this same transactional support to Spark by implementing a transaction log and enforcing serializable isolation for concurrent operations.
Data versioning and time travel. Because all transactions are logged in the transaction log, you can track multiple versions of each table row, and even use the time travel feature to retrieve a previous version of a row in a query.
Support for batch and streaming data. While most relational databases include tables that store static data, Spark includes native support for streaming data through the Spark Structured Streaming API. Delta Lake tables can be used as both sinks (destinations) and sources for streaming data.
Standard formats and interoperability. The underlying data for Delta Lake tables is stored in Parquet format, which is commonly used in data lake ingestion pipelines.

 Tip

For more information about Delta Lake in Azure Databricks, see the Delta Lake guide in the Azure Databricks documentation.




Introduction - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/use-delta-lake-azure-databricks/01-introduction
Introduction
1 minute

Linux foundation Delta Lake is an open-source storage layer for Spark that enables relational database capabilities for batch and streaming data. By using Delta Lake, you can implement a data lakehouse architecture in Spark to support SQL_based data manipulation semantics with support for transactions and schema enforcement. The result is an analytical data store that offers many of the advantages of a relational database system with the flexibility of data file storage in a data lake.

In this module, you'll learn how to:

Describe core features and capabilities of Delta Lake.
Create and use Delta Lake tables in Azure Databricks.
Create Spark catalog tables for Delta Lake data.
Use Delta Lake tables for streaming data.

 Note

The version of Delta Lake available in an Azure Databricks cluster depends on the version of the Databricks Runtime being used. The information in this module reflects Delta Lake version 1.2.1, which is installed with Databricks Runtime version 10.5 - 11.0.




Use Delta Lake in Azure Databricks - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/use-delta-lake-azure-databricks/

Use Delta Lake in Azure Databricks
Module
8 Units
Feedback
Intermediate
Data Engineer
Azure Databricks

Delta Lake is an open source relational storage area for Spark that you can use to implement a data lakehouse architecture in Azure Databricks.

Learning objectives

In this module, you'll learn how to:

Describe core features and capabilities of Delta Lake.
Create and use Delta Lake tables in Azure Databricks.
Create Spark catalog tables for Delta Lake data.
Use Delta Lake tables for streaming data.
Add
Prerequisites

Before starting this module, you should have a basic knowledge of Azure Databricks. Consider completing the previous modules in the Data Engineering with Azure Databricks learning path before this one.

Introduction
min
Get Started with Delta Lake
min
Create Delta Lake tables
min
Create and query catalog tables
min
Use Delta Lake for streaming data
min
Exercise - Use Delta Lake in Azure Databricks
min
Knowledge check
min
Summary
min


Summary - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/use-apache-spark-azure-databricks/09-summary
Summary
1 minute

Apache Spark is a key technology used in big data analytics, and the Spark support in Azure Databricks enables you to combine big data processing in Spark with large-scale data analytics.

In this module, you learned how to:

Describe key elements of the Apache Spark architecture.
Create and configure a Spark cluster.
Describe use cases for Spark.
Use Spark to process and analyze data stored in files.
Use Spark to visualize data.




Knowledge check - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/use-apache-spark-azure-databricks/08-knowledge-check
Knowledge check
3 minutes
Check your knowledge
1. 

Which definition best describes Apache Spark?

 

A highly scalable relational database management system.

A virtual server with a Python runtime.

A distributed platform for parallel data processing using multiple languages.

2. 

You need to use Spark to analyze data in a parquet file. What should you do?

 

Load the parquet file into a dataframe.

Import the data into a table in a serverless SQL pool.

Convert the data to CSV format.

3. 

You want to write code in a notebook cell that uses a SQL query to retrieve data from a view in the Spark catalog. Which magic should you use?

 

%spark

%pyspark

%sql

Check your answers




Exercise - Use Spark in Azure Databricks - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/use-apache-spark-azure-databricks/07-exercise-databricks-spark
Exercise - Use Spark in Azure Databricks
45 minutes

Now it's your opportunity to use a Spark cluster in Azure Databricks. In this exercise, you'll use a provided script to provision an Azure Databricks workspace in your Azure subscription; and then create a Spark cluster and use a notebook to analyze and visualize data from files in the databricks file system (DBFS).

 Note

To complete this lab, you will need an Azure subscription in which you have administrative access.

Launch the exercise and follow the instructions.




Visualize data - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/use-apache-spark-azure-databricks/06-visualize-data
Visualize data
6 minutes

One of the most intuitive ways to analyze the results of data queries is to visualize them as charts. Notebooks in Azure Databricks provide charting capabilities in the user interface, and when that functionality doesn't provide what you need, you can use one of the many Python graphics libraries to create and display data visualizations in the notebook.

Using built-in notebook charts

When you display a dataframe or run a SQL query in a Spark notebook in Azure Databricks, the results are displayed under the code cell. By default, results are rendered as a table, but you can also view the results as a visualization and customize how the chart displays the data, as shown here:

The built-in visualization functionality in notebooks is useful when you want to quickly summarize the data visually. When you want to have more control over how the data is formatted, or to display values that you have already aggregated in a query, you should consider using a graphics package to create your own visualizations.

Using graphics packages in code

There are many graphics packages that you can use to create data visualizations in code. In particular, Python supports a large selection of packages; most of them built on the base Matplotlib library. The output from a graphics library can be rendered in a notebook, making it easy to combine code to ingest and manipulate data with inline data visualizations and markdown cells to provide commentary.

For example, you could use the following PySpark code to aggregate data from the hypothetical products data explored previously in this module, and use Matplotlib to create a chart from the aggregated data.

from matplotlib import pyplot as plt

# Get the data as a Pandas dataframe
data = spark.sql("SELECT Category, COUNT(ProductID) AS ProductCount \
                  FROM products \
                  GROUP BY Category \
                  ORDER BY Category").toPandas()

# Clear the plot area
plt.clf()

# Create a Figure
fig = plt.figure(figsize=(12,8))

# Create a bar plot of product counts by category
plt.bar(x=data['Category'], height=data['ProductCount'], color='orange')

# Customize the chart
plt.title('Product Counts by Category')
plt.xlabel('Category')
plt.ylabel('Products')
plt.grid(color='#95a5a6', linestyle='--', linewidth=2, axis='y', alpha=0.7)
plt.xticks(rotation=70)

# Show the plot area
plt.show()


The Matplotlib library requires data to be in a Pandas dataframe rather than a Spark dataframe, so the toPandas method is used to convert it. The code then creates a figure with a specified size and plots a bar chart with some custom property configuration before showing the resulting plot.

The chart produced by the code would look similar to the following image:

You can use the Matplotlib library to create many kinds of chart; or if preferred, you can use other libraries such as Seaborn to create highly customized charts.

 Note

The Matplotlib and Seaborn libraries may already be installed on Databricks clusters, depending on the Databricks Runtime for the cluster. If not, or if you want to use a different library that is not already installed, you can add it to the cluster. See Cluster Libraries in the Azure Databricks documentation for details.




Use Spark to work with data files - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/use-apache-spark-azure-databricks/05-write-spark-code
Use Spark to work with data files
5 minutes

One of the benefits of using Spark is that you can write and run code in various programming languages, enabling you to use the programming skills you already have and to use the most appropriate language for a given task. The default language in a new Azure Databricks Spark notebook is PySpark - a Spark-optimized version of Python, which is commonly used by data scientists and analysts due to its strong support for data manipulation and visualization. Additionally, you can use languages such as Scala (a Java-derived language that can be used interactively) and SQL (a variant of the commonly used SQL language included in the Spark SQL library to work with relational data structures). Software engineers can also create compiled solutions that run on Spark using frameworks such as Java.

Exploring data with dataframes

Natively, Spark uses a data structure called a resilient distributed dataset (RDD); but while you can write code that works directly with RDDs, the most commonly used data structure for working with structured data in Spark is the dataframe, which is provided as part of the Spark SQL library. Dataframes in Spark are similar to those in the ubiquitous Pandas Python library, but optimized to work in Spark's distributed processing environment.

 Note

In addition to the Dataframe API, Spark SQL provides a strongly-typed Dataset API that is supported in Java and Scala. We'll focus on the Dataframe API in this module.

Loading data into a dataframe

Let's explore a hypothetical example to see how you can use a dataframe to work with data. Suppose you have the following data in a comma-delimited text file named products.csv in the data folder in your Databricks File System (DBFS) storage:

ProductID,ProductName,Category,ListPrice
771,"Mountain-100 Silver, 38",Mountain Bikes,3399.9900
772,"Mountain-100 Silver, 42",Mountain Bikes,3399.9900
773,"Mountain-100 Silver, 44",Mountain Bikes,3399.9900
...


In a Spark notebook, you could use the following PySpark code to load the data into a dataframe and display the first 10 rows:

%pyspark
df = spark.read.load('/data/products.csv',
    format='csv',
    header=True
)
display(df.limit(10))


The %pyspark line at the beginning is called a magic, and tells Spark that the language used in this cell is PySpark. Here's the equivalent Scala code for the products data example:

%spark
val df = spark.read.format("csv").option("header", "true").load("/data/products.csv")
display(df.limit(10))


The magic %spark is used to specify Scala.

 Tip

You can also select the language you want to use for each cell in the Notebook interface.

Both of the examples shown previously would produce output like this:

Expand table
ProductID	ProductName	Category	ListPrice
771	Mountain-100 Silver, 38	Mountain Bikes	3399.9900
772	Mountain-100 Silver, 42	Mountain Bikes	3399.9900
773	Mountain-100 Silver, 44	Mountain Bikes	3399.9900
...	...	...	...
Specifying a dataframe schema

In the previous example, the first row of the CSV file contained the column names, and Spark was able to infer the data type of each column from the data it contains. You can also specify an explicit schema for the data, which is useful when the column names aren't included in the data file, like this CSV example:

771,"Mountain-100 Silver, 38",Mountain Bikes,3399.9900
772,"Mountain-100 Silver, 42",Mountain Bikes,3399.9900
773,"Mountain-100 Silver, 44",Mountain Bikes,3399.9900
...


The following PySpark example shows how to specify a schema for the dataframe to be loaded from a file named product-data.csv in this format:

from pyspark.sql.types import *
from pyspark.sql.functions import *

productSchema = StructType([
    StructField("ProductID", IntegerType()),
    StructField("ProductName", StringType()),
    StructField("Category", StringType()),
    StructField("ListPrice", FloatType())
    ])

df = spark.read.load('/data/product-data.csv',
    format='csv',
    schema=productSchema,
    header=False)
display(df.limit(10))


The results would once again be similar to:

Expand table
ProductID	ProductName	Category	ListPrice
771	Mountain-100 Silver, 38	Mountain Bikes	3399.9900
772	Mountain-100 Silver, 42	Mountain Bikes	3399.9900
773	Mountain-100 Silver, 44	Mountain Bikes	3399.9900
...	...	...	...
Filtering and grouping dataframes

You can use the methods of the Dataframe class to filter, sort, group, and otherwise manipulate the data it contains. For example, the following code example uses the select method to retrieve the ProductName and ListPrice columns from the df dataframe containing product data in the previous example:

pricelist_df = df.select("ProductID", "ListPrice")


The results from this code example would look something like this:

Expand table
ProductID	ListPrice
771	3399.9900
772	3399.9900
773	3399.9900
...	...

In common with most data manipulation methods, select returns a new dataframe object.

 Tip

Selecting a subset of columns from a dataframe is a common operation, which can also be achieved by using the following shorter syntax:

pricelist_df = df["ProductID", "ListPrice"]

You can "chain" methods together to perform a series of manipulations that results in a transformed dataframe. For example, this example code chains the select and where methods to create a new dataframe containing the ProductName and ListPrice columns for products with a category of Mountain Bikes or Road Bikes:

bikes_df = df.select("ProductName", "ListPrice").where((df["Category"]=="Mountain Bikes") | (df["Category"]=="Road Bikes"))
display(bikes_df)


The results from this code example would look something like this:

Expand table
ProductName	ListPrice
Mountain-100 Silver, 38	3399.9900
Road-750 Black, 52	539.9900
...	...

To group and aggregate data, you can use the groupBy method and aggregate functions. For example, the following PySpark code counts the number of products for each category:

counts_df = df.select("ProductID", "Category").groupBy("Category").count()
display(counts_df)


The results from this code example would look something like this:

Expand table
Category	count
Headsets	3
Wheels	14
Mountain Bikes	32
...	...
Using SQL expressions in Spark

The Dataframe API is part of a Spark library named Spark SQL, which enables data analysts to use SQL expressions to query and manipulate data.

Creating database objects in the Spark catalog

The Spark catalog is a metastore for relational data objects such as views and tables. The Spark runtime can use the catalog to seamlessly integrate code written in any Spark-supported language with SQL expressions that may be more natural to some data analysts or developers.

One of the simplest ways to make data in a dataframe available for querying in the Spark catalog is to create a temporary view, as shown in the following code example:

df.createOrReplaceTempView("products")


A view is temporary, meaning that it's automatically deleted at the end of the current session. You can also create tables that are persisted in the catalog to define a database that can be queried using Spark SQL.

 Note

We won't explore Spark catalog tables in depth in this module, but it's worth taking the time to highlight a few key points:

You can create an empty table by using the spark.catalog.createTable method. Tables are metadata structures that store their underlying data in the storage location associated with the catalog. Deleting a table also deletes its underlying data.
You can save a dataframe as a table by using its saveAsTable method.
You can create an external table by using the spark.catalog.createExternalTable method. External tables define metadata in the catalog but get their underlying data from an external storage location; typically a folder in a data lake. Deleting an external table does not delete the underlying data.
Using the Spark SQL API to query data

You can use the Spark SQL API in code written in any language to query data in the catalog. For example, the following PySpark code uses a SQL query to return data from the products view as a dataframe.

bikes_df = spark.sql("SELECT ProductID, ProductName, ListPrice \
                      FROM products \
                      WHERE Category IN ('Mountain Bikes', 'Road Bikes')")
display(bikes_df)


The results from the code example would look similar to the following table:

Expand table
ProductName	ListPrice
Mountain-100 Silver, 38	3399.9900
Road-750 Black, 52	539.9900
...	...
Using SQL code

The previous example demonstrated how to use the Spark SQL API to embed SQL expressions in Spark code. In a notebook, you can also use the %sql magic to run SQL code that queries objects in the catalog, like this:

%sql

SELECT Category, COUNT(ProductID) AS ProductCount
FROM products
GROUP BY Category
ORDER BY Category


The SQL code example returns a resultset that is automatically displayed in the notebook as a table, like the one below:

Expand table
Category	ProductCount
Bib-Shorts	3
Bike Racks	1
Bike Stands	1
...	...




Use Spark in notebooks - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/use-apache-spark-azure-databricks/04-use-spark
Use Spark in notebooks
6 minutes

You can run many different kinds of application on Spark, including code in Python or Scala scripts, Java code compiled as a Java Archive (JAR), and others. Spark is commonly used in two kinds of workload:

Batch or stream processing jobs to ingest, clean, and transform data - often running as part of an automated pipeline.
Interactive analytics sessions to explore, analyze, and visualize data.
Running Spark code in notebooks

Azure Databricks includes an integrated notebook interface for working with Spark. Notebooks provide an intuitive way to combine code with Markdown notes, commonly used by data scientists and data analysts. The look and feel of the integrated notebook experience within Azure Databricks is similar to that of Jupyter notebooks - a popular open source notebook platform.

Notebooks consist of one or more cells, each containing either code or markdown. Code cells in notebooks have some features that can help you be more productive, including:

Syntax highlighting and error support.
Code auto-completion.
Interactive data visualizations.
The ability to export results.

 Tip

To learn more about working with notebooks in Azure Databricks, see the Notebooks article in the Azure Databricks documentation.




Create a Spark cluster - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/use-apache-spark-azure-databricks/03-spark-cluster
Create a Spark cluster
3 minutes

You can create one or more clusters in your Azure Databricks workspace by using the Azure Databricks portal.

When creating the cluster, you can specify configuration settings, including:

A name for the cluster.
A cluster mode, which can be:
Standard: Suitable for single-user workloads that require multiple worker nodes.
High Concurrency: Suitable for workloads where multiple users will be using the cluster concurrently.
Single Node: Suitable for small workloads or testing, where only a single worker node is required.
The version of the Databricks Runtime to be used in the cluster; which dictates the version of Spark and individual components such as Python, Scala, and others that get installed.
The type of virtual machine (VM) used for the worker nodes in the cluster.
The minimum and maximum number of worker nodes in the cluster.
The type of VM used for the driver node in the cluster.
Whether the cluster supports autoscaling to dynamically resize the cluster.
How long the cluster can remain idle before being shut down automatically.
How Azure manages cluster resources

When you create an Azure Databricks workspace, a Databricks appliance is deployed as an Azure resource in your subscription. When you create a cluster in the workspace, you specify the types and sizes of the virtual machines (VMs) to use for both the driver and worker nodes, and some other configuration options, but Azure Databricks manages all other aspects of the cluster.

The Databricks appliance is deployed into Azure as a managed resource group within your subscription. This resource group contains the driver and worker VMs for your clusters, along with other required resources, including a virtual network, a security group, and a storage account. All metadata for your cluster, such as scheduled jobs, is stored in an Azure Database with geo-replication for fault tolerance.

Internally, Azure Kubernetes Service (AKS) is used to run the Azure Databricks control-plane and data-planes via containers running on the latest generation of Azure hardware (Dv3 VMs), with NvMe SSDs capable of blazing 100us latency on high-performance Azure virtual machines with accelerated networking. Azure Databricks utilizes these features of Azure to further improve Spark performance. After the services within your managed resource group are ready, you can manage the Databricks cluster through the Azure Databricks UI and through features such as auto-scaling and auto-termination.

 Note

You also have the option of attaching your cluster to a pool of idle nodes to reduce cluster startup time. For more information, see Pools in the Azure Databricks documentation.




Get to know Spark - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/use-apache-spark-azure-databricks/02-understand-spark
Get to know Spark
4 minutes

To gain a better understanding of how to process and analyze data with Apache Spark in Azure Databricks, it's important to understand the underlying architecture.

High-level overview

From a high level, the Azure Databricks service launches and manages Apache Spark clusters within your Azure subscription. Apache Spark clusters are groups of computers that are treated as a single computer and handle the execution of commands issued from notebooks. Clusters enable processing of data to be parallelized across many computers to improve scale and performance. They consist of a Spark driver and worker nodes. The driver node sends work to the worker nodes and instructs them to pull data from a specified data source.

In Databricks, the notebook interface is typically the driver program. This driver program contains the main loop for the program and creates distributed datasets on the cluster, then applies operations to those datasets. Driver programs access Apache Spark through a SparkSession object regardless of deployment location.

Microsoft Azure manages the cluster, and auto-scales it as needed based on your usage and the setting used when configuring the cluster. Auto-termination can also be enabled, which allows Azure to terminate the cluster after a specified number of minutes of inactivity.

Spark jobs in detail

Work submitted to the cluster is split into as many independent jobs as needed. This is how work is distributed across the Cluster's nodes. Jobs are further subdivided into tasks. The input to a job is partitioned into one or more partitions. These partitions are the unit of work for each slot. In between tasks, partitions may need to be reorganized and shared over the network.

The secret to Spark's high performance is parallelism. Scaling vertically (by adding resources to a single computer) is limited to a finite amount of RAM, Threads and CPU speeds; but clusters scale horizontally, adding new nodes to the cluster as needed.

Spark parallelizes jobs at two levels:

The first level of parallelization is the executor - a Java virtual machine (JVM) running on a worker node, typically, one instance per node.
The second level of parallelization is the slot - the number of which is determined by the number of cores and CPUs of each node.
Each executor has multiple slots to which parallelized tasks can be assigned.

The JVM is naturally multi-threaded, but a single JVM, such as the one coordinating the work on the driver, has a finite upper limit. By splitting the work into tasks, the driver can assign units of work to *slots in the executors on worker nodes for parallel execution. Additionally, the driver determines how to partition the data so that it can be distributed for parallel processing. So, the driver assigns a partition of data to each task so that each task knows which piece of data it is to process. Once started, each task will fetch the partition of data assigned to it.

Jobs and stages

Depending on the work being performed, multiple parallelized jobs may be required. Each job is broken down into stages. A useful analogy is to imagine that the job is to build a house:

The first stage would be to lay the foundation.
The second stage would be to erect the walls.
The third stage would be to add the roof.

Attempting to do any of these steps out of order just doesn't make sense, and may in fact be impossible. Similarly, Spark breaks each job into stages to ensure everything is done in the right order.




Introduction - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/use-apache-spark-azure-databricks/01-introduction
Introduction
1 minute

Azure Databricks offers a highly scalable platform for data analytics and processing using Apache Spark.

Spark is a flexible platform that supports many different programming languages and APIs. Most data processing and analytics tasks can be accomplished using the Dataframe API, which is what we'll focus on in this module.

In this module, you'll learn how to:

Describe key elements of the Apache Spark architecture.
Create and configure a Spark cluster.
Describe use cases for Spark.
Use Spark to process and analyze data stored in files.
Use Spark to visualize data.




Use Apache Spark in Azure Databricks - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/use-apache-spark-azure-databricks/

Use Apache Spark in Azure Databricks
Module
9 Units
Feedback
Intermediate
Data Engineer
Azure Databricks

Azure Databricks is built on Apache Spark and enables data engineers and analysts to run Spark jobs to transform, analyze and visualize data at scale.

Learning objectives

In this module, you'll learn how to:

Describe key elements of the Apache Spark architecture.
Create and configure a Spark cluster.
Describe use cases for Spark.
Use Spark to process and analyze data stored in files.
Use Spark to visualize data.
Add
Prerequisites

Before starting this module, you should have a basic knowledge of Azure Databricks. Consider completing the previous modules in the Data Engineering with Azure Databricks learning path before this one.

Introduction
min
Get to know Spark
min
Create a Spark cluster
min
Use Spark in notebooks
min
Use Spark to work with data files
min
Visualize data
min
Exercise - Use Spark in Azure Databricks
min
Knowledge check
min
Summary
min


Summary - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/explore-azure-databricks/07-summary
Summary
1 minute

Azure Databricks is a scalable platform for data analytics in Microsoft Azure. You can use Azure Databricks to build highly scalable solutions for data science and engineering, machine learning, and SQL-based data analytics.

In this module, you learned how to:

Provision an Azure Databricks workspace.
Identify core workloads and personas for Azure Databricks.
Describe key concepts of an Azure Databricks solution.




Knowledge check - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/explore-azure-databricks/06-knowledge-check
Knowledge check
3 minutes
1. 

You plan to create an Azure Databricks workspace and use it to work with a SQL Warehouse. Which of the following pricing tiers can you select?

 

Enterprise

Standard

Premium

2. 

You've created an Azure Databricks workspace in which you plan to use code to process data files. What must you create in the workspace?

 

A SQL Warehouse

A Spark cluster

A Windows Server virtual machine

3. 

You want to use Python code to interactively explore data in a text file that you've uploaded to your Azure Databricks workspace. What should you create?

 

A SQL query

An Azure function

A Notebook

Check your answers




Exercise - Explore Azure Databricks - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/explore-azure-databricks/05-exercise-explore-databricks
Exercise - Explore Azure Databricks
30 minutes

Now it's your chance to explore Azure Databricks for yourself. In this exercise, you'll use a provided script to provision an Azure Databricks workspace in your Azure subscription; and then use the Azure Databricks portal to create a Spark cluster and perform some common data analytics tasks.

 Note

To complete this lab, you will need an Azure subscription in which you have administrative access.

Launch the exercise and follow the instructions.




Understand key concepts - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/explore-azure-databricks/04-key-concepts
Understand key concepts
3 minutes

Azure Databricks is an amalgamation of multiple technologies that enable you to work with data at scale. Before using Azure Databricks, there are some key concepts that you should understand.

Apache Spark clusters - Spark is a distributed data processing solution that makes use of clusters to scale processing across multiple compute nodes. Each Spark cluster has a driver node to coordinate processing jobs, and one or more worker nodes on which the processing occurs. This distributed model enables each node to operate on a subset of the job in parallel; reducing the overall time for the job to complete. To learn more about clusters in Azure Databricks, see Clusters in the Azure Databricks documentation.
Databricks File System (DBFS) - While each cluster node has its own local file system (on which operating system and other node-specific files are stored), the nodes in a cluster have access to a shared, distributed file system in which they can access and operate on data files. The Databricks File System (DBFS) enables you to mount cloud storage and use it to work with and persist file-based data. To learn more about DBFS, see Databricks File System (DBFS) in the Azure Databricks documentation.
Notebooks - One of the most common ways for data analysts, data scientists, data engineers, and developers to work with Spark is to write code in notebooks. Notebooks provide an interactive environment in which you can combine text and graphics in Markdown format with cells containing code that you run interactively in the notebook session. To learn more about notebooks, see Notebooks in the Azure Databricks documentation.
Hive metastore - Hive is an open source technology used to define a relational abstraction layer of tables over file-based data. The tables can then be queried using SQL syntax. The table definitions and details of the file system locations on which they're based is stored in the metastore for a Spark cluster. A Hive metastore is created for each cluster when it's created, but you can configure a cluster to use an existing external metastore if necessary - see Metastores in the Azure Databricks documentation for more details.
Delta Lake - Delta Lake builds on the relational table schema abstraction over files in the data lake to add support for SQL semantics commonly found in relational database systems. Capabilities provided by Delta Lake include transaction logging, data type constraints, and the ability to incorporate streaming data into a relational table. To learn more about Delta Lake, see Delta Lake Guide in the Azure Databricks documentation.
SQL Warehouses - SQL Warehouses are relational compute resources with endpoints that enable client applications to connect to an Azure Databricks workspace and use SQL to work with data in tables. The results of SQL queries can be used to create data visualizations and dashboards to support business analytics and decision making. SQL Warehouses are only available in premium tier Azure Databricks workspaces. To learn more about SQL Warehouses, see SQL Warehouses in the Azure Databricks documentation.




Identify Azure Databricks workloads - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/explore-azure-databricks/03-workloads
Identify Azure Databricks workloads
3 minutes

Azure Databricks is a comprehensive platform that offers many data processing capabilities. While you can use the service to support any workload that requires scalable data processing, Azure Databricks is optimized for three specific types of data workload and associated user personas:

Data Science and Engineering
Machine Learning
SQL*

*SQL workloads are only available in premium tier workspaces.

Data Science and Engineering

Azure Databricks provides Apache Spark based processing and analysis of large volumes of data in a data lake. Data engineers, data scientists, and data analysts can use interactive notebooks to run code in Python, Scala, SparkSQL, or other languages to cleanse, transform, aggregate, and analyze data.

Machine Learning

Azure Databricks supports machine learning workloads that involve data exploration and preparation, training and evaluating machine learning models, and serving models to generate predictions for applications and analyses. Data scientists and ML engineers can use AutoML to quickly train predictive models, or apply their skills with common machine learning frameworks such as SparkML, Scikit-Learn, PyTorch, and Tensorflow. They can also manage the end-to-end machine learning lifecycle with MLFlow.

SQL

Azure Databricks supports SQL-based querying for data stored in tables in a SQL Warehouse. This capability enables data analysts to query, aggregate, summarize, and visualize data using familiar SQL syntax and a wide range of SQL-based data analytical tools.

 Note

SQL Warehouses are only available in premium Azure Databricks workspaces.




Get started with Azure Databricks - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/explore-azure-databricks/02-azure-databricks
Get started with Azure Databricks
3 minutes

Azure Databricks is a cloud-based distributed platform for data processing built on Apache Spark. Databricks was designed to unify data science, data engineering, and business data analytics on Spark by creating an easy to use environment that enables users to spend more time working effectively with data, and less time focused on managing clusters and infrastructure. As the platform has evolved, it has kept up to date with the latest advances in the Spark runtime and added usability features to support common data workloads in a single, centrally managed interface.

Azure Databricks is hosted on the Microsoft Azure cloud platform, and integrated with Azure services such as Microsoft Entra ID, Azure Storage, Azure Synapse Analytics, and Azure Machine Learning. Organizations can apply their existing capabilities with the Databricks platform, and build fully integrated data analytics solutions that work with cloud infrastructure used by other enterprise applications.

Creating an Azure Databricks workspace

To use Azure Databricks, you must create an Azure Databricks workspace in your Azure subscription. You can accomplish this by:

Using the Azure portal user interface.
Using an Azure Resource Manager (ARM) or Bicep template.
Using the New-AzDatabricksWorkspace Azure PowerShell cmdlet
Using the az databricks workspace create Azure command line interface (CLI) command.

When you create a workspace, you must specify one of the following pricing tiers:

Standard - Core Apache Spark capabilities with Microsoft Entra integration.
Premium - Role-based access controls and other enterprise-level features.
Trial - A 14-day free trial of a premium-level workspace

Using the Azure Databricks portal

After you've provisioned an Azure Databricks workspace, you can use the Azure Databricks portal to work with data and compute resources. The Azure Databricks portal is a web-based user interface through which you can create and manage workspace resources (such as Spark clusters) and use notebooks and queries to work with data in files and tables.




Introduction - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/explore-azure-databricks/01-introduction
Introduction
1 minute

Azure Databricks is a fully managed, cloud-based data analytics platform, which empowers developers to accelerate AI and innovation by simplifying the process of building enterprise-grade data applications. Built as a joint effort by Microsoft and the team that started Apache Spark, Azure Databricks provides data science, engineering, and analytical teams with a single platform for big data processing and machine learning.

By combining the power of Databricks, an end-to-end, managed Apache Spark platform optimized for the cloud, with the enterprise scale and security of Microsoft's Azure platform, Azure Databricks makes it simple to run large-scale Spark workloads.

In this module, you'll learn how to:

Provision an Azure Databricks workspace.
Identify core workloads and personas for Azure Databricks.
Describe key concepts of an Azure Databricks solution.




Explore Azure Databricks - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/explore-azure-databricks/

Explore Azure Databricks
Module
7 Units
Feedback
Intermediate
Data Engineer
Azure Databricks

Azure Databricks is a cloud service that provides a scalable platform for data analytics using Apache Spark.

Learning objectives

In this module, you'll learn how to:

Provision an Azure Databricks workspace.
Identify core workloads and personas for Azure Databricks.
Describe key concepts of an Azure Databricks solution.
Add
Prerequisites

Before starting this module, you should have a fundamental knowledge of data analytics concepts. Consider completing Azure Data Fundamentals certification before starting this module.

Introduction
min
Get started with Azure Databricks
min
Identify Azure Databricks workloads
min
Understand key concepts
min
Exercise - Explore Azure Databricks
min
Knowledge check
min
Summary
min


Summary - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/integrate-microsoft-purview-azure-synapse-analytics/08-summary
Summary
1 minute

Comprehensive data governance is an important element of an enterprise data analytics solution. By combining Azure Synapse Analytics and Microsoft Purview, you can improve data discoverability while addressing data trustworthiness and compliance requirements across your data estate.

In this module you learned how to:

Catalog Azure Synapse Analytics database assets in Microsoft Purview.
Configure Microsoft Purview integration in Azure Synapse Analytics.
Search the Microsoft Purview catalog from Synapse Studio.
Track data lineage in Azure Synapse Analytics pipelines activities.




Knowledge check - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/integrate-microsoft-purview-azure-synapse-analytics/07-knowledge-check
Knowledge check
4 minutes
1. 

You want to scan data assets in a dedicated SQL pool in your Azure Synapse Analytics workspace. What kind of source should you register in Microsoft Purview?

 

Azure Synapse Analytics

Azure Data Lake Storage Gen2

Azure SQL Database

2. 

You want to scan data assets in the default data lake used by your Azure Synapse Analytics workspace. What kind of source should you register in Microsoft Purview?

 

Azure Synapse Analytics

Azure Data Lake Storage Gen2

Azure Cosmos DB

3. 

You want data analysts using Synapse Studio to be able to find data assets that are registered in a Microsoft Purview collection. What should you do?

 

Register an Azure Synapse Analytics source in the Purview account

Add a Data Explorer pool to the Synapse Workspace

Connect the Purview account to the Synapse analytics workspace

4. 

Which of the following pipeline activities records data lineage data in a connected Purview account?

 

Get Metadata

Copy Data

Lookup

Check your answers




Exercise - Integrate Azure Synapse Analytics and Microsoft Purview - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/integrate-microsoft-purview-azure-synapse-analytics/06-exercise-synapse-purview
Exercise - Integrate Azure Synapse Analytics and Microsoft Purview
40 minutes

Now it's your chance to explore integration between Microsoft Purview and Azure Synapse Analytics for yourself. In this exercise, you'll use a provided script to provision an Azure Synapse Analytics workspace and a Microsoft Purview account in your Azure subscription; and then you'll catalog data assets in your Azure Synapse Analytics workspace and data lake before connecting the Purview account to the workspace to support data discovery and lineage tracking.

 Note

To complete this lab, you will need an Azure subscription in which you have administrative access.

Launch the exercise and follow the instructions.




Track data lineage in pipelines - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/integrate-microsoft-purview-azure-synapse-analytics/05-track-data-lineage
Track data lineage in pipelines
4 minutes

In a typical large-scale analytics solution, data is transferred and transformed across multiple systems until it's loaded into an analytical data store for reporting and analysis. Tracking the lineage of data as moves across the enterprise is an important factor in determining the provenance, trustworthiness, and recency of data assets used to inform analysis and decision making.

Generate and view data lineage information

In Azure Synapse Analytics, data movement and transformation is managed by using pipelines, which consist of an orchestrated set of activities that operate on data. The design and implementation of pipelines is too large a subject to cover in depth in this module, but a key point to be aware of is that there are two activity types available in Synapse Analytics pipelines that automatically generate data lineage information in a connected Purview catalog:

The Copy Data activity
The Data Flow activity

Running a pipeline that includes either of these activities in a workspace with a connected Purview account will result in the creation or update of data assets with lineage information. The assets recorded include:

The source from which the data is extracted.
The activity used to transfer the data.
The destination where the data is stored.

In the Microsoft Purview Governance Portal, you can open the assets in the Purview catalog, and view the lineage information as shown here:

You can also view the lineage for a pipeline activity in Synapse Studio.

 Tip

For more information about tracking data lineage for Azure Synapse Analytics pipelines in Microsoft Purview, see How to get lineage from Azure Synapse Analytics into Microsoft Purview.

You'll get a chance to generate and view data lineage from a Synapse Analytics pipeline in the exercise later in this module.




Search a Purview catalog in Synapse Studio - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/integrate-microsoft-purview-azure-synapse-analytics/04-search-purview
Search a Purview catalog in Synapse Studio
4 minutes

After connecting an Azure Synapse Analytics workspace to a Microsoft Purview account, you can search the Purview catalog from Synapse Studio. This ability to discover and examine data assets from across the enterprise can greatly assist data engineers, data analysts, and other consumers of data by providing a curated catalog of documented data sources for analysis and reporting.

Search the Purview catalog in Synapse Studio

You can search the catalog from a connected Purview account by using the Search bar in the Data, Develop, or Integrate pages in Synapse Studio, as shown here:

The search results interface, and the details for each asset found reflect the user interface in the Microsoft Purview Governance Portal, ensuring that the data discovery and examination experience in Synapse Studio is consistent for users of Microsoft Purview in its own portal.

 Tip

For more information about searching the Purview catalog in Synapse Studio, see Discover, connect, and explore data in Synapse using Microsoft Purview.

You'll get a chance to try searching a connected Purview account for yourself in the exercise later in this module.




Connect Microsoft Purview to an Azure Synapse Analytics workspace - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/integrate-microsoft-purview-azure-synapse-analytics/03-configure-purview-integration
Connect Microsoft Purview to an Azure Synapse Analytics workspace
5 minutes

So far, you've learned how you can use Azure Synapse Analytics data stores as sources for a Microsoft Purview catalog; which is similar in most respects to using any other data source.

What sets Azure Synapse Analytics apart from many other data sources is the ability to configure direct integration between an Azure Synapse Analytics workspace and a Microsoft Purview account. By linking your workspace to a Purview account, you can:

Search the Purview catalog in the Synapse Studio user interface.
Push details of data pipeline activities to Purview in order to track data lineage information.
Connect a Purview account to a Synapse Analytics workspace

You connect a Microsoft Purview account to an Azure Synapse Analytics workspace on the Manage page of Synapse Studio, as shown here:

Security considerations

To connect a Purview account by using the Synapse Studio interface, you require Collection Administrator access to the Purview account's root collection. After successfully connecting the account, the managed identity used by your Azure Synapse Analytics workspace will be added to the collection's Data Curator role.

If your Microsoft Purview account is behind a firewall, you need to create a managed endpoint, and configure the connection to access Purview using that endpoint. For more information, see Access a secured Microsoft Purview account from Azure Synapse Analytics.

 Tip

To learn more about connecting Azure Synapse Analytics to Microsoft Purview, see QuickStart:Connect a Synapse workspace to a Microsoft Purview account.

You'll get a chance to connect an Azure Synapse Analytics workspace to a Microsoft Purview account in the exercise later in this module.




Catalog Azure Synapse Analytics data assets in Microsoft Purview - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/integrate-microsoft-purview-azure-synapse-analytics/02-catalog-azure-synapse
Catalog Azure Synapse Analytics data assets in Microsoft Purview
8 minutes

Azure Synapse Analytics is a platform for cloud-scale analytics workloads that process data in multiple sources; including:

Relational databases in serverless and dedicated SQL pools
Files in Azure Data Lake Storage Gen2

A comprehensive data analytics solution can include many folders and files in a data lake, and multiple databases that each contain many tables, each with multiple fields. For a data analyst, finding and understanding the data assets associated with a Synapse Analytics workspace can present a significant challenge before any analysis or reporting can even begin.

Microsoft Purview can help in this scenario by cataloging the data assets in a data map, and enabling data stewards to add metadata, categorization, subject matter contact details, and other information that helps data analysts identify and understand data.

Configure data access for Microsoft Purview

In order to scan the data assets in the data lake storage and databases used in your Azure Synapse Workspace, Microsoft Purview must have appropriate permissions to read the data. In practice, this means that the account used by your Microsoft Purview account (usually a system-assigned managed identity that is created when Microsoft Purview is provisioned) needs to be a member of the appropriate role-based access control (RBAC) and database roles.

The diagram shows that Microsoft Purview requires role membership that permits the following access:

Read access to the Azure Synapse workspace (achieved through membership of the Reader role for the Azure Synapse Workspace resource in the Azure subscription).
Read access to each SQL database that will be scanned (achieved through membership of the db_datareader fixed database role in each database).
Read access to data lake storage (achieved through membership of the Storage Blob Data Reader role for the Azure Storage account hosting the Azure Data Lake Storage Gen2 container for the data lake).

 Tip

Learn more:

For more information about RBAC in Microsoft Azure, see What is Azure role-based access control (Azure RBAC)?
For more information about database-level roles in Azure Synapse Analytics SQL pools, see Database-level roles.

You'll get a chance to assign RBAC and SQL database role membership to support Microsoft Purview data access for yourself in the exercise later in this module.

Register and scan data sources

Microsoft Purview supports the creation of a data map that catalogs data assets in collections by scanning registered sources. Collections form a hierarchy of logical groupings of related data assets, under a root collection that is created when you provision a Microsoft Purview account. You can use the Microsoft Purview Governance Portal to create and manage collections in your account.

To include assets from a particular data source, you need to register the source in a collection. Microsoft Purview supports many kinds of source, including:

Azure Synapse Analytics - One or more SQL databases in a Synapse Analytics workspace.
Azure Data Lake Storage Gen2 - Blob containers used to host folders and files in a data lake.

To catalog assets used in an Azure Synapse Analytics workspace, you can register one or both of these sources in a collection, as shown here:

After registering the sources where your data assets are stored, you can scan each source to catalog the assets it contains. You can scan each source interactively, and you can schedule period scans to keep the data map up to date.

 Tip

To learn more about registering and scanning sources, see Scans and ingestion in Microsoft Purview.

You'll get a chance to register and scan sources for an Azure Synapse Analytics workspace in the exercise later in this module.

View and manage cataloged data assets

As each scan finds data assets in the registered sources, they're added to the associated collection in the data catalog. You can query the data catalog in the Microsoft Purview Governance Portal to view and filter the data assets, as shown here:

Data assets include items in the registered data stores at multiple levels. For example, assets from an Azure Synapse Analytics source include databases, schemas, tables, and individual fields; and assets from an Azure Data Lake Storage Gen 2 source include containers, folders, and files.

You can view and edit the properties of each asset to add contextual information such as descriptions, contacts for expert help, and other useful metadata. Data assets can also be classified using built-in or custom classifications that match specific patterns of data field to common types of data - for example, passport numbers, credit card numbers, and others.

 Tip

To learn more about data asset classification, see Data classification in the Microsoft Purview governance portal.




Introduction - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/integrate-microsoft-purview-azure-synapse-analytics/01-introduction
Introduction
1 minute

Microsoft Purview is a cloud service that provides the basis of a data governance solution in which you can catalog, classify, and track data assets across a large-scale data estate.

Azure Synapse Analytics is a cloud-scale data analytics suite that supports data ingestion and transformation, distributed big data processing and exploration with SQL and Spark, and enterprise data warehousing.

When combined, Microsoft Purview and Azure Synapse Analytics can be used to create a comprehensive solution for reliable, massively scalable data analytics with rich data asset discovery and lineage tracking capabilities.

In this module you'll learn how to:

Catalog Azure Synapse Analytics database assets in Microsoft Purview.
Configure Microsoft Purview integration in Azure Synapse Analytics.
Search the Microsoft Purview catalog from Synapse Studio.
Track data lineage in Azure Synapse Analytics pipelines activities.




Integrate Microsoft Purview and Azure Synapse Analytics - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/integrate-microsoft-purview-azure-synapse-analytics/

Integrate Microsoft Purview and Azure Synapse Analytics
Module
8 Units
Feedback
Intermediate
Data Analyst
Data Engineer
Azure Synapse Analytics
Microsoft Purview

Learn how to integrate Microsoft Purview with Azure Synapse Analytics to improve data discoverability and lineage tracking.

Learning objectives

After completing this module, you'll be able to:

Catalog Azure Synapse Analytics database assets in Microsoft Purview.
Configure Microsoft Purview integration in Azure Synapse Analytics.
Search the Microsoft Purview catalog from Synapse Studio.
Track data lineage in Azure Synapse Analytics pipelines activities.
Add
Prerequisites

Before starting this module, you should be familiar with both Azure Synapse Analytics and Microsoft Purview. Consider completing the following modules before starting this one:

Introduction to Azure Synapse Analytics
Introduction to Microsoft Purview
Introduction
min
Catalog Azure Synapse Analytics data assets in Microsoft Purview
min
Connect Microsoft Purview to an Azure Synapse Analytics workspace
min
Search a Purview catalog in Synapse Studio
min
Track data lineage in pipelines
min
Exercise - Integrate Azure Synapse Analytics and Microsoft Purview
min
Knowledge check
min
Summary
min


Summary - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/manage-power-bi-artifacts-use-microsoft-purview/6-summary
Summary
2 minutes

As data grows in your organization, tracking its origins and evolution becomes more and more difficult. Microsoft Purview makes it possible to see end-to-end lineage of data sources both in Power BI and extending to the data platform.

From an enterprise perspective, the ability to scan and view data across your entire Power BI tenant is critical. Microsoft Purview enables users to find trusted data and also to troubleshoot and understand dependencies across the analytics landscape.

Learn more
Register and scan a Power BI tenant in Microsoft Purview




Knowledge check - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/manage-power-bi-artifacts-use-microsoft-purview/5-knowledge-check
Knowledge check
3 minutes

Choose the best response for each of the questions below. Then select Check your answers.

Check your knowledge
1. 

What are the prerequisite steps to register and scan a Power BI tenant in Microsoft Purview?

 

Set up authentication between Purview and Power BI, and configure the Power BI tenant.

Set up and deploy a Power BI data gateway.

Configure the Power BI tenant only.

2. 

What steps are required in the Power BI admin portal to enable the scanning and display of enhanced metadata?

 

There are no other steps required. Enhanced metadata displays by default.

Enable enhanced metadata scanning in the Azure portal.

Enable an admin API setting in the Power BI admin portal.

3. 

What details of an asset would be helpful in performing an impact analysis?

 

Lineage.

Properties.

Schema.

Check your answers




View Power BI metadata and lineage - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/manage-power-bi-artifacts-use-microsoft-purview/4-view-lineage
View Power BI metadata and lineage
4 minutes

Purview and Power BI together are powerful, enhancing the ability of the search and browse features to see both the schema and lineage of Power BI assets.

Extend your search with enhanced metadata

Metadata scanning facilitates governance by making it possible to catalog and report on the metadata of your organization's Power BI artifacts. The results of metadata scanning are displayed on the schema tab of the asset.

 Note

Metadata scanning must be enabled in the Power BI admin portal. See Set up metadata scanning in your organization to learn more.

After performing a search in the Purview Governance Portal, select a Power BI asset from your search result to see the sensitivity labels and endorsement metadata. Additional business metadata includes the dataset user configuration, create datetime, and description.

Under the Schema tab, you can see the list of all the tables, columns, and measures created inside the Power BI dataset.

For more detail, selecting a particular field within the schema tab will take you to the details for that field. You can then view the overview, properties, lineage, contacts, and related assets for that particular field.

Metadata scanning requires no special license. It works for all of your tenant metadata, including items that are located in non-Premium workspaces.

If you'd like more information about assets, you also have the option open the Power BI dataset in the Power BI service for further analytics, root-cause investigation, impact analysis, management tasks, and dataset enrichment.

Extend your search with lineage

If you're using the search and browse features in Microsoft Purview to find assets for reporting or to troubleshoot existing assets, you likely need more information on where data actually comes from, and what transformation steps it has undergone. The lineage view displays the flow of data from the source through to Power BI assets, including dataflows, datasets, reports, and dashboards.

Although you can track data lineage in Power BI, this information is limited to the items in a single workspace. Lineage in Purview enables you to view the movement of data across more than one workspace, in a single view.

Lineage enables easy troubleshooting and deeper analysis of analytics projects. You're able to look both up and down-stream, to perform either root cause or impact analysis.

For example, you can detect the Azure Synapse Analytics pipeline that is responsible for the transformation of the data upstream of Power BI.

In the Purview Governance Portal, lineage is displayed from the asset you're currently viewing.




Search and browse Power BI assets - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/manage-power-bi-artifacts-use-microsoft-purview/3-search-browse-assets
Search and browse Power BI assets
4 minutes

After data is registered and scanned, analysts and data consumers need to be able to find data, view enhanced metadata, and track data lineage. Search and browse in the Purview Data Catalog enables you to quickly find trustworthy data.

After scanning your Power BI tenant, you'll see those assets appear in the search results, including underlying data sources.

Search the Microsoft Purview Data Catalog

From the Microsoft Purview Governance Portal, you can type relevant keywords to start discovering assets. In this scenario, you're looking for "sales."

The screenshot below displays the search result, with all assets corresponding to the keywords entered in the search engine. Notice the appearance of Power BI assets.

You can fine-tune your search using the filters on the left side of the page. Filters available include source type, keyword, object type, collection, classification, contact, label, and glossary term.

Browse the Microsoft Purview Data Catalog

Searching for specific assets is great if you know what you're looking for, but analysts and data consumers may not know exactly how their data estate is structured. The browse experience enables you to explore what data is available, either by collection or through traversing the hierarchy of each data source in the catalog.

To access the browse experience, select Browse assets from the governance portal home page.

You can browse the data catalog either by collection or by source type, depending on your needs. Browsing by either collection or source type allows you to see assets you have access to. Once you find the asset you're looking for, you can select it to see details on schema, lineage, and a detailed classification list.

Uniquely, browsing by source type allows you to see the hierarchies of data sources using an explorer view. This is a helpful and familiar way to navigate to see lists of scanned assets.

 Note

Assets in Purview are organized by collection and permissions are granted at collection level. Both searching and browsing require data reader permissions. See Access control in the Microsoft Purview Data Map for details on permissions.

Select an asset to see details about the properties, schema, lineage, contacts, and related assets.




Register and scan a Power BI tenant - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/manage-power-bi-artifacts-use-microsoft-purview/2-register-scan-tenant
Register and scan a Power BI tenant
4 minutes

To get an understanding of what is going on in your Power BI tenant, you can perform a full scan in Microsoft Purview to view the schema and lineage of assets across all workspaces. After, you can schedule incremental scans on workspaces that have changed since the previous scan.

There are a few pre-requisite steps required to scan your Power BI tenant in Microsoft Purview.

 Tip

If you need to create a Microsoft Purview account, see the quickstart guide to create a Microsoft Purview account in the Azure Portal.

Establish a connection between Microsoft Purview and Power BI

Microsoft Purview can connect to and scan Power BI either in the same tenant or across tenants. You'll need to set up authentication either by using a Managed Identity or a Delegated Authentication.

 Note

See Register and scan a Power BI tenant to learn more about the set-up and authentication of Power BI connections in same and cross-tenant scenarios.

Authenticate to Power BI tenant

Give Microsoft Purview permissions to access your Power BI tenant.

If you're using Managed Identity to authenticate to Power BI, you'll need to create a security group in Microsoft Entra ID, and add your Microsoft Purview managed identity to this security group.

If a security group containing the Purview managed identity already exists, you can proceed to configuring the Power BI tenant.

Configure Power BI tenant

Next you need to enable access to Power BI by Microsoft Purview in Power BI itself. This is done by enabling Allow service principals to use read-only Power BI admin APIs in the Power BI admin portal.

Register and scan Power BI

Now that you've got access set up in both Microsoft Purview and Power BI, you can register and scan your Power BI tenant.

After registering the Power BI tenant, initiate the scan by selecting New scan. Give your scan a name and step through the interface, where you'll be able to to exclude personal workspaces, confirm integration runtime and credentials, and select a collection. Test the connection to ensure authentication is set up properly.

 Note

If you're performing the scan, you must be both a Data Source Administrator and a Data Reader. See Access control in the Microsoft Purview Data Map for details on permissions.

You're able to track the progress of the scan in the data map, and once the scan is complete, you'll be able to search and browse the contents of your entire Power BI tenant!

If you're having any issues with scanning your Power BI tenant, see Troubleshoot Power BI tenant scans in Microsoft Purview for details and helpful hints.




Introduction - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/manage-power-bi-artifacts-use-microsoft-purview/1-introduction
Introduction
3 minutes

As the landscape of enterprise data continues to grow, it's critical to get an accurate view of your organization's data. Microsoft Purview and Power BI integration enables you to scan your entire Power BI tenant to search and browse Power BI assets, explore enhanced dataset metadata, trace end-to-end data lineage, and drill-down into datasets in Power BI for further analysis.

Learning objectives

In this module, you will:

Register and scan a Power BI tenant.
Use the search and browse functions to find data assets.
Describe the schema details and data lineage tracing of Power BI data assets.




Manage Power BI assets by using Microsoft Purview - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/manage-power-bi-artifacts-use-microsoft-purview/

Manage Power BI assets by using Microsoft Purview
Module
6 Units
Feedback
Beginner
Data Analyst
Data Engineer
Power BI
Microsoft Purview

Improve data governance and asset discovery using Power BI and Microsoft Purview integration.

Learning objectives

By the end of this module, youll be able to:

Register and scan a Power BI tenant.
Use the search and browse functions to find data assets.
Describe the schema details and data lineage tracing of Power BI data assets.
Add
Prerequisites
Familiarity with the Azure data ecosystem.
Introduction
min
Register and scan a Power BI tenant
min
Search and browse Power BI assets
min
View Power BI metadata and lineage
min
Knowledge check
min
Summary
min


Summary - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/catalog-data-artifacts-use-microsoft-purview/6-summary
Summary
3 minutes

Data classification in Microsoft Purview is similar to subject tagging, and is used to mark and identify data of a specific type that's found within your data estate during scanning. Classification is based on the business context of the data.

Prior to data classification and labeling, data assets must be registered and scanned in Microsoft Purview. After assets are registered, scanned, classified, and labeled, analysts and other data consumers can easily identify and use data assets.

Learn more
Quickstart: Create an account in the Microsoft Purview governance portal
Microsoft Purview how-to guides




Knowledge check - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/catalog-data-artifacts-use-microsoft-purview/5-knowledge-check
Knowledge check
3 minutes

Choose the best response for each of the questions below. Then select Check your answers.

Check your knowledge
1. 

What level are user permissions set at in Microsoft Purview?

 

Tenant.

Data catalog.

Collection.

2. 

What are the two types of classification in Microsoft Purview?

 

System classifications and custom classifications.

Microsoft Information Protection Sensitivity Labels and system classifications.

Custom classifications and user-defined classifications.

3. 

If a data analyst is looking for a specific resource for reporting, what should they use?

 

Purview Data Catalog to search.

The business glossary.

Import into Power BI and create a custom report.

Check your answers




Search the data catalog - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/catalog-data-artifacts-use-microsoft-purview/4-search-data-catalog
Search the data catalog
3 minutes

A data catalog search can empower business and data analysts to find and interpret data. The data catalog provides intelligent recommendations based on data relationships, business context, and search history. The Purview data catalog can assist data teams by adding business context to assets to drive analytics, AI and ML initiatives.

The data catalog can be searched by keyword, object type, collection, classification, contact, label, or assigned term. Results can then be sorted by relevance or name.

For more information about searching for trusted assets for reporting, see Discover trusted data using Microsoft Purview.




Classify and label data - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/catalog-data-artifacts-use-microsoft-purview/3-classify-data
Classify and label data
6 minutes

Glossary terms, classifications and labels are all annotations to a data asset. Each of them have a different meaning in the context of the data catalog.

What is data classification?

Classifications are annotations that can be assigned to entities. The flexibility of classifications enables you to use them for multiple scenarios such as:

understanding the nature of data stored in the data assets
defining access control policies

Classification is based on the business context of the data. For example, you might classify assets byPassport Number, Driver's License Number,Credit Card Number,SWIFT Code, Persons Name, and so on. Microsoft Purview has more than 200 system classifiers today. Users can also define their own classifiers in the data catalog. As part of the scanning process, classifications are automatically detected and applied as metadata within the Purview Data Catalog.

Classification rules

In Microsoft Purview, you can apply system or custom classifications on a file, table, or column asset. Microsoft Purview makes use of Regex patterns and bloom filters to classify data. These classifications are then associated with the metadata discovered in the Azure Purview Data Catalog.

Metadata is used to help describe the data that is being scanned and made available in the catalog. During the configuration of a scan set, you can specify classification rules to apply during the scan that will also serve as metadata. The existing classification rules fall under five major categories:

Government - covers attributes such as government identity cards, driver license numbers, passport numbers, etc.
Financial - covers attributes such as bank account numbers or credit card numbers.
Personal - personal information such as a person's age, date of birth, email address, phone number, etc.
Security - attributes like passwords that may be stored.
Miscellaneous - attributes not covered in the other categories.
Why classify data?

A good data governance strategy includes a process to classify data to understand its level of confidentiality, determine if the data source is compliant with various regulations, or how long to retain it for. Classification in Microsoft Purview makes data assets easier to understand, search, and govern. Classification can also help you implement measures to protect sensitive data.

Once a classification is tagged to a data source after a scan, you can generate reports and insights to gain a stronger understanding of your data estate. Because classification is based on the business context of the data, it can help bridge the gap between the business and the data team.

Data classification: system vs. custom classification

Microsoft Purview supports both system and custom classifications. There are over +200 system classifications available in Microsoft Purview today. Data teams need to know that if necessary classifications aren't available out of the box, they can work with the data stewards to create custom classifications, to meet their own organizational data governance requirements.

 Important

For the entire list of available system classifications, see Supported classifications in Microsoft Purview.

Who creates custom classifications?

Purview Data Curators can create, update, and delete custom classifiers and classification rules. Purview Data Readers can only view classifiers and classification rules.

In practical terms, Data Curators may not be members of the data team. It is however critical that data team members understand classification to be able to successfully work together and govern data across an organization.

What are data labels?

The Microsoft Purview Data Map supports labeling structured and unstructured data stored across various data sources. This may sound familiar to you from other Microsoft technologies - and may be known as sensitivity labels. The data map extends the use of sensitivity labels from Microsoft Purview Information Protection to assets stored in infrastructure cloud locations and structured data sources.

Labels are defined in Microsoft Purview Information Protection, and you can extend the application to Microsoft Purview Data Catalog.

The screenshot below shows both data classification and label in the Microsoft Purview Data Catalog. You can see that this Azure SQL table has a column called CreditCard:

Classified as Credit Card Number because scan detected numbers corresponding to credit card pattern rules.
Labeled as Confidential  Finance because credit card number was defined in your organization as confidential information (and this label brings encryption).




Register and scan data - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/catalog-data-artifacts-use-microsoft-purview/2-register-scan-data
Register and scan data
10 minutes

Registration and scanning of data enables discoverability of data across an estate.

Before you can register and scan data, its important to understand the concept of collections. In Microsoft Purview Data Catalog, collections are key concept because they drive permissions and asset protection. Collections are also used to understand data estate health and catalog usage and adoption, as featured in the data stewardship section of your Data Estate Insights.

Collections

The data map is at the core of Microsoft Purview, which keeps an up-to-date map of assets and their metadata across your data estate. To hydrate the data map, you need to register and scan your data sources, which is done at the collection level. Collections support organizational mapping of metadata. By using collections, you can manage and maintain data sources, scans, and assets in a hierarchy instead of a flat structure. Collections allow you to build a custom hierarchical model of your data landscape based on how your organization plans to use Microsoft Purview to govern your landscape.

Collections also provide a security boundary for your metadata in the data map. Access to collections, data sources, and metadata is set up and maintained based on the collections hierarchy in Microsoft Purview, following a least-privilege model:

Users have the minimum amount of access they need to do their jobs.
Users don't have access to sensitive data that they don't need.

Data sources are registered at the collection level. Scan results can then be sent to this collection or a sub collection. The image below displays the structure of a collection.

 Tip

Learn more about Microsoft Purview collections architectures and best practices.

Register and scan data sources

Data governance use begins at collection level, with the registration of data sources in Microsoft Purview governance portal. Microsoft Purview supports an array of data sources. Data teams (analysts, engineers, and scientists) may not be actively registering and scanning data in Microsoft Purview, but it's critical that data consumers understand governance efforts. Registering and scanning assets requires Data Curator permissions.

 Important

Data registered and scanned in Microsoft Purview only collects metadata information. Data remains in its location and isn't migrated to any other platform.

Register a data source

Registering a data source is done from within the Azure portal. Once you have a Microsoft Purview service configured in Azure, you use the Microsoft Purview governance portal to register your data sources.

To register a data source, you'll select the icon to register a data source as displayed in the image below. Selecting this icon will give you access to all data source connectors.

Below is a small sample of available connectors in Microsoft Purview Data Catalog. See supported data sources and file types for an up-to-date list of supported data sources and connectors.

Registering a data source is straightforward, you need to complete the required fields. Authentication will be done during the scanning phase.

Each type of data source you choose will require specific information to complete the registration. For example, if your data sources reside in your Azure subscription, you'll choose the necessary subscription and storage account name.

Scan a data source

Once you have data sources registered in the Microsoft Purview governance portal and displayed in the data map, you can set up scanning. The scanning process can be triggered to run immediately or can be scheduled to run on a periodic basis to keep your Microsoft Purview account up to date.

Scanning assets is as simple as selecting New scan from the resource as displayed in the data map.

You'll now need to configure your scan and assign the following details:

Assign a friendly name.
Define which integration runtime to use to perform the scan.
Create credentials to authenticate to your registered data sources.
Choose a collection to send scan results.

After the basic configuration, you'll scope your scan, which allows you to choose just a specific zone of your data source. For instance, if you have a collection called Raw in your data map, you can define the scope to scan only the raw container of your data lake.

After configuring and scoping your scan, you'll define the scan rule set. A scan rule set is a container for grouping a set of scan rules together so that you can easily associate them with a scan. For example, you might create a default scan rule set for each of your data source types, and then use these scan rule sets by default for all scans within your company. You might also want users with the right permissions to create other scan rule sets with different configurations based on business need.

Once a scan is complete, you can refer to the scan details to view information about the number of scans completed, assets detected, assets classified, Scan information. Its a good place to monitor scan progress, including success or failure.

 Tip

Refer to Scanning best practices for more information on scanning assets.

Roles and permissions

Permissions in Microsoft Purview are assigned at collection level. Collections are used to organize assets and sources and can be thought of as a logical grouping of data assets.

Data teams looking to discover and use data need to be assigned the Data Reader role in a collection in Microsoft Purview. The Data Reader role enables users to find assets, but doesn't enable users to edit anything. The Data Curator role is required to edit information about assets, assign classifications, and associate assets with glossary entries. To set up scans via the Microsoft Purview Governance Portal, individuals need to be either a data curator on the collection or data curator and data source administrator where the source is registered.

When a Microsoft Purview account is created, it starts with a root collection that has the same name as the Microsoft Purview account itself. The creator of the Microsoft Purview account is automatically added as a Collection Admin, who can then assign Data Source Admin, Data Curator, and Data Reader on this root collection, and can edit and manage this collection.

 Tip

Learn more about Microsoft Purview permissions and access.




Introduction - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/catalog-data-artifacts-use-microsoft-purview/1-introduction
Introduction
3 minutes

The Microsoft Purview Data Catalog offers a browse experience that enables users to explore available data. Users can explore the data catalog either by collection or through traversing the hierarchy of each data source. The first step in understanding the contents of your data map is registering and scanning data, after which you can classify data for easy identification of assets to use for reporting.

Learning objectives

In this module, you will:

Describe asset classification in Microsoft Purview.




Catalog data artifacts by using Microsoft Purview - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/catalog-data-artifacts-use-microsoft-purview/

Catalog data artifacts by using Microsoft Purview
Module
6 Units
Feedback
Beginner
Data Analyst
Data Engineer
Microsoft Purview

Register, scan, catalog, and view data assets and their relevant details in Microsoft Purview.

Learning objectives

By the end of this module, youll be able to:

Describe asset classification in Microsoft Purview.
Add
Prerequisites
Experience using the Azure data ecosystem.
Introduction
min
Register and scan data
min
Classify and label data
min
Search the data catalog
min
Knowledge check
min
Summary
min


Summary - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/discover-trusted-data-use-azure-purview/7-summary
Summary
3 minutes

Using search in Azure Purview data catalog enabled you to find the correct asset to use for the sales report. You were able to locate the asset, verify that it is certified for use, contact experts and dataset owners, and open a Power BI desktop report containing a connection to the asset. You also integrated Microsoft Purview into Azure Synapse studio to enrich the Azure Synapse experience.

Microsoft Purview empowers data analysts and other data consumers to find valuable, trustworthy data by searching and browsing data assets across an organization's data estate.

Learn more
Microsoft Purview Permissions
Microsoft Source Readiness at Scale




Knowledge check - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/discover-trusted-data-use-azure-purview/6-knowledge-check
Knowledge check
3 minutes

Choose the best response for each of the questions below. Then select Check your answers.

Check your knowledge
1. 

What feature of Microsoft Purview can analysts and other data consumers use to find trustworthy data for reports?

 

Data map.

Data catalog.

Data policies.

2. 

If an analyst is looking for a specific asset by name and type, what is the most efficient way to find that asset in the data catalog?

 

Browse assets.

Manage glossary.

Search catalog.

3. 

How can users download Power BI data source files that contain connections to assets discovered in Microsoft Purview?

 

In the Microsoft Purview data catalog, in the asset view.

In the Microsoft Purview insights report.

Users can't download Power BI data source files from Microsoft Purview.

Check your answers




Demo-Introduction - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/discover-trusted-data-use-azure-purview/5-integrate
Integrate with Azure Synapse Analytics
4 minutes

Microsoft Purview can be integrated directly into Azure Synapse. If Azure Synapse Studio is massively deployed in your organization, you can get the data catalog experience directly in Azure Synapse Studio.

This integrated experience allows you to discover Microsoft Purview assets, interact with them through Synapse capabilities, and push lineage information to Microsoft Purview.

 Note

To connect an Microsoft Purview Account to a Synapse workspace, you need 2 types of permissions. You need a contributor role in Synapse workspace from Azure portal identity and access management (IAM). You also need access to that Microsoft Purview Account. For more information, see Microsoft Purview permissions.

Lets imagine you need to find and understand some assets before working with them in pipelines or notebooks. From Azure Synapse Studio, you can easily query your Microsoft Purview data catalog.

In Azure Synapse Studio, from the Data blade on the left, select Purview in the dropdown next to the search bar.

Search for the asset that exists in Purview. Imagine you're looking for movie files. Enter the keyword movie in the search bar, and fine tune your search by selecting Files as the object type and Raw as the collection.

Select the first asset Movies.csv to get asset details. Because you are in Azure Synapse Studio, you can also leverage Azure Synapse capabilities.

For instance, you can use Azure Synapse serverless to query your assets. Select Develop, New SQL Script and Select top 100.

Double check you're connected to your serveless instance and select Run to execute the script and get an overview of your data.

After reviewing data, you can use the asset, for example, adding to a new dataflow in Azure Synapse.

 Note

See Connect an Microsoft Purview Account for detailed information about integrating Microsoft Purview into Azure Synapse Analytics.




Use assets with Power BI - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/discover-trusted-data-use-azure-purview/4-use-asset-power-bi
Use assets with Power BI
3 minutes

The integration of Microsoft Purview and Power BI makes it possible to gain a more complete understanding of the data across your estate.

Request access to assets

In your search or browsing session, you may come across assets that you don't have access to. Microsoft Purview makes it simple to request access directly from the Data Catalog by using the Request access button. Requesting access will kick off a workflow that manages requests and approvals.

Build a Power BI report using data discovered in Purview

Working as a new analyst, you've taken the time to search and browse assets and now you'd like to use those trusted assets in a Power BI report. Purview makes it simple, with the ability to open the asset in Power BI desktop.

Selecting Open in Power BI Desktop initiates the download of a Power BI Data Source file (PBIDS) you can open with Power BI Desktop. PBIDS files contain a connection to the data source, so all you need to do is enter credentials upon opening the file and you're ready to start building.

Scan a Power BI tenant

In addition to using Purview to find trusted data sources in the data estate to build reports, you can also scan your Power BI tenant to manage and catalog assets. The metadata of Power BI assets, and information about their lineage across Power BI workspaces and their connections to data sources, are then available in Microsoft Purview.

 Note

See Connect to and manage a Power BI tenant in Microsoft Purview for more details.




Browse assets - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/discover-trusted-data-use-azure-purview/3-browse-assets
Browse assets
4 minutes

Searching a data catalog is a great tool for data discovery you know what you're looking for. Often, you may not know how your data estate is structured. The Microsoft Purview data catalog offers a browse experience that enables exploration of available data, either by collection or by exploring the hierarchy of each data source in the catalog.

Browse by collection or source type

If you're new to an organization or department, you may want to familiarize yourself with the contents of the data estate. From the Microsoft Purview Studio home page, select the Browse assets tile to browse either by collection or by source type.

Here you can specify whether you'd like to browse by collection or by source type.

Browse by collection allows you to explore the different collections you're a data reader or curator for. You'll only see collections you have access to. Select a collection to get a list of assets in that collection with the facets and filters available in search.

 Tip

Collections are a tool to manage ownership and access control across assets and data sources. They also organize assets and sources into categories that are customized to match the business flow. See Create and manage collections in Microsoft Purview to learn more.

Browse by source type allows you to explore the hierarchies of data sources using an explorer view.

After selecting a tile associated with a data source type, you'll see a list of assets belonging to that type. From there, you'll be able to use the explorer view to see parent and child assets.

The Microsoft Purview data catalog browse experience enables analysts or data consumers to explore what data is available in many different ways. Microsoft Purview has the ability to enable users access to data you may not have known about before. The possibilities are endless so long as your organization's data stewards have scanned and classified data across the estate.




Search for assets - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/discover-trusted-data-use-azure-purview/2-search-browse-assets
Search for assets
10 minutes

Microsoft Purview offers a central place to discover and understand assets to use in your day-to-day activities. This central place, Microsoft Purview data catalog, provides advanced search capabilities to quickly find the right assets and information. Using keywords, business terms and Microsoft Purview data catalog functionalities, you can find the assets needed to build and design reports.

As a data analyst looking for assets, you'll be searching the Microsoft Purview data catalog. This assumes that the Microsoft Purview data Map has been created by your organization. The data map provides the foundation for data discovery. The data map captures metadata about enterprise data in analytics and operations systems on-premises and in the cloud and must be established before the data catalog can be searched.

 Note

Learn more about the Microsoft Purview data map components.

Search the Microsoft Purview data catalog

From the Microsoft Purview Studio home page, users can type relevant keywords to start discovering assets. In this scenario, you're looking for product sales.

The screenshot below displays the search result, with all assets corresponding to the keywords entered in the search engine.

You can fine-tune your search using the filters on the left side of the page.

You can filter by:

Source type (and instance if needed)
Object type
Classification
Glossary term
If needed, more options are available like Collection, Contact and Label

You've been instructed to connect to sources like Azure SQL tables. In the result displayed below, two assets are displayed. To use the correct asset, its possible to browse each asset to dig for more detailed information. Alternatively, you can rely on the work done by the data stewards who have labeled certified assets for the organization.

Before using this asset to create your report, you need to verify more details and validate where data comes from to populate this asset. Select the asset to access more information.

Understand a single asset
Asset overview

Select an asset to see the overview. The overview displays information at a glance, including a description, asset classification, schema classification, collection path, asset hierarchy, and glossary terms.

The asset description provides a brief explanation of the purpose of an asset. Data stewards have made data analysts lives easier in the screenshot below, by noting that this is the correct resource to use for sales reporting.

Beneath the description, you'll see the asset classification and schema classification.

Data classification, in the context of Microsoft Purview, is a way of categorizing data assets by assigning unique logical labels or classes. Classification is based on the business context of the data. For example, you might classify assets by Passport Number, Driver's License Number, Credit Card Number, SWIFT Code, Persons Name, and so on. Asset classifications can be automatically applied during a scan or applied manually.

 Note

Microsoft Purview comes with more than 200 classifications out of the box. For a full list of classifications, see System classifications in Microsoft Purview.

The overview tab reflects both asset level classifications and column level classifications that have been applied, which you can also view as part of the schema.

 Important

You may notice that the classifications displayed above are sensitive or contain personally identifiable information (PII). data encryption is done at the source level, and Microsoft Purview stores only the metadata. It does not preview data.

You can also view the collection path, hierarchy and glossary terms on the right side of the overview tab.

The collection path refers to the location of the asset inside Microsoft Purview. You have the option to move an asset to another collection.

You can view the full asset hierarchy within the overview tab. As an example: if you navigate to a SQL table, then you can see the schema, database, and the server the table belongs to.

Glossary terms are a managed vocabulary for business terms that can be used to categorize and relate assets across your environment. For example, terms like 'customer,' 'buyer, 'cost center,' or any terms that give your data context for your users. You can view the glossary terms for an asset in the overview section, and you can add a glossary term on an asset by editing the asset.

 Note

For more information, see the business glossary page.

Asset schema

The schema view of the asset includes more granular details about the asset, such as column names, data types, column level classifications, terms, and descriptions.

Asset lineage

Asset lineage gives you a clear view of how the asset is populated and where data comes from. Data lineage is broadly understood as the lifecycle that spans the data's origin, and where it moves over time across the data estate. Data lineage is important to analysts because it enables understanding of where data is coming from, what upstream changes may have occurred, and how it flows through the enterprise data systems.

A single view on the asset lineage tab displays the data flow to and from the asset. Asset lineage can also help you understand how the asset was built and how the asset is used inside the organization.

The columns pane on the left side of the lineage tab allows users to select and track columns as they flow through the lineage. For example, if you select the column Full Name, you can see how the Full Name field was created and where the information comes from.

 Note

The lineage view is a powerful way to understand the transformation process an asset has undergone. Learn more about the lineage experience in Microsoft Purview data catalog

Asset contacts and related assets

Asset contacts provide you contact details of experts or dataset owners with any questions. As a new analyst searching for the right data sources for your report, you may find these individuals helpful.

If needed, you can also navigate through the technical hierarchy of assets that are related to the current asset you're viewing.

The ability to search the Microsoft Purview data catalog has the potential to break down data silos and enable the next level of enterprise analytics.




Introduction - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/discover-trusted-data-use-azure-purview/1-introduction
Introduction
2 minutes

Microsoft Purview is a unified data governance service that helps you manage and govern your on-premises, multi-cloud, and software-as-a-service (SaaS) data. Data professionals can easily create a holistic, up-to-date map of the entire data landscape. Microsoft Purview includes automated data discovery, sensitive data classification, and end-to-end data lineage. Microsoft Purview can empower data analysts and other data consumers to find valuable, trustworthy data.

Imagine that you're a new Data Analyst at Contoso. In your second week, the sales manager desperately asks for the latest inventory and sales data for an impromptu review. After clarifying the requirements, you know you need to quickly find accurate assets to create a report. With the help of Microsoft Purview data catalog, you'll be able to search, browse, and discover assets. More importantly, you'll be able to validate that youre using the right data sources for your reports.

Learning objectives

In this module, you will:

Browse and search data catalog assets.
Use data catalog assets with Power BI.
Use Microsoft Purview in Azure Synapse Studio.




Discover trusted data using Microsoft Purview - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/discover-trusted-data-use-azure-purview/

Discover trusted data using Microsoft Purview
Module
7 Units
Feedback
Beginner
Data Analyst
Data Engineer
Power BI
Microsoft Purview

Use Microsoft Purview Studio to discover trusted organizational assets for reporting.

Learning objectives

After completing this module, you'll be able to:

Browse, search, and manage data catalog assets.
Use data catalog assets with Power BI.
Use Microsoft Purview in Azure Synapse Studio.
Add
Prerequisites
Experience using the Azure data ecosystem.
Introduction
min
Search for assets
min
Browse assets
min
Use assets with Power BI
min
Integrate with Azure Synapse Analytics
min
Knowledge check
min
Summary
min


lineage-end-end-expanded.png (1908817)
https://learn.microsoft.com/en-us/training/modules/intro-to-microsoft-purview/media/lineage-end-end-expanded.png#lightbox


scan-rule-sets-expanded.png (1733607)
https://learn.microsoft.com/en-us/training/modules/intro-to-microsoft-purview/media/scan-rule-sets-expanded.png#lightbox


where-is-data-expanded.png (936526)
https://learn.microsoft.com/en-us/training/modules/intro-to-microsoft-purview/media/where-is-data-expanded.png#lightbox


summary - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/intro-to-microsoft-purview/6-summary
Summary
1 minute

Our goal was to help you evaluate whether Microsoft Purview is the right choice to help you manage your enterprise data environment and its various data sources. We looked at how to:

Register data sources.
Map data sources.
Scan data in your sources.
Explore metadata and classification of the data.

We covered how you can use the Microsoft Purview governance portal to register data sources and create a data map. Setting up scanning causes Microsoft Purview to scan through the selected data types in the sources and list metadata associated with those sources. This metadata documents the expected usage to help users discover what's contained in the data sources. We also showed how the metadata includes classification information to help identify sensitive data.

These criteria help you evaluate how Microsoft Purview can help your business catalog data for users and data producers. We also showed how Microsoft Purview can help your company meet its data governance needs by using the metadata and classification features.

References

For more information, see:

Introduction to Microsoft Purview
Map your data estate with Microsoft Purview
Microsoft Purview for unified data governance
Enable unified data governance with Microsoft Purview (video)
Deploy Microsoft Purview and scan an Azure Data Lake resource using the Azure portal




Knowledge check - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/intro-to-microsoft-purview/5-knowledge-check
Knowledge check
15 minutes

Choose the best response for each of the following questions, and then select Check your answers.

1. 

What does Microsoft Purview do with the data it discovers from your registered sources?

 

It catalogs and classifies the data that's scanned.

It moves the data to your Azure subscription, automatically creating the necessary storage accounts.

It performs data transformations to match your on-premises schemas.

2. 

Where would you register your data sources for use in Microsoft Purview?

 

On the Overview tab of the Microsoft Purview account page.

On the Managed Resources tab of the Microsoft Purview account page.

In the Microsoft Purview governance portal.

3. 

What aspect of Microsoft Purview is used to configure the data discovery for your data sources?

 

Scan rules

Collections

Classifications

Check your answers




when-to-use-microsoft-purview - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/intro-to-microsoft-purview/4-when-to-use-microsoft-purview
When to use Microsoft Purview
5 minutes

In this unit, we discuss how you can decide whether Microsoft Purview is the right choice for your data governance and discovery needs. The criteria that indicate whether Microsoft Purview will meet your requirements are:

Discovery
Governance

Let's take a look at these criteria and see how Microsoft Purview can help address their needs.

Discovery

Without a central location to register data sources, you might be unaware of a data source unless you come into contact with it as part of another process.

Unless you know the location of a data source, you can't connect to the data by using a client application. You're required to know the connection string or path.

The intended use of the data is hidden to you unless you know the location of a data source's documentation. Data sources and documentation might live in several places and be utilized through different kinds of experiences.

Governance

As the data in your organization grows, the task of discovering, protecting, and governing that data becomes more difficult. Data is stored in different locations, which might be required for compliance reasons. The data might contain sensitive information such as credit card numbers, social security numbers, or other personal information.

Compliance with company security policies, government regulations, and customer needs are critical considerations for data governance. Understanding which data sources contain sensitive information is key to knowing where protections are needed and how to guard against access to this sensitive data.

Apply the criteria

Let's take a look at how Microsoft Purview can address the data discovery and governance criteria.

Does Microsoft Purview help with data discovery?

Do you require a solution or centralized location to register data sources? Often, users might be unaware of a data source unless they come into contact with it as part of another process. Microsoft Purview can help provide a solution.

After you've registered data sources in the Microsoft Purview governance portal and displayed them in the data map, you can set up scanning of those data sources. The metadata that's returned catalogs the data in those sources. In this way, it's easier for users to discover what the data sources contain. The metadata is indexed to make each data source easy to discover via search. It's also more understandable to the users who discover it.

Users can contribute to the catalog by tagging, documenting, and annotating data sources that have already been registered. They can register new data sources so that other catalog users can discover, understand, and utilize them.

Does Microsoft Purview help with data governance?

Microsoft Purview can scan and automatically classify data in files and tables. Microsoft Purview classifies data by Bloom Filter and RegEx. Bloom Filter classifications include attributes for city, country/region, place, and person information. RegEx classifications cover attributes that include categories like bank information (ABA routing numbers or country/region-specific banking account numbers), passport numbers, and country/region-specific identification numbers. You can find the full list of supported classifications in the documentation for Microsoft Purview.

Microsoft Purview also uses predefined Data Plane roles to help control who has access to the information in Microsoft Purview. For access, users can use the Microsoft Purview governance portal only if they're placed in at least one of the three supported roles. When a Microsoft Purview account is created, no one but the creator can access the account or use its APIs. New users must be put in one or more of the following roles:

Purview Data Reader role: Has access to the Microsoft Purview governance portal and can read all content in Microsoft Purview except for scan bindings.
Purview Data Curator role: Has access to the Microsoft Purview governance portal and can read all content in Microsoft Purview except for scan bindings. Can edit information about assets, classification definitions, and glossary terms. Can also apply classifications and glossary terms to assets.
Purview Data Source Administrator role: Doesn't have access to the Microsoft Purview governance portal (unless the user is also in the Data Reader or Data Curator roles). Can manage all aspects of scanning data into Microsoft Purview. Doesn't have read or write access to content in Microsoft Purview beyond those tasks related to scanning.

These roles are assigned by using the collections where your data sources are registered. You can grant users access to the data they might need without granting them access to the entire data estate. By assigning roles, you can promote resource discoverability while still protecting sensitive information.




how-azure-purview-works - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/intro-to-microsoft-purview/3-how-microsoft-purview-works
How Microsoft Purview works
5 minutes

Here's where we take a look at how Microsoft Purview works. In this unit, you learn the core operational theory behind the functioning of Microsoft Purview for mapping and scanning your data sources. The key areas we focus on include how to:

Load data in the data map.
Browse and search information in the data catalog.
Load data in the data map

The Microsoft Purview Data Map is a unified map of your data assets and their relationships. As one cohesive map, it's easier for you and your users to visualize and govern. It also houses the metadata that underpins the Microsoft Purview Data Catalog and Data Estate Insights. The Data Map scales up and down to meet your enterprise compliance requirements. You can use it to govern your data estate in a way that makes the most sense for your business.

Source data

Sourcing your data starts with a process where you register data sources. Microsoft Purview supports an array of data sources that span on-premises, multicloud, and software-as-a-service (SaaS) options. You register the various data sources so that Microsoft Purview is aware of them. The data remains in its location and isn't migrated to any other platform.

After you have a Microsoft Purview service configured in Azure, you use the Microsoft Purview governance portal to register your data sources.

Each type of data source you choose requires specific information to complete the registration. For example, if your data sources reside in your Azure subscription, you choose the necessary subscription and storage account name. The following image is an example of choosing an Azure Blob Storage source.

After registration, you scan the data source. Scanning ingests metadata about your data source into the Microsoft Purview Data Map. Each data source has specific requirements for authenticating and configuration to permit scanning of the assets in that data source.

For example, if you have data stored in an Amazon S3 standard bucket, you'll need to provide a configuration for the connection. For this service, you use Microsoft Purview to provide a Microsoft account with secure access to AWS, where the Microsoft Purview scanner will run. The Microsoft Purview scanner uses this access to your Amazon S3 buckets to read your data. The scanner then reports the results (including only the metadata and classification) back to Azure. You can use the Microsoft Purview classification and labeling reports to analyze and review your data scan results.

 Note

Check the Microsoft Purview connector for Amazon S3 documentation for region support related to AWS S3 sources.

In Microsoft Purview, there are a few options to use for authentication when the service needs to scan data sources. Some of these options are:

Microsoft Purview managed identity
Account key (using Azure Key Vault)
SQL authentication (using Key Vault)
Service principal (using Key Vault)
Map data

The data map is the foundational platform for Microsoft Purview. The data map consists of:

Data assets.
Data lineage.
Data classifications.
Business context.

Customers create a knowledge graph of data that comes in from a range of sources. Microsoft Purview makes it easy to register and automatically scan and classify data at scale. Within a data map, you can identify the type of data source, along with other details around security and scanning.

The data map uses collections to organize these details. Collections are a way of grouping data assets into logical categories to simplify management and discovery of assets within the catalog. You also use collections to manage access to the metadata that's available in the data map.

Select Map view in the Microsoft Purview governance portal to display the data sources in a graphical view, along with the collections you created for them.

Scan data

After you register your data sources, you'll need to run a scan to access their metadata and browse the asset information. Before you can scan the data sources, you're required to enter the credentials for these sources. You can use Azure Key Vault to store the credentials for security and ease of access by your scan rules. The Microsoft Purview governance portal comes with existing system scan rule sets that you can select when you create a new scan rule. You can also specify a custom scan rule set.

A scan rule set is a container for grouping scan rules together to use the same rules repeatedly. A scan rule set lets you select file types for schema extraction and classification. It also lets you define new custom file types. You might create a default scan rule set for each of your data source types. Then you can use these scan rule sets by default for all scans within your company.

For example, you might want to scan only the .csv files in an Azure Data Lake Storage account. Or you might want to check your data only for credit card numbers rather than all the possible classifications. You might also want users with the right permissions to create other scan rule sets with different configurations based on business needs.

Classification

Metadata is used to help describe the data that's being scanned and made available in the catalog. During the configuration of a scan set, you can specify classification rules to apply during the scan that also serve as metadata. The classification rules fall under five major categories:

Government: Attributes such as government identity cards, driver license numbers, and passport numbers.
Financial: Attributes such as bank account numbers or credit card numbers.
Personal: Personal information such as a person's age, date of birth, email address, and phone number.
Security: Attributes like passwords that can be stored.
Miscellaneous: Attributes not included in the other categories.

You can use several system classifications to classify your data. These classifications align with the sensitive information types in the Microsoft Purview compliance portal. You can also create custom classifications to identify other important or sensitive information types in your data estate.

After you register a data source, you can enrich its metadata. With proper access, you can annotate a data source by providing descriptions, ratings, tags, glossary terms, identifying experts, or other metadata for requesting data-source access. This descriptive metadata supplements the structural metadata, such as column names and data types, that's registered from the data source.

Discovering and understanding data sources and their use is the primary purpose of registering the sources. If you're an enterprise user, you might need data for business intelligence, application development, data science, or any other task where the right data is required. You can use the data catalog discovery experience to quickly find data that matches your needs. You can evaluate the data for its fitness for the purpose and then open the data source in your tool of choice.

At the same time, you can contribute to the catalog by tagging, documenting, and annotating data sources that have already been registered. You can also register new data sources, which are then discovered, evaluated, and used by the community of catalog users.

Browse and search

Microsoft Purview allows you to search information from the data map by using the Microsoft Purview Data Catalog. You can perform text-based search and browse through results by using filters like data source type, tags, ratings, or collection.

You can use business context to search information from the Microsoft Purview catalog. You can define business glossaries and bulk import existing ones, too. You can also apply business context onto assets in the data map. By using a metamodel, you can define business processes in your environment and associate your data sources with those processes. Users can then apply these business contexts to browse and search for information in the data catalog.

Discovery enables you to use:

Semantic search and browse.
Business glossary and workflows.
Data lineage with sources, owners, transformations, and lifecycle.

Data lineage

The concept of data lineage focuses on the lifecycle of data. The lifecycle concerns itself with the various stages data might go through. Data is sourced, moved, and stored throughout its lifecycle. Data might also undergo transformations in the extract, load, and transform/extract, transform, and load (ELT/ETL) operations.

Data lineage can offer insights into the data lifecycle by looking at the data pipeline. You can use the lineage to identify the root cause of data issues, perform data quality analysis, and verify compliance.

Microsoft Purview represents this data lineage in a visual form by showing data movement from source to destination.




what-is-Microsoft-Purview - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/intro-to-microsoft-purview/2-what-is-microsoft-purview
What is Microsoft Purview?
3 minutes

Let's start with a few definitions and a quick tour of the core features of Microsoft Purview.

What's Microsoft Purview?

Microsoft Purview is a unified data-governance service that helps you manage and govern your on-premises, multicloud, and software-as-a-service (SaaS) data. You can easily create a broad, up-to-date map of your data landscape with:

Automated data discovery.
Sensitive data classification.
End-to-end data lineage.

You can also empower data users to find valuable, trustworthy data.

Microsoft Purview is designed to help enterprises get the most value from their existing information assets. With this cloud-based service, you can register your data sources to help you discover and manage them. Your data sources remain in place, but a copy of the metadata for the source is added to Microsoft Purview.

You can register a wide range of sources in Azure and across your multicloud data estate in Microsoft Purview. These sources include Azure Data Lake Storage, AWS, Azure SQL Database on-premises and in the cloud, and many more.

Microsoft Purview has three main elements:

Microsoft Purview Data Map: The data map provides a structure for your data estate in Microsoft Purview, where you can map your existing data stores into groups and hierarchies. In the data map, you can grant users and teams access to these groups so that they have access to find relevant data stores. The data map can then scan your data stores and gather metadata such as schemas and data types. It can also identify sensitive data types so that you can keep track of them in your data estate.

Microsoft Purview Data Catalog: The data catalog allows your users to browse the metadata stored in the data map so that they can find reliable data and understand its context. For example, users can see where the data comes from and who are the experts they can contact about that data source. The data catalog also integrates with other Azure products, like the Azure Synapse Analytics workspace, so that users can search for the data they need from the applications they need it in.

Microsoft Purview Data Estate Insights: Insights offer a high-level view into your data catalog, covering these key facets:

Data stewardship: A report on how curated your data assets are so that you can track your governance progress.
Catalog adoption: A report on the number of active users in your data catalog, their top searches, and your most viewed assets.
Asset insights: A report on the data estate and source-type distribution. You can view by source type, classification, and file size. View the insights as a graph or as key performance indicators.
Scan insights: A report that provides information on the health of your scans (successes, failures, or canceled).
Glossary insights: A status report on the glossary to help users understand the distribution of glossary terms by status, and view how the terms are attached to assets.
Classification insights: A report that shows where classified data is located. It allows security administrators to understand the types of information found in their organization's data estate.
Sensitivity insights: A report that focuses on sensitivity labels found during scans. Security administrators can make use of this information to ensure security is appropriate for the data estate.




introduction - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/intro-to-microsoft-purview/1-introduction
Introduction
2 minutes

As the volume and variety of data increases, the challenges of good data governance are likely to become more difficult. Digital transformation technologies have resulted in new data sources. How do users know what data is available? How do administrators manage data when they might not know what type of data exists and where it's stored? Does the data contain sensitive or personal information?

All these questions aren't easy to answer without insights into the data and the source of storage. Before you can develop data-governance plans for usage and storage, you need to understand the data your organization uses.

Example scenario

As a user or producer of data, you might be a business or technical data analyst, data scientist, or data engineer. You probably spend significant time on manual processes to annotate, catalog, and find trusted data sources.

Without a central location to register data sources, you might be unaware of a data source unless you come into contact with it as part of another process.

Writing metadata descriptions for data sources is often a wasted effort. Client applications typically ignore descriptions that are stored in the data source. Creating documentation for data sources is difficult because you must keep documentation in sync with data sources. Users also might not trust documentation that they think is out of date.

Without the ability to track data from end to end, you must spend time tracing problems created by data pipelines that other teams own. If you make changes to your datasets, you can accidentally affect related reports that are business or mission critical.

Microsoft Purview is designed to address these issues and help enterprises get the most value from their existing information assets. Its catalog makes data sources easy to discover and understand by the users who manage the data.

What will we be doing?

This high-level overview of Microsoft Purview helps you discover the key aspects that make it the tool of choice for mapping out your enterprise data. You learn how it can help you:

Manage and govern your data across various platforms and locations.
Map out your data landscape.
Classify sensitive data.
Empower customers to find trustworthy data.
What's the main goal?

By the end of this session, you'll be able to decide whether Microsoft Purview is the right choice to help you manage your enterprise data environment and your various data sources.




Introduction to Microsoft Purview - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/intro-to-microsoft-purview/

Introduction to Microsoft Purview
Module
6 Units
Feedback
Beginner
Developer
Data Analyst
Data Analyst
Data Scientist
Database Administrator
Administrator
Solution Architect
AI Engineer
Microsoft Purview

In this module, you'll evaluate whether Microsoft Purview is the right choice for your data discovery and governance needs.

Learning objectives

By the end of this module, you'll be able to:

Evaluate whether Microsoft Purview is appropriate for your data discovery and governance needs.
Describe how the features of Microsoft Purview work to provide data discovery and governance.
Add
Prerequisites
Knowledge of Azure accounts and services
Knowledge of various data sources such as SQL Server and Azure Cosmos DB
Knowledge of the concepts around data governance
Introduction
min
What is Microsoft Purview?
min
How Microsoft Purview works
min
When to use Microsoft Purview
min
Knowledge check
min
Summary
min


Summary - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/visualize-real-time-data-azure-stream-analytics-power-bi/7-summary
Summary
1 minute

In this module, you've learned how to integrate Azure Stream Analytics and Microsoft Power BI to generate real-time data visualizations.

By using a Power BI output in Azure Stream Analytics, you can send the results of a stream processing query to a dataset in Power BI, from which you can create real-time dashboard visualizations. You can also use the dataset to support reports that reflect the latest data when rendered.

 Note

To learn more about real-time visualization in Power BI, see Real-time streaming in Power BI in the Power BI documentation.




Knowledge check - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/visualize-real-time-data-azure-stream-analytics-power-bi/6-knowledge-check
Knowledge check
3 minutes
Check your knowledge
1. 

Which type of Azure Stream Analytics output should you use to support real-time visualizations in Microsoft Power BI?

 

Azure Synapse Analytics

Azure Event Hubs

Power BI

2. 

You want to use an output to write the results of a Stream Analytics query to a table named device-events in a dataset named realtime-data in a Power BI workspace named analytics workspace. What should you do?

 

Create only the workspace. The dataset and table will be created automatically.

Create the workspace and dataset. The table will be created automatically.

Create the workspace, dataset, and table before creating the output.

3. 

You want to create a visualization that updates dynamically based on a table in a streaming dataset in Power BI. What should you do?

 

Create a report from the dataset.

Create a dashboard with a tile based on the streaming dataset.

Export the streaming dataset to Excel and create a report from the Excel workbook.

Check your answers




Exercise - Create a real-time data visualization - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/visualize-real-time-data-azure-stream-analytics-power-bi/5-exercise-create-real-time-data-visualization
Exercise - Create a real-time data visualization
45 minutes

Now it's your opportunity to use Azure Stream Analytics and Power BI to create a real-time data visualization.

 Note

To complete this lab, you will need an Azure subscription in which you have administrative access. You'll also need access to the Microsoft Power BI service. Your school or organization may already provide this, or you can sign up for the Power BI service as an individual.

Launch the exercise and follow the instructions.




Create real-time data visualizations in Power BI - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/visualize-real-time-data-azure-stream-analytics-power-bi/4-create-real-time-data-visualizations-power-bi
Create real-time data visualizations in Power BI
5 minutes

When you successfully run an Azure Stream Analytics job that sends results to a Power BI output, a streaming dataset containing a single table is created in the Power BI workspace specified for the output. The table contains the data produced by the Stream Analytics query.

Creating real-time visualizations in a dashboard

To visualize data in real-time, you can create a dashboard with a real-time visualization tile. Real-time visualizations on a dashboard show data from a streaming dataset, and are updated dynamically as new data flows into the dataset.




Create a query for real-time visualization - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/visualize-real-time-data-azure-stream-analytics-power-bi/3-realtime-query
Create a query for real-time visualization
6 minutes

To send streaming data to Power BI, your Azure Stream Analytics job uses a query that writes its results to a Power BI output. A simple query that forwards event data from an event hub directly to Power BI might look something like this:

SELECT
    EventEnqueuedUtcTime AS ReadingTime,
    SensorID,
    ReadingValue
INTO
    [powerbi-output]
FROM
    [eventhub-input] TIMESTAMP BY EventEnqueuedUtcTime


The results of the query determine the schema of the table in the output dataset in Power BI.

Alternatively, you might use your query to filter and/or aggregate the data, sending only relevant or summarized data to the Power BI dataset. For example, the following query calculates the maximum reading for each sensor other than sensor 0 for each consecutive minute in which an event occurs.

SELECT
    DateAdd(second, -60, System.TimeStamp) AS StartTime,
    System.TimeStamp AS EndTime,
    SensorID,
    MAX(ReadingValue) AS MaxReading
INTO
    [powerbi-output]
FROM
    [eventhub-input] TIMESTAMP BY EventEnqueuedUtcTime
WHERE SensorID <> 0
GROUP BY SensorID, TumblingWindow(second, 60)
HAVING COUNT(*) > 1


When working with window functions (such as the TumblingWindow function in the previous example), consider that Power BI is capable of handling a call every second. Additionally, streaming visualizations support packets with a maximum size of 15 KB. As a general rule, use window functions to ensure data is sent to Power BI no more frequently than every second, and minimize the fields included in the results to optimize the size of the data load.

 Note

For more information about Power BI output limitations, see Power BI output from Azure Stream Analytics in the Azure Stream Analytics documentation.




Use a Power BI output in Azure Stream Analytics - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/visualize-real-time-data-azure-stream-analytics-power-bi/2-power-bi-output
Use a Power BI output in Azure Stream Analytics
6 minutes

All Azure Stream Analytics jobs include at least one input and output. In most cases, inputs reference sources of streaming data (though you can also define inputs for static reference data to augment the streamed event data). Outputs determine where the results of the stream processing query will be sent. To support real-time data visualization, you can use a Power BI output.

Streaming data inputs

Inputs for streaming data consumed by Azure Stream Analytics can include:

Azure Event Hubs
Azure IoT Hubs
Azure Blob or Data Lake Gen 2 Storage

Depending on the specific input type, the data for each streamed event includes the event's data fields and input-specific metadata fields. For example, data consumed from an Azure Event Hubs input includes an EventEnqueuedUtcTime field indicating the time when the event was received in the event hub.

 Note

For more information about streaming inputs, see Stream data as input into Stream Analytics in the Azure Stream Analytics documentation.

Power BI outputs

You can use a Power BI output to write the results of a Stream Analytics query to a table in a Power BI streaming dataset, from where it can be visualized in a dashboard. When adding a Power BI output to a Stream Analytics job, you need to specify the following properties:

Output alias: A name for the output that can be used in a query.
Group workspace: The Power BI workspace in which you want to create the resulting dataset.
Dataset name: The name of the dataset to be generated by the output. You shouldn't pre-create this dataset as it will be created automatically (replacing any existing dataset with the same name).
Table name: The name of the table to be created in the dataset.
Authorize connection: You must authenticate the connection to your Power BI tenant so that the Stream Analytics job can write data to the workspace.

 Note

For more information about Power BI outputs, see Power BI output from Azure Stream Analytics in the Azure Stream Analytics documentation.




Introduction - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/visualize-real-time-data-azure-stream-analytics-power-bi/1-introduction
Introduction
2 minutes

Microsoft Power BI is used by organizations all around the world to create dynamic, interactive data visualizations that reveal insights on which important business decisions are based. Access to timely data can be the difference between failure and success, so the ability to capture and visualize data in real-time, or as near as possible, is critical in many scenarios.

Azure Stream Analytics provides a way to process a stream of real-time data from an input such as Azure Event Hubs, and direct the results to an output. One possible output is a Power BI dataset, from which dashboards can consume data for real-time visualization.

In this module, we'll examine how to use Azure Stream Analytics to process a stream of real-time data, and send the results to a Power BI dataset for visualization.




Visualize real-time data with Azure Stream Analytics and Power BI - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/visualize-real-time-data-azure-stream-analytics-power-bi/

Visualize real-time data with Azure Stream Analytics and Power BI
Module
7 Units
Feedback
Intermediate
Data Analyst
Data Engineer
Azure
Azure Stream Analytics
Power BI

By combining the stream processing capabilities of Azure Stream Analytics and the data visualization capabilities of Microsoft Power BI, you can create real-time data dashboards.

Learning objectives

In this module, you'll learn how to:

Configure a Stream Analytics output for Power BI.
Use a Stream Analytics query to write data to Power BI.
Create a real-time data visualization in Power BI.
Add
Prerequisites

Before starting this module, you should be familiar with Azure Stream Analytics and Microsoft Power BI. Consider completing the following modules first:

Explore fundamentals of data visualization.
Get started with Azure Stream Analytics.
Introduction
min
Use a Power BI output in Azure Stream Analytics
min
Create a query for real-time visualization
min
Create real-time data visualizations in Power BI
min
Exercise - Create a real-time data visualization
min
Knowledge check
min
Summary
min


Summary - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/ingest-streaming-data-use-azure-stream-analytics-synapse/8-summary
Summary
3 minutes

In this module, you explored multiple ways in which you can use Azure Stream Analytics to ingest real-time data into Azure Synapse Analytics.

The ability to include real-time data ingestion in an analytics solution enables organizations to build reports and interactive data models that reflect the most up-to-date information available. Azure Synapse Analytics offers a wide range of analytical capabilities that you can use with both streaming and batch data to create highly scalable solutions for massive volumes of data. To learn more, see the Introduction to Azure Synapse Analytics module on Microsoft Learn.




Knowledge check - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/ingest-streaming-data-use-azure-stream-analytics-synapse/7-knowledge-check
Knowledge check
3 minutes
Check your knowledge
1. 

Which type of output should you use to ingest the results of an Azure Stream Analytics job into a dedicated SQL pool table in Azure Synapse Analytics?

 

Azure Synapse Analytics

Blob storage/ADLS Gen2

Azure Event Hubs

2. 

Which type of output should be used to ingest the results of an Azure Stream Analytics job into files in a data lake for analysis in Azure Synapse Analytics?

 

Azure Synapse Analytics

Blob storage/ADLS Gen2

Azure Event Hubs

Check your answers




Exercise - Ingest streaming data into Azure Synapse Analytics - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/ingest-streaming-data-use-azure-stream-analytics-synapse/6-exercise-ingest-streaming-data
Exercise - Ingest streaming data into Azure Synapse Analytics
45 minutes

Now it's your opportunity to use Azure Stream Analytics to ingest real-time data into Azure Synapse Analytics.

 Note

To complete this lab, you will need an Azure subscription in which you have administrative access.

Launch the exercise and follow the instructions.




Run a job to ingest data - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/ingest-streaming-data-use-azure-stream-analytics-synapse/5-run-job-ingest
Run a job to ingest data
3 minutes

When you've created and saved your query, you can run the Azure Stream Analytics job to process events in the input(s) and write the results to output(s). Once started, the query will run perpetually until stopped; constantly ingesting new event data into your Azure Synapse Analytics workspace (into a table in relational data warehouse or files in a data lake, depending on the output type).

Working with ingested data

You can work with the ingested streaming data like any other data in Azure Synapse Analytics, combining it with data ingested using batch processing techniques or synchronized from operational data sources by using Azure Synapse Link.

Querying data in a relational data warehouse

If you used an Azure Synapse Analytics output to ingest the results of your stream processing job into a table in a dedicated SQL pool, you can query the table using a SQL query, just like any other table. The results of the query will always include the latest data to be ingested at the time the query is run. Your data warehouse can include tables for streaming data as well as tables for batch ingested data, enabling you to join real-time and batch data for historical analytics.

For example, the following SQL code could be used to query a table named factSensorReadings that contains the results of stream processing, and combine it with a dimDate table containing detailed data about the dates on which readings were captured.

SELECT d.Weekday, s.SensorID, AVG(s.SensorReading) AS AverageReading
FROM factSensorReadings AS s
JOIN dimDate AS d
    ON CAST(s.ReadingTime AS DATE) = d.DateKey
GROUP BY d.Weekday, s.SensorID


 Tip

To Learn more about using a dedicated SQL pool to analyze data in a data warehouse, see the Analyze data in a relational data warehouse module on Microsoft Learn.

Querying data in a data lake

As streaming data is ingested into files in a data lake, you can query those files by using a serverless SQL pool in Azure Synapse Analytics. For example, the following query reads all fields from all Parquet files under the sensors folder in the data file system container.

SELECT *
FROM OPENROWSET(
    BULK 'https://mydatalake.blob.core.windows.net/data/sensors/*',
    FORMAT = 'parquet') AS rows


 Tip

To Learn more about using serverless SQL pools to query files in a data lake, see the Use Azure Synapse serverless SQL pool to query files in a data lake module on Microsoft Learn.

You can also query the data lake by using code running in an Apache Spark pool, as shown in this example:

%%pyspark
df = spark.read.load('abfss://data@datalake.dfs.core.windows.net/sensors/*', format='parquet'
)
display(df)


 Tip

To Learn more about using Apache Spark pools to query files in a data lake, see the Analyze data with Apache Spark in Azure Synapse Analytics module on Microsoft Learn.




Define a query to select, filter, and aggregate data - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/ingest-streaming-data-use-azure-stream-analytics-synapse/4-define-query
Define a query to select, filter, and aggregate data
5 minutes

After defining the input(s) and output(s) for your Azure Stream Analytics job, you can define a query to process the incoming data from an input and write the results to an output.

Selecting input fields

The simplest approach to ingesting streaming data into Azure Synapse Analytics is to capture the required field values for every event using a SELECT...INTO query, as shown here:

SELECT
    EventEnqueuedUtcTime AS ReadingTime,
    SensorID,
    ReadingValue
INTO
    [synapse-output]
FROM
    [streaming-input] TIMESTAMP BY EventEnqueuedUtcTime


 Tip

When using an Azure Synapse Analytics output to write the results to a table in a dedicated SQL pool, the schema of the results produced by the query must match the table into which the data is to be loaded. You can use AS clauses to rename fields, and cast them to alternative (compatible) data types as necessary.

Filtering event data

In some cases, you might want to filter the data to include only specific events by adding a WHERE clause. For example, the following query writes data only for events with a negative ReadingValue field value.

SELECT
    EventEnqueuedUtcTime AS ReadingTime,
    SensorID,
    ReadingValue
INTO
    [synapse-output]
FROM
    [streaming-input] TIMESTAMP BY EventEnqueuedUtcTime
WHERE ReadingValue < 0

Aggregating events over temporal windows

A common pattern for streaming queries is to aggregate event data over temporal (time-based) intervals, or windows. To accomplish this, you can use a GROUP BY clause that includes a Window function defining the kind of window you want to define (for example, tumbling, hopping, or sliding).

 Tip

For more information about window functions, see Introduction to Stream Analytics windowing functions in the Azure Stream Analytics documentation.

The following example groups streaming sensor readings into 1 minute tumbling (serial, non-overlapping) windows, recording the start and end time of each window and the maximum reading for each sensor. The HAVING clause filters the results to include only windows where at least one event occurred.

SELECT
    DateAdd(second, -60, System.TimeStamp) AS StartTime,
    System.TimeStamp AS EndTime,
    SensorID,
    MAX(ReadingValue) AS MaxReading
INTO
    [synapse-output]
FROM
    [streaming-input] TIMESTAMP BY EventEnqueuedUtcTime
GROUP BY SensorID, TumblingWindow(second, 60)
HAVING COUNT(*) >= 1


 Tip

For more information about common patters for queries, see Common query patterns in Azure Stream Analytics in the Azure Stream Analytics documentation.




Configure inputs and outputs - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/ingest-streaming-data-use-azure-stream-analytics-synapse/3-configure-inputs-outputs
Configure inputs and outputs
9 minutes

All Azure Stream Analytics jobs include at least one input and output. In most cases, inputs reference sources of streaming data (though you can also define inputs for static reference data to augment the streamed event data). Outputs determine where the results of the stream processing query will be sent. In the case of data ingestion into Azure Synapse Analytics, the output usually references an Azure Data Lake Storage Gen2 container or a table in a dedicated SQL pool database.

Streaming data inputs

Inputs for streaming data consumed by Azure Stream Analytics can include:

Azure Event Hubs
Azure IoT Hubs
Azure Blob or Data Lake Gen 2 Storage

Depending on the specific input type, the data for each streamed event includes the event's data fields as well as input-specific metadata fields. For example, data consumed from an Azure Event Hubs input includes an EventEnqueuedUtcTime field indicating the time when the event was received in the event hub.

 Note

For more information about streaming inputs, see Stream data as input into Stream Analytics in the Azure Stream Analytics documentation.

Azure Synapse Analytics outputs

If you need to load the results of your stream processing into a table in a dedicated SQL pool, use an Azure Synapse Analytics output. The output configuration includes the identity of the dedicated SQL pool in an Azure Synapse Analytics workspace, details of how the Azure Stream Analytics job should establish an authenticated connection to it, and the existing table into which the data should be loaded.

Authentication to Azure Synapse Analytics is usually accomplished through SQL Server authentication, which requires a username and password. Alternatively, you can use a managed identity to authenticate. When using an Azure Synapse Analytics output, your Azure Stream Analytics job configuration must include an Azure Storage account in which authentication metadata for the job is stored securely.

 Note

For more information about using an Azure Synapse Analytics output, see Azure Synapse Analytics output from Azure Stream Analytics in the Azure Stream Analytics documentation.

Azure Data Lake Storage Gen2 outputs

If you need to write the results of stream processing to an Azure Data Lake Storage Gen2 container that hosts a data lake in an Azure Synapse Analytics workspace, use a Blob storage/ADLS Gen2 output. The output configuration includes details of the storage account in which the container is defined, authentication settings to connect to it, and details of the files to be created. You can specify the file format, including CSV, JSON, Parquet, and Delta formats. You can also specify custom patterns to define the folder hierarchy in which the files are saved - for example using a pattern such as YYYY/MM/DD to generate a folder hierarchy based on the current year, month, and day.

You can specify minimum and maximum row counts for each batch, which determines the number of output files generated (each batch creates a new file). You can also configure the write mode to control when the data is written for a time window - appending each row as it arrives or writing all rows once (which ensures "exactly once" delivery).

 Note

For more information about using a Blob storage/ADLS Gen2 output, see Blob storage and Azure Data Lake Gen2 output from Azure Stream Analytics in the Azure Stream Analytics documentation.




Stream ingestion scenarios - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/ingest-streaming-data-use-azure-stream-analytics-synapse/2-stream-ingestion-scenarios
Stream ingestion scenarios
5 minutes

Azure Synapse Analytics provides multiple ways to analyze large volumes of data. Two of the most common approaches to large-scale data analytics are:

Data warehouses - relational databases, optimized for distributed storage and query processing. Data is stored in tables and queried using SQL.
Data lakes - distributed file storage in which data is stored as files that can be processed and queried using multiple runtimes, including Apache Spark and SQL.
Data warehouses in Azure Synapse Analytics

Azure Synapse Analytics provides dedicated SQL pools that you can use to implement enterprise-scale relational data warehouses. Dedicated SQL pools are based on a massively parallel processing (MPP) instance of the Microsoft SQL Server relational database engine in which data is stored and queried in tables.

To ingest real-time data into a relational data warehouse, your Azure Stream Analytics query must write its results to an output that references the table into which you want to load the data.

Data lakes in Azure Synapse Analytics

An Azure Synapse Analytics workspace typically includes at least one storage service that is used as a data lake. Most commonly, the data lake is hosted in an Azure Storage account using a container configured to support Azure Data Lake Storage Gen2. Files in the data lake are organized hierarchically in directories (folders), and can be stored in multiple file formats, including delimited text (such as comma-separated values, or CSV), Parquet, and JSON.

When ingesting real-time data into a data lake, your Azure Stream Analytics query must write its results to an output that references the location in the Azure Data Lake Gen2 storage container where you want to save the data files. Data analysts, engineers, and scientists can then process and query the files in the data lake by running code in an Apache Spark pool, or by running SQL queries using a serverless SQL pool.




Introduction - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/ingest-streaming-data-use-azure-stream-analytics-synapse/1-introduction
Introduction
1 minute

Suppose a retail company captures real-time sales transaction data from an e-commerce website, and wants to analyze this data along with more static data related to products, customers, and employees. A common way to approach this problem is to ingest the stream of real-time data into a data lake or data warehouse, where it can be queried together with data that is loaded using batch processing techniques.

Microsoft Azure Synapse Analytics provides a comprehensive enterprise data analytics platform, into which real-time data captured in Azure Event Hubs or Azure IoT Hub, and processed by Azure Stream Analytics can be loaded.

A typical pattern for real-time data ingestion in Azure consists of the following sequence of service integrations:

A real-time source of data is captured in an event ingestor, such as Azure Event Hubs or Azure IoT Hub.
The captured data is perpetually filtered and aggregated by an Azure Stream Analytics query.
The results of the query are loaded into a data lake or data warehouse in Azure Synapse Analytics for subsequent analysis.

In this module, you'll explore multiple ways in which you can use Azure Stream Analytics to ingest real-time data into Azure Synapse Analytics.




Ingest streaming data using Azure Stream Analytics and Azure Synapse Analytic - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/ingest-streaming-data-use-azure-stream-analytics-synapse/

Ingest streaming data using Azure Stream Analytics and Azure Synapse Analytics
Module
8 Units
Feedback
Intermediate
Data Engineer
Azure Stream Analytics
Azure Synapse Analytics

Azure Stream Analytics provides a real-time data processing engine that you can use to ingest streaming event data into Azure Synapse Analytics for further analysis and reporting.

Learning objectives

After completing this module, you'll be able to:

Describe common stream ingestion scenarios for Azure Synapse Analytics.
Configure inputs and outputs for an Azure Stream Analytics job.
Define a query to ingest real-time data into Azure Synapse Analytics.
Run a job to ingest real-time data, and consume that data in Azure Synapse Analytics.
Add
Prerequisites

Before starting this module, you should be familiar with Azure Stream Analytics and Azure Synapse Analytics. Consider completing the following modules first:

Get started with Azure Stream Analytics
Introduction to Azure Synapse Analytics
Introduction
min
Stream ingestion scenarios
min
Configure inputs and outputs
min
Define a query to select, filter, and aggregate data
min
Run a job to ingest data
min
Exercise - Ingest streaming data into Azure Synapse Analytics
min
Knowledge check
min
Summary
min


Summary - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/introduction-to-data-streaming/6-summary
Summary
5 minutes

Azure Stream Analytics is a platform-as-a-service (PaaS) solution that you can use to process a perpetual stream of data for real-time reporting, automated action, or integration into an enterprise analytical solution.

In this module, you learned how to:

Understand data streams.
Understand event processing.
Get started with Azure Stream Analytics.

To learn more about the capabilities of Azure Stream Analytics, see the Azure Stream Analytics documentation.




Knowledge check - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/introduction-to-data-streaming/5-knowledge-check
Knowledge check
3 minutes
Check your knowledge
1. 

Which definition of stream processing is correct?

 

Data is processed continually as new data records arrive.

Data is collected in a temporary store, and all records are processed together as a batch.

Data that is incomplete or contains errors is redirected to separate storage for correction by a human operator.

2. 

You need to process a stream of sensor data, aggregating values over one minute windows and storing the results in a data lake. Which service should you use?

 

Azure SQL Database

Azure Cosmos DB

Azure Stream Analytics

3. 

You want to aggregate event data by contiguous, fixed-length, non-overlapping temporal intervals. What kind of window should you use?

 

Sliding

Session

Tumbling

Check your answers




Exercise - Get started with Azure Stream Analytics - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/introduction-to-data-streaming/4-process-events-azure-stream-analytics
Exercise - Get started with Azure Stream Analytics
15 minutes

Now it's your opportunity to explore Azure Stream Analytics in a sample solution that aggregates streaming data events.

 Note

To complete this lab, you will need an Azure subscription in which you have administrative access.

Launch the exercise and follow the instructions.




Understand window functions - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/introduction-to-data-streaming/3b-understand-windows
Understand window functions
6 minutes

A common goal of stream processing is to aggregate events into temporal intervals, or windows. For example, to count the number of social media posts per minute or to calculate the average rainfall per hour.

Azure Stream Analytics includes native support for five kinds of temporal windowing functions. These functions enable you to define temporal intervals into which data is aggregated in a query. The supported windowing functions are Tumbling, Hopping, Sliding, Session, and Snapshot.

Tumbling

Tumbling window functions segment a data stream into a contiguous series of fixed-size, non-overlapping time segments and operate against them. Events can't belong to more than one tumbling window.

The Tumbling window example, represented by the following query, finds the maximum reading value in each one-minute window. Windowing functions are applied in Stream Analytics jobs using the GROUP BY clause of the query syntax. The GROUP BY clause in the following query contains the TumblingWindow() function, which specifies a one-minute window size.

SELECT DateAdd(minute,-1,System.TimeStamp) AS WindowStart,
       System.TimeStamp() AS WindowEnd,
       MAX(Reading) AS MaxReading
INTO
    [output]
FROM
    [input] TIMESTAMP BY EventProcessedUtcTime
GROUP BY TumblingWindow(minute, 1)

Hopping

Hopping window functions model scheduled overlapping windows, jumping forward in time by a fixed period. It's easiest to think of them as Tumbling windows that can overlap and be emitted more frequently than the window size. In fact, tumbling windows are simply a hopping window whose hop is equal to its size. When you use Hopping windows, events can belong to more than one window result set.

To create a hopping window, you must specify three parameters. The first parameter indicates the time unit, such as second, minute, or hour. The following parameter sets the window size, which designates how long each window lasts. The final required parameter is the hop size, which specifies how much each window moves forward relative to the previous one. An optional fourth parameter denoting the offset size may also be used.

The following query demonstrates using a HoppingWindow() where the timeunit is set to second. The windowsize is 60 seconds, and the hopsize is 30 seconds. This query outputs an event every 30 seconds containing the maximum reading value that occurred over the last 60 seconds.

SELECT DateAdd(second,-60,System.TimeStamp) AS WindowStart,
       System.TimeStamp() AS WindowEnd,
       MAX(Reading) AS MaxReading
INTO
    [output]
FROM
    [input] TIMESTAMP BY EventProcessedUtcTime
GROUP BY HoppingWindow(second, 60, 30)


Sliding

Sliding windows generate events for points in time when the content of the window actually changes. This function model limits the number of windows that need to be considered. Azure Stream Analytics outputs events for only those points in time when an event entered or exited the window. As such, every window contains a minimum of one event. Events in Sliding windows can belong to more than one sliding window, similar to Hopping windows.

The following query uses the SlidingWindow() function to find the maximum reading value in each one-minute window in which an event occurred.

SELECT DateAdd(minute,-1,System.TimeStamp) AS WindowStart,
       System.TimeStamp() AS WindowEnd,
       MAX(Reading) AS MaxReading
INTO
    [output]
FROM
    [input] TIMESTAMP BY EventProcessedUtcTime
GROUP BY SlidingWindow(minute, 1)

Session

Session window functions cluster together events that arrive at similar times, filtering out periods of time where there's no data. It has three primary parameters: timeout, maximum duration, and partitioning key (optional).

The occurrence of the first event starts a session window. Suppose another event occurs within the specified timeout from the last ingested event. In that case, the window will be extended to incorporate the new event. However, if no other events occur within the specified timeout period, the window will be closed at the timeout. If events keep happening within the specified timeout, the session window will extend until the maximum duration is reached.

The following query measures user session length by creating a SessionWindow over clickstream data with a timeoutsize of 20 seconds and a maximumdurationsize of 60 seconds.

SELECT DateAdd(second,-60,System.TimeStamp) AS WindowStart,
       System.TimeStamp() AS WindowEnd,
       MAX(Reading) AS MaxReading
INTO
    [output]
FROM
    [input] TIMESTAMP BY EventProcessedUtcTime
GROUP BY SessionWindow(second, 20, 60)

Snapshot

Snapshot windows groups events by identical timestamp values. Unlike other windowing types, a specific window function isn't required. You can employ a snapshot window by specifying the System.Timestamp() function to your query's GROUP BY clause.

For example, the following query finds the maximum reading value for events that occur at precisely the same time.

SELECT System.TimeStamp() AS WindowTime,
       MAX(Reading) AS MaxReading
INTO
    [output]
FROM
    [input] TIMESTAMP BY EventProcessedUtcTime
GROUP BY System.Timestamp()


System.Timestamp() is considered in the GROUP BY clause as a snapshot window definition because it groups events into a window based on the equality of timestamps.




Understand event processing - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/introduction-to-data-streaming/3-understand-event-processing
Understand event processing
5 minutes

Azure Stream Analytics is a service for complex event processing and analysis of streaming data. Stream Analytics is used to:

Ingest data from an input, such as an Azure event hub, Azure IoT Hub, or Azure Storage blob container.
Process the data by using a query to select, project, and aggregate data values.
Write the results to an output, such as Azure Data Lake Gen 2, Azure SQL Database, Azure Synapse Analytics, Azure Functions, Azure event hub, Microsoft Power BI, or others.

Once started, a Stream Analytics query will run perpetually, processing new data as it arrives in the input and storing results in the output.

Stream Analytics guarantees exactly once event processing and at-least-once event delivery, so events are never lost. It has built-in recovery capabilities in case the delivery of an event fails. Also, Stream Analytics provides built-in checkpointing to maintain the state of your job and produces repeatable results. Because Azure Stream Analytics is a platform-as-a-service (PaaS) solution, it's fully managed and highly reliable. Its built-in integration with various sources and destinations and provides a flexible programmability model. The Stream Analytics engine enables in-memory compute, so it offers high performance.

Azure Stream Analytics jobs and clusters

The easiest way to use Azure Stream Analytics is to create a Stream Analytics job in an Azure subscription, configure its input(s) and output(s), and define the query that the job will use to process the data. The query is expressed using structured query language (SQL) syntax, and can incorporate static reference data from multiple data sources to supply lookup values that can be combined with the streaming data ingested from an input.

If your stream process requirements are complex or resource-intensive, you can create a Stream Analysis cluster, which uses the same underlying processing engine as a Stream Analytics job, but in a dedicated tenant (so your processing is not affected by other customers) and with configurable scalability that enables you to define the right balance of throughput and cost for your specific scenario.

Inputs

Azure Stream Analytics can ingest data from the following kinds of input:

Azure Event Hubs
Azure IoT Hub
Azure Blob storage
Azure Data Lake Storage Gen2

Inputs are generally used to reference a source of streaming data, which is processed as new event records are added. Additionally, you can define reference inputs that are used to ingest static data to augment the real-time event stream data. For example, you could ingest a stream of real-time weather observation data that includes a unique ID for each weather station, and augment that data with a static reference input that matches the weather station ID to a more meaningful name.

Outputs

Outputs are destinations to which the results of stream processing are sent. Azure Stream Analytics supports a wide range of outputs, which can be used to:

Persist the results of stream processing for further analysis; for example by loading them into a data lake or data warehouse.
Display a real-time visualization of the data stream; for example by appending data to a dataset in Microsoft Power BI.
Generate filtered or summarized events for downstream processing; for example by writing the results of stream processing to an event hub.
Queries

The stream processing logic is encapsulated in a query. Queries are defined using SQL statements that SELECT data fields FROM one or more inputs, filter or aggregate the data, and write the results INTO an output. For example, the following query filters the events from the weather-events input to include only data from events with a temperature value less than 0, and writes the results to the cold-temps output:

SELECT observation_time, weather_station, temperature
INTO cold-temps
FROM weather-events TIMESTAMP BY observation_time
WHERE temperature < 0


A field named EventProcessedUtcTime is automatically created to define the time when the event is processed by your Azure Stream Analytics query. You can use this field to determine the timestamp of the event, or you can explicitly specify another DateTime field by using the TIMESTAMP BY clause, as shown in this example. Depending on the input from which the streaming data is read, one or more potential timestamp fields may be created automatically; for example, when using an Event Hubs input, a field named EventQueuedUtcTime is generated to record the time when the event was received in the event hub queue.

The field used as a timestamp is important when aggregating data over temporal windows, which is discussed next.




Understand data streams - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/introduction-to-data-streaming/2-understand-data-streams
Understand data streams
5 minutes

A data stream consists of a perpetual series of data, typically related to specific point-in-time events. For example, a stream of data might contain details of messages submitted to a social media micro-blogging site, or a series of environmental measurements recorded by an internet-connected weather sensor. Streaming data analytics is most often used to better understand change over time. For example, a marketing organization may perform sentiment analysis on social media messages to see if an advertising campaign results in more positive comments about the company or its products, or an agricultural business might monitor trends in temperature and rainfall to optimize irrigation and crop harvesting.

Common goals for stream analytics include

Continuously analyzing data to report issues or trends.
Understanding component or system behavior under various conditions to help plan future enhancements.
Triggering specific actions or alerts when certain events occur or thresholds are exceeded.
Characteristics of stream processing solutions

Stream processing solutions typically exhibit the following characteristics:

The source data stream is unbounded - data is added to the stream perpetually.
Each data record in the stream includes temporal (time-based) data indicating when the event to which the record relates occurred (or was recorded).
Aggregation of streaming data is performed over temporal windows - for example, recording the number of social media posts per minute or the average rainfall per hour.
The results of streaming data processing can be used to support real-time (or near real-time) automation or visualization, or persisted in an analytical store to be combined with other data for historical analysis. Many solutions combine these approaches to support both real-time and historical analytics.




Introduction - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/introduction-to-data-streaming/1-introduction
Introduction
1 minute

Today, massive amounts of real-time data are generated by connected applications, Internet of Things (IoT) devices and sensors, and various other sources. The proliferation of streaming data sources has made the ability to consume and make informed decisions from these data in near-real-time an operational necessity for many organizations.

Some typical examples of streaming data workloads include:

Online stores analyzing real-time clickstream data to provide product recommendations to consumers as they browse the website.
Manufacturing facilities using telemetry data from IoT sensors to remotely monitor high-value assets.
Credit card transactions from point-of-sale systems being scrutinized in real-time to detect and prevent potentially fraudulent activities.

Azure Stream Analytics provides a cloud-based stream processing engine that you can use to filter, aggregate, and otherwise process a real-time stream of data from various sources. The results of this processing can then be used to trigger automated activity by a service or application, generate real-time visualizations, or integrate streaming data into an enterprise analytics solution.

In this module, you'll learn how to get started with Azure Stream Analytics, and use it to process a stream of event data.




Get started with Azure Stream Analytics - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/introduction-to-data-streaming/

Get started with Azure Stream Analytics
Module
7 Units
Feedback
Beginner
Data Engineer
Azure
Azure Stream Analytics

Azure Stream Analytics enables you to process real-time data streams and integrate the data they contain into applications and analytical solutions.

Learning objectives

In this module, you'll learn how to:

Understand data streams.
Understand event processing.
Understand window functions.
Get started with Azure Stream Analytics.
Add
Prerequisites

Before starting this module, you should be familiar with Microsoft Azure and have a basic knowledge of data storage and querying using SQL.

Introduction
min
Understand data streams
min
Understand event processing
min
Understand window functions
min
Exercise - Get started with Azure Stream Analytics
min
Knowledge check
min
Summary
min


Summary - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/implement-synapse-link-for-sql/7-summary
Summary
1 minute

Azure Synapse Link for SQL makes it possible to replicate data from SQL Server 2022 or Azure SQL Database to a dedicated pool in Azure Synapse Analytics with low latency. This replication enables you to analyze operational data in near-real-time without incurring a large resource utilization overhead on your transactional data store.

In this module, you learned how to:

Understand key concepts and capabilities of Azure Synapse Link for SQL.
Configure Azure Synapse Link for Azure SQL Database.
Configure Azure Synapse Link for Microsoft SQL Server.




Knowledge check - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/implement-synapse-link-for-sql/6-knowledge-check
Knowledge check
3 minutes
1. 

From which of the following data sources can you use Azure Synapse Link for SQL to replicate data to Azure Synapse Analytics?

 

Azure Cosmos DB

SQL Server 2022

Azure SQL Managed Instance

2. 

What must you create in your Azure Synapse Analytics workspace as a target database for Azure Synapse Link for Azure SQL Database?

 

A serverless SQL pool

An Apache Spark pool

A dedicated SQL pool

3. 

You plan to use Azure Synapse Link for SQL to replicate tables from SQL Server 2022 to Azure Synapse Analytics. What additional Azure resource must you create?

 

An Azure Storage account with an Azure Data Lake Storage Gen2 container

An Azure Key Vault containing the SQL Server admin password

An Azure Application Insights resource

Check your answers




Exercise - Implement Azure Synapse Link for SQL - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/implement-synapse-link-for-sql/5-exercise-synapse-link-sql
Exercise - Implement Azure Synapse Link for SQL
35 minutes

Now it's your chance to explore Azure Synapse Link for SQL for yourself. In this exercise, you'll use a provided script to provision an Azure Synapse Analytics workspace and an Azure SQL Database resource in your Azure subscription; and then you'll enable Azure Synapse Link for Azure SQL Database and use it to synchronize data.

 Note

To complete this lab, you will need an Azure subscription in which you have administrative access.

Launch the exercise and follow the instructions.




Configure Azure Synapse Link for SQL Server 2022 - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/implement-synapse-link-for-sql/4-synapse-link-sql-server
Configure Azure Synapse Link for SQL Server 2022
5 minutes

Microsoft SQL Server is one of the world's most commonly used relational database systems. SQL Server 2022 is the latest release, and includes many enhancements and new features; including the ability to be used as a source for Azure Synapse Link.

Azure Synapse Link for SQL Server uses a link connection to map one or more tables in an Azure SQL Database instance to tables in a dedicated SQL pool in Azure Synapse Analytics. When the link connection is started, the tables are initialized by copying a .parquet file for each source table to a landing zone in Azure Data Lake Storage Gen2; from where the data is imported into tables in the dedicated SQL pool. Subsequently, the change feed process copies all changes as .csv files to the landing zone where they're applied to the target tables.

Synchronization between SQL Server (which can be on-premises or in a private network) and Azure Synapse Analytics is achieved through a self-hosted integration runtime. An integration runtime is a software agent that handles secure connectivity when using Azure Data Factory or Azure Synapse Analytics to transfer data across networks. It must be installed on a Microsoft Windows computer with direct access to your SQL Server instance.

 Tip

For more information about using a self-hosted integration runtime to work with Azure Synapse Analytics, see Create and configure a self-hosted integration runtime.

Implementing Azure Synapse Link for SQL Server 2022

To use Azure Synapse Link for SQL Server 2022, you need to create storage for the landing zone in Azure and configure your SQL Server instance before creating a link connection in Azure Synapse Analytics.

Create landing zone storage

You need to create an Azure Data Lake Storage Gen2 account in your Azure subscription to use as a landing zone. You can't use the default storage for your Azure Synapse Analytics workspace.

 Tip

For more information about provisioning an Azure Data Lake Storage Gen2 account, see Create a storage account to use with Azure Data Lake Storage Gen2.

Create a master key in the SQL Server database

To support Azure Synapse Link, your SQL Server database must contain a master key. You can use a CREATE MASTER KEY SQL statement like the following example to create one:

CREATE MASTER KEY ENCRYPTION BY PASSWORD = 'my$ecretPa$$w0rd';

Create a dedicated SQL pool in Azure Synapse Analytics

In your Azure Synapse Analytics workspace, you need to create a dedicated SQL pool where the target tables will be created. You also need to create master key in this database by using the following SQL statement:

CREATE MASTER KEY

Create a linked service for the SQL Server source database

Next, in Azure Synapse Analytics, create a linked service for your SQL Server database. When you do this, you need to specify the self-hosted integration runtime to be used for connectivity between SQL Server and Azure Synapse Analytics. If you haven't already configured a self-hosted integration runtime, you can create one now, and then download and install the agent onto a Windows machine in the network where your SQL Server instance is located.

Create a linked service for your Data Lake Storage Gen2 account

In addition to the linked service for SQL Server, you need a linked service for the Data Lake Storage Gen2 account that will be used as a landing zone. To support this, you need to add the managed identity of your Azure Synapse Analytics Workspace to the Storage Blob Data Contributor role for your storage account and configure the linked service to use the managed identity for authentication.

Create a link connection for Azure Synapse Link

Finally, you're ready to create a link connection for Azure Synapse Link data synchronization. As you do so, you'll specify the service link for the SQL Server source database, the individual tables to be replicated, the number of CPU cores to be used for the synchronization process, and the Azure Data Lake Storage Gen2 linked service and folder location for the landing zone.

After the link connection is created, you can start it to initialize synchronization. After a short time, the tables will be available to query in the dedicated SQL pool, and will be kept in sync with modifications in the source database by the change feed process.

 Tip

Learn more:

For more information about Synapse Link for SQL Server 2022, see Azure Synapse Link for SQL Server 2022.
To learn about limitations and restrictions that apply to Synapse Link for Azure SQL Database, see Known limitations and issues with Azure Synapse Link for SQL.
For a step-by-step guide to setting up Synapse Link for SQL Server 2022, see Get started with Azure Synapse Link for SQL Server 2022.




Configure Azure Synapse Link for Azure SQL Database - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/implement-synapse-link-for-sql/3-synapse-link-azure-sql
Configure Azure Synapse Link for Azure SQL Database
5 minutes

Azure SQL Database is a platform-as-a-service (PaaS) relational database service based on the SQL Server database engine. It's commonly used in cloud-native applications as a scalable, secure, and easy to manage relational database store for operational data.

Azure Synapse Link for Azure SQL Database uses a link connection to map one or more tables in an Azure SQL Database instance to tables in a dedicated SQL pool in Azure Synapse Analytics. When the link connection is started, the tables are initialized by copying a snapshot of the source tables to the target tables. Subsequently, the change feed process applies all modifications made in the source tables to the target tables.

Implementing Azure Synapse Link for Azure SQL Database

To use Azure Synapse Link for Azure SQL Database, you need to configure some settings in your Azure SQL Database server, before creating a link connection in Azure Synapse Analytics.

Configure Azure SQL Database

Before you can use Azure SQL Database as a source for a linked connection in Azure Synapse Analytics, you must ensure the following settings are configured in the Azure SQL Database server that hosts the database you want to synchronize:

System assigned managed identity - enable this option so that your Azure SQL Database server uses a system assigned managed identity.
Firewall rules - ensure that Azure services can access your Azure SQL Database server.

In addition to these server-level settings, if you plan to configure the link connection from Azure Synapse Analytics to use a managed identity when connecting to Azure SQL Database, you must create a user for the workspace identity in the database and add it to the db_owner role, as shown in the following code example:

CREATE USER my_synapse_workspace FROM EXTERNAL PROVIDER;
ALTER ROLE [db_owner] ADD MEMBER my_synapse_workspace;


 Tip

If you intend to use SQL authentication, you can omit this step.

Prepare the target SQL pool

Azure Synapse Link for Azure SQL Database synchronizes the source data to tables in a dedicated SQL pool in Azure Synapse Analytics. You therefore need to create and start a dedicated SQL pool in your Azure Synapse Analytics workspace before you can create the link connection.

The database associated with the dedicated SQL pool must include the appropriate schema for the target table. If source tables are defined in a schema other than the default dbo schema, you must create a schema of the same name in the dedicated SQL pool database:

CREATE SCHEMA myschema;

Create a link connection

To create a linked connection, add a linked connection on the Integrate page in Azure Synapse Studio. You'll need to:

Select or create a linked service for your Azure SQL Database. You can create this separately ahead of time, or as part of the process of creating a linked connection for Azure Synapse Link. You can use a managed identity or SQL authentication to connect the linked service to Azure SQL Database.
Select the tables in the source database that you want to include in the linked connection.
Select the target dedicated SQL pool in which the target tables should be created.
Specify the number of CPU cores you want to use to process synchronization. Four driver cores will be used in addition to the number of cores you specify.

After creating the linked connection, you can configure the mappings between the source and target tables. In particular, you can specify the table structure (index) type and distribution configuration for the target tables.

 Note

Some data types in your source tables may not be supported by specific dedicated SQL pool index types. For example, you cannot use a clustered columnstore index for tables that include VARBINARY(MAX) columns. You can map such tables to a heap (an unindexed table) in the dedicated SQL pool.

When the linked connection is configured appropriately, you can start it to initialize synchronization. The source tables are initially copied to the target database as snapshots, and then subsequent data modifications are replicated.

 Tip

Learn more:

For more information about Synapse Link for Azure SQL Database, see Azure Synapse Link for Azure SQL Database.
To learn about limitations and restrictions that apply to Synapse Link for Azure SQL Database, see Known limitations and issues with Azure Synapse Link for SQL.
For a step-by-step guide to setting up Synapse Link for Azure SQL Database, see Get started with Azure Synapse Link for Azure SQL Database. You'll also get a chance to try configuring Synapse Link for Azure SQL Database in the exercise, later in this module.




Introduction - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/implement-synapse-link-for-sql/1-introduction
Introduction
1 minute

Azure Synapse Link for SQL is a hybrid transactional / analytical processing (HTAP) capability in Azure Synapse Analytics that you can use to synchronize transactional data in Azure SQL Database or Microsoft SQL Server with a dedicated SQL pool in Azure Synapse Analytics. This synchronization enables you to perform near real-time analytical workloads on operational data with minimal impact on the transactional store used by business applications.

In this module, you'll learn how to:

Understand key concepts and capabilities of Azure Synapse Link for SQL.
Configure Azure Synapse Link for Azure SQL Database.
Configure Azure Synapse Link for Microsoft SQL Server.




What is Azure Synapse Link for SQL? - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/implement-synapse-link-for-sql/2-understand-synapse-link-sql
What is Azure Synapse Link for SQL?
5 minutes

Many organizations use a relational database in Azure SQL Database or Microsoft SQL Server to support business applications. These databases are optimized for transactional workloads that store and manipulate operational data. Performing analytical queries on the data in these databases to support reporting and data analysis incurs resource contention that can be detrimental to application performance.

A traditional approach to resolving this problem is to implement an extract, transform, and load (ETL) solution that loads data from the operational data store into an analytical store as a batch operation at regular intervals. While this solution supports the analytical workloads required for reporting and data analysis, it suffers from the following limitations:

The ETL process can be complex to implement and operate.
The analytical store is only updated at periodic intervals, so reporting doesn't reflect the most up-to-date operational data.
Azure Synapse Link for SQL

Azure Synapse Link for SQL addresses the limitations of a traditional ETL process by automatically replicating changes made to tables in the operational database to corresponding tables in an analytical database. After the initial synchronization process, the changes are replicated in near real-time without the need for a complex ETL batch process.

In the diagram above, the following key features of the Azure Synapse Link for SQL architecture are illustrated:

An Azure SQL Database or SQL Server 2022 instance contains a relational database in which transactional data is stored in tables.
Azure Synapse Link for SQL replicates the table data to a dedicated SQL pool in an Azure Synapse workspace.
The replicated data in the dedicated SQL pool can be queried in the dedicated SQL pool, or connected to as an external source from a Spark pool without impacting the source database.
Source and target databases

Azure Synapse Link for SQL supports the following source databases (used as operational data stores):

Azure SQL Database
Microsoft SQL Server 2022

 Note

Azure Synapse link for SQL is not supported for Azure SQL Managed Instance.

The target database (used as an analytical data store) must be a dedicated SQL pool in an Azure Synapse Analytics workspace.

The implementation details for Azure Synapse Link vary between the two types of data source, but the high-level principle is the same - changes made to tables in the source database are synchronized to the target database.

Change feed

Azure Synapse Link for SQL uses the change feed feature in Azure SQL Database and Microsoft SQL Server 2022 to capture changes to the source tables. All data modifications are recorded in the transaction log for the source database. The change feed feature monitors the log and applies the same data modifications in the target database. In the case of Azure SQL Database, the modifications are made directly to the target database. When using Azure Synapse Link for SQL Server, the changes are recorded in files and saved to a landing zone in Azure Data Lake Gen2 storage before being applied to the target database.

 Note

Change feed is similar to the change data capture (CDC) feature in SQL Server. The key difference is that CDC is used to reproduce data modifications in a table in the same database as the modified table. Change feed caches the data modification in memory and forwards it to Azure Synapse Analytics.

After implementing Azure Synapse Link for SQL, you can use system views and stored procedures in your Azure SQL Database or SQL Server database to monitor and manage change feed activity.

 Tip

Learn more:

For more information about change feed, see Azure Synapse Link for SQL change feed.
To learn more about monitoring and managing change feed, see Manage Azure Synapse Link for SQL Server and Azure SQL Database.




Implement Azure Synapse Link for SQL - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/implement-synapse-link-for-sql/

Implement Azure Synapse Link for SQL
Module
7 Units
Feedback
Intermediate
Data Analyst
Data Engineer
Azure SQL Database
Azure Synapse Analytics
SQL Server

Azure Synapse Link for SQL enables low-latency synchronization of operational data in a relational database to Azure Synapse Analytics.

Learning objectives

In this module, you'll learn how to:

Understand key concepts and capabilities of Azure Synapse Link for SQL.
Configure Azure Synapse Link for Azure SQL Database.
Configure Azure Synapse Link for Microsoft SQL Server.
Add
Prerequisites

Before starting this module, you should be familiar with Azure Synapse Analytics, Azure SQL Database, and SQL Server. Consider completing Explore relational database services in Azure and Introduction to Azure Synapse Analytics first.

Introduction
min
What is Azure Synapse Link for SQL?
min
Configure Azure Synapse Link for Azure SQL Database
min
Configure Azure Synapse Link for SQL Server 2022
min
Exercise - Implement Azure Synapse Link for SQL
min
Knowledge check
min
Summary
min


Summary - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/configure-azure-synapse-link-with-azure-cosmos-db/9-summary
Summary
1 minute

HTAP integration between Azure Cosmos DB and Azure Synapse Analytics enables a range of analytical workloads, including:

Supply chain analytics, forecasting & reporting.
Real-time personalization.
IOT predictive maintenance.

In this module, you learned how to:

Configure an Azure Cosmos DB Account to use Azure Synapse Link.
Create an analytical store enabled container.
Create a linked service for Azure Cosmos DB.
Analyze linked data using Spark.
Analyze linked data using Synapse SQL.

To learn more about using Azure Synapse Link to enable analytics scenarios, see Azure Synapse Link for Azure Cosmos DB: Near real-time analytics use cases.




Knowledge check - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/configure-azure-synapse-link-with-azure-cosmos-db/8-knowledge-check
Knowledge check
5 minutes
1. 

You have an Azure Cosmos DB for NoSQL account and an Azure Synapse Analytics workspace. What must you do first to enable HTAP integration with Azure Synapse Analytics?

 

Configure global replication in Azure Cosmos DB.

Create a dedicated SQL pool in Azure Synapse Analytics.

Enable Azure Synapse Link in Azure Cosmos DB.

2. 

You have an existing container in a Cosmos DB core (SQL) database. What must you do to enable analytical queries over Azure Synapse Link from Azure Synapse Analytics?

 

Delete and recreate the container.

Enable Azure Synapse Link in the container to create an analytical store.

Add an item to the container.

3. 

You plan to use a Spark pool in Azure Synapse Analytics to query an existing analytical store in Azure Cosmos DB. What must you do?

 

Create a linked service for the Azure Cosmos DB database where the analytical store enabled container is defined.

Disable automatic pausing for the Spark pool in Azure Synapse Analytics.

Install the Azure Cosmos DB SDK for Python package in the Spark pool.

4. 

You're writing PySpark code to load data from an Azure Cosmos DB analytical store into a dataframe. What format should you specify?

 

cosmos.json

cosmos.olap

cosmos.sql

5. 

You're writing a SQL code in a serverless SQL pool to query an analytical store in Azure Cosmos DB. What function should you use?

 

OPENDATASET

ROW

OPENROWSET

Check your answers




Exercise - Implement Azure Synapse Link for Cosmos DB - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/configure-azure-synapse-link-with-azure-cosmos-db/7-exercise-synapse-link-cosmos-db
Exercise - Implement Azure Synapse Link for Cosmos DB
35 minutes

Now it's your chance to explore Azure Synapse Link for Azure Cosmos DB for yourself. In this exercise, you'll use a provided script to provision an Azure Synapse Analytics workspace and an Azure Cosmos DB account in your Azure subscription; and then you'll enable Azure Synapse Link for Azure Cosmos DB and use it to analyze data with Spark and SQL.

 Note

To complete this lab, you will need an Azure subscription in which you have administrative access.

Launch the exercise and follow the instructions.




Query Cosmos DB with Synapse SQL - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/configure-azure-synapse-link-with-azure-cosmos-db/6-query-with-sql
Query Cosmos DB with Synapse SQL
5 minutes

In addition to using a Spark pool, you can also query an Azure Cosmos DB analytical container by using a built-in serverless SQL pool in Azure Synapse Analytics. To do this, you can use the OPENROWSET SQL function to connect to the linked service for your Azure Cosmos DB database.

Using OPENROWSET with an authentication key

By default, access to an Azure Cosmos DB account is authenticated by an authentication key. You can use this key as part of a connection string in an OPENROWSET statement to connect through a linked service from a SQL pool, as shown in the following example:

SELECT *
FROM OPENROWSET(
    'CosmosDB',
    'Account=my-cosmos-db;Database=my-db;Key=abcd1234....==',
    [my-container]) AS products_data


 Tip

You can find a primary and secondary key for your Cosmos DB account on its Keys page in the Azure portal.

The results of this query might look something like the following, including metadata and application-defined fields from the items in the Azure Cosmos DB container:

Expand table
_rid	_ts	productID	productName	id	_etag
mjMaAL...==	1655414791	123	Widget	7248f072-11c3-42b1-a368-...	54004b09-0000-2300-...
mjMaAL...==	1655414829	124	Wotsit	dc33131c-65c7-421a-a0f7-...	5400ca09-0000-2300-...
mjMaAL...==	1655414835	125	Thingumy	ce22351d-78c7-428a-a1h5-...	5400ca09-0000-2300-...
...	...	...	...	...	...

The data is retrieved from the analytical store, and the query doesn't impact the operational store.

Using OPENROWSET with a credential

Instead of including the authentication key in each call to OPENROWSET, you can define a credential that encapsulates the authentication information for your Cosmos DB account, and use the credential in subsequent queries. To create a credential, use the CREATE CREDENTIAL statement as shown in this example:

 CREATE CREDENTIAL my_credential
 WITH IDENTITY = 'SHARED ACCESS SIGNATURE',
 SECRET = 'abcd1234....==';


With the credential in place, you can use it in an OPENROWSET function like this:

SELECT *
FROM OPENROWSET(PROVIDER = 'CosmosDB',
                CONNECTION = 'Account=my-cosmos-db;Database=my-db',
                OBJECT = 'my-container',
                SERVER_CREDENTIAL = 'my_credential'
) AS products_data


Once again, the results include metadata and application-defined fields from the analytical store:

Expand table
_rid	_ts	productID	productName	id	_etag
mjMaAL...==	1655414791	123	Widget	7248f072-11c3-42b1-a368-...	54004b09-0000-2300-...
mjMaAL...==	1655414829	124	Wotsit	dc33131c-65c7-421a-a0f7-...	5400ca09-0000-2300-...
mjMaAL...==	1655414835	125	Thingumy	ce22351d-78c7-428a-a1h5-...	5400ca09-0000-2300-...
...	...	...	...	...	...
Specifying a schema

The OPENROWSET syntax includes a WITH clause that you can use to define a schema for the resulting rowset. You can use this to specify individual fields and assign data types as shown in the following example:

 SELECT *
 FROM OPENROWSET(PROVIDER = 'CosmosDB',
                 CONNECTION = 'Account=my-cosmos-db;Database=my-db',
                 OBJECT = 'my-container',
                 SERVER_CREDENTIAL = 'my_credential'
 )
 WITH (
    productID INT,
    productName VARCHAR(20)
 ) AS products_data


In this case, assuming the fields in the analytical store include productID and productName, the resulting rowset will resemble the following table:

Expand table
productID	productName
123	Widget
124	Wotsit
125	Thingumy
...	...

You can of course specify individual column names in the SELECT clause (for example, SELECT productID, productName ...), so this ability to specify individual columns may seem of limited use. However, consider cases where the source JSON documents stored in the operational store include multiple levels of fields, as show in the following example:

{
    "productID": 126,
    "productName": "Sprocket",
    "supplier": {
        "supplierName": "Contoso",
        "supplierPhone": "555-123-4567"
    }
    "id": "62588f072-11c3-42b1-a738-...",
    "_rid": "mjMaAL...==",
    ...
}


The WITH clause supports the inclusion of explicit JSON paths, enabling you to handle nested fields and to assign aliases to field names; as shown in this example:

 SELECT *
 FROM OPENROWSET(PROVIDER = 'CosmosDB',
                 CONNECTION = 'Account=my-cosmos-db;Database=my-db',
                 OBJECT = 'my-container',
                 SERVER_CREDENTIAL = 'my_credential'
 )
 WITH (
    ProductNo INT '$.productID',
    ProductName VARCHAR(20) '$.productName',
    Supplier VARCHAR(20) '$.supplier.supplierName',
    SupplierPhoneNo VARCHAR(15) '$.supplier.supplierPhone'
 ) AS products_data


The results of this query would include the following row for product 126:

Expand table
ProductNo	ProductName	Supplier	SupplierPhoneNo
126	Sprocket	Contoso	555-123-4567
Creating a view in a database

If you need to query the same data frequently, or you need to use reporting and visualization tools that rely on SELECT statements that don't include the OPENROWSET function, you can use a view to abstract the data. To create a view, you should create a new database in which to define it (user-defined views in the master database aren't supported), as shown in the following example:

CREATE DATABASE sales_db
   COLLATE Latin1_General_100_BIN2_UTF8;
 GO;

 USE sales_db;
 GO;

 CREATE VIEW products
 AS
 SELECT *
 FROM OPENROWSET(PROVIDER = 'CosmosDB',
                 CONNECTION = 'Account=my-cosmos-db;Database=my-db',
                 OBJECT = 'my-container',
                 SERVER_CREDENTIAL = 'my_credential'
 )
 WITH (
    ProductNo INT '$.productID',
    ProductName VARCHAR(20) '$.productName',
    Supplier VARCHAR(20) '$.supplier.supplierName',
    SupplierPhoneNo VARCHAR(15) '$.supplier.supplierPhone'
 ) AS products_data
 GO


 Tip

When creating a database that will access data in Cosmos DB, it's best to use a UTF-8 based collation to ensure compatibility with strings in Cosmos DB.

After the view has been created, users and client applications can query it like any other SQL view or table:

SELECT * FROM products;

Considerations for Serverless SQL pools and Azure Cosmos DB

When planning to use a serverless SQL pool to query data in an Azure Cosmos DB analytical store, consider the following best practices:

Provision your Azure Cosmos DB analytical storage and any client applications (for example Microsoft Power BI) in the same region as serverless SQL pool.

Azure Cosmos DB containers can be replicated to multiple regions. If you have a multi-region container, you can specify a region parameter in the OPENROWSET connection string to ensure queries are sent to a specific regional replica of the container.

When working with string columns, use the OPENROWSET function with the explicit WITH clause and specify an appropriate data length for the string data.




Query Cosmos DB data with Spark - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/configure-azure-synapse-link-with-azure-cosmos-db/5-query-with-spark
Query Cosmos DB data with Spark
5 minutes

After you've added a linked service for your analytical store enabled Azure Cosmos DB database, you can use it to query the data using a Spark pool in your Azure Synapse Analytics workspace.

Loading Azure Cosmos DB analytical data into a dataframe

For initial exploration or quick analysis of data from an Azure Cosmos DB linked service, it's often easiest to load data from a container into a dataframe using a Spark-supported language like PySpark (A Spark-specific implementation of Python) or Scala (a Java-based language often used on Spark).

For example, the following PySpark code could be used to load a dataframe named df from the data in the my-container container connected to using the my_linked_service linked service, and display the first 10 rows of data:

 df = spark.read
     .format("cosmos.olap")\
     .option("spark.synapse.linkedService", "my_linked_service")\
     .option("spark.cosmos.container", "my-container")\
     .load()

display(df.limit(10))


Let's suppose the my-container container is used to store items similar to the following example:

{
    "productID": 123,
    "productName": "Widget",
    "id": "7248f072-11c3-42b1-a368-...",
    "_rid": "mjMaAL...==",
    "_self": "dbs/mjM...==/colls/mjMaAL...=/docs/mjMaAL...==/",
    "_etag": "\"54004b09-0000-2300-...\"",
    "_attachments": "attachments/",
    "_ts": 1655414791
}


The output from the PySpark code would be similar to the following table:

Expand table
_rid	_ts	productID	productName	id	_etag
mjMaAL...==	1655414791	123	Widget	7248f072-11c3-42b1-a368-...	54004b09-0000-2300-...
mjMaAL...==	1655414829	124	Wotsit	dc33131c-65c7-421a-a0f7-...	5400ca09-0000-2300-...
mjMaAL...==	1655414835	125	Thingumy	ce22351d-78c7-428a-a1h5-...	5400ca09-0000-2300-...
...	...	...	...	...	...

The data is loaded from the analytical store in the container, not from the operational store; ensuring that there's no querying overhead on the operational store. The fields in the analytical data store include the application-defined fields (in this case productID and productName) and automatically created metadata fields.

After loading the dataframe, you can use its native methods to explore the data. For example, the following code creates a new dataframe containing only the productID and productName columns, ordered by the productName:

products_df = df.select("productID", "productName").orderBy("productName")

display(products_df.limit(10))


The output of this code would look similar this table:

Expand table
productID	productName
125	Thingumy
123	Widget
124	Wotsit
...	...
Writing a dataframe to a Cosmos DB container

In most HTAP scenarios, you should use the linked service to read data into Spark from the analytical store. However you can write the contents of a dataframe to the container as shown in the following example:

mydf.write.format("cosmos.oltp")\
    .option("spark.synapse.linkedService", "my_linked_service")\
    .option("spark.cosmos.container", "my-container")\
    .mode('append')\
    .save()


 Note

Writing a dataframe to a container updates the operational store and can have an impact on its performance. The changes are then synchronized to the analytical store.

Using Spark SQL to query Azure Cosmos DB analytical data

Spark SQL is a Spark API that provides SQL language syntax and relational database semantics in a Spark pool. You can use Spark SQL to define metadata for tables that can be queried using SQL.

For example, the following code creates a table named Products based on the hypothetical container used in the previous examples:

%%sql

-- Create a logical database in the Spark metastore
CREATE DATABASE mydb;

USE mydb;

-- Create a table from the Cosmos DB container
CREATE TABLE products using cosmos.olap options (
    spark.synapse.linkedService 'my_linked_service',
    spark.cosmos.container 'my-container'
);

-- Query the table
SELECT productID, productName
FROM products;


 Tip

The %%sql keyword at the beginning of the code is a magic that instructs the Spark pool to run the code as SQL rather than the default language (which is usually set to PySpark).

By using this approach, you can create a logical database in your Spark pool that you can then use to query the analytical data in Azure Cosmos DB to support data analysis and reporting workloads without impacting the operational store in your Azure Cosmos DB account.




Create a linked service for Cosmos DB - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/configure-azure-synapse-link-with-azure-cosmos-db/4-implement-synapse-link
Create a linked service for Cosmos DB
5 minutes

When you have an Azure Cosmos DB container with analytical store support, you can create a linked service in an Azure Synapse Analytics workspace to connect to it.

To create a linked service to an Azure Cosmos DB analytical data store, use Azure Synapse Studio, and add a linked service on the Data page by selecting the Connect to external data option, as shown here:

As you complete the steps to create your linked service, select the type of Azure Cosmos DB account and then assign your linked service a meaningful name and provide the necessary information to connect to your Azure Cosmos DB database.

To connect to the Azure Cosmos DB database, you can use any of the following authentication options:

Account key: Specify an authentication key for your Cosmos DB account.
Service Principal: Use the identity of the Azure Synapse Analytics service.
System Assigned Managed Identity: Use system-assigned managed identity.
User Managed Identity: Use a user-defined managed identity.

 Tip

For more information about using managed identities in Microsoft Entra ID, see What are managed identities for Azure resources?

After creating a linked service, the Azure Cosmos DB database and its containers will be shown in the Data page of Azure Synapse Studio, as shown here:

 Note

The user interface differentiates between containers with analytical store support and those without by using the following icons:

Expand table
Analytical store enabled	Analytical store not enabled
	

You can query a container without an analytical store, but you won't benefit from the advantages of an HTAP solution that offloads analytical query overhead from the operational data store.




Create an analytical store enabled container - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/configure-azure-synapse-link-with-azure-cosmos-db/3-create-analytical-store-enabled-container
Create an analytical store enabled container
5 minutes

After enabling Azure Synapse Link in an Azure Cosmos DB account, you can create or update a container with support for an analytical store.

An analytical store is a column-based store within the same container as a row-based operational store. An auto-sync process synchronizes changes in the operational store to the analytical store; from where it can be queried without incurring processing overhead in the operational store.

Analytical store schema types

As the data from the operational store is synchronized to the analytical store, the schema is updated dynamically to reflect the structure of the documents being synchronized. The specific behavior of this dynamic schema maintenance depends on the analytical store schema type configured for the Azure Cosmos DB account. Two types of schema representation are supported:

Well-defined: The default schema type for an Azure Cosmos DB for NoSQL account.
Full fidelity: The default (and only supported) schema type for an Azure Cosmos DB for MongoDB account.

The analytical store receives JSON data from the operational store and organizes it into a column-based structure. In a well-defined schema, the first non-null occurrence of a JSON field determines the data type for that field. Subsequent occurrences of the field that aren't compatible with the assigned data type aren't ingested into the analytical store.

For example, consider the following two JSON documents:

{"productID": 123, "productName": "Widget"}
{"productID": "124", "productName": "Wotsit"}


The first document determines that the productID field is a numeric (integer) value. When the second document is encountered, its productID field has a string value, and so isn't imported into the analytical store. The document and the rest of its field is imported, but the incompatible field is dropped. The following columns represent the data in the analytical store:

Expand table
productID	productName
123	Widget
	Wotsit

In a full fidelity schema, the data type is appended to each instance of the field, with new columns created as necessary; enabling the analytical store to contain multiple occurrences of a field, each with a different data type, as shown in the following table:

Expand table
productID.int32	productName.string	productID.string
123	Widget	
	Wotsit	124

 Note

For more information, see What is Azure Cosmos DB analytical store?.

Enabling analytical store support in a container

You can enable analytical store support when creating a new container or for an existing container. To enable analytical store support, you can use the Azure portal, or you can use the Azure CLI or Azure PowerShell from a command line or in a script.

Using the Azure portal

To enable analytical store support when creating a new container in the Azure portal, select the On option for Analytical Store, as shown here:

Alternatively, you can enable analytical store support for an existing container in the Azure Synapse Link page in the Integrations section of the page for your Cosmos DB account, as shown here:

Using the Azure CLI

To use the Azure CLI to enable analytical store support in an Azure Cosmos DB for NoSQL container, run the az cosmosdb sql container create command (to create a new container) or az cosmosdb sql container update command (to configure an existing container) with the --analytical-storage-ttl parameter, assigning a retention time for analytical data. Specifying an -analytical-storage-ttl parameter of -1 enables permanent retention of analytical data. For example, the following command creates a new container named my-container with analytical store support.

az cosmosdb sql container create --resource-group my-rg --account-name my-cosmos-db --database-name my-db --name my-container --partition-key-path "/productID" --analytical-storage-ttl -1


For an Azure Cosmos DB for MongoDB account, you can use the az cosmosdb mongodb collection create or az cosmosdb mongodb collection update command with the --analytical-storage-ttl parameter. For an Azure Cosmos DB for Apache Gremlin account, use the az cosmosdb gremlin graph create or az cosmosdb gremlin graph update command with the --analytical-storage-ttl parameter.

Using Azure PowerShell

To use Azure PowerShell to enable analytical store support in n Azure Cosmos DB for NoSQL container, run the New-AzCosmosDBSqlContainer cmdlet (to create a new container) or Update-AzCosmosDBSqlContainer cmdlet (to configure an existing container) with the -AnalyticalStorageTtl parameter, assigning a retention time for analytical data. Specifying an -AnalyticalStorageTtl parameter of -1 enables permanent retention of analytical data. For example, the following command creates a new container named my-container with analytical store support.

New-AzCosmosDBSqlContainer -ResourceGroupName "my-rg" -AccountName "my-cosmos-db" -DatabaseName "my-db" -Name "my-container" -PartitionKeyKind "hash" -PartitionKeyPath "/productID" -AnalyticalStorageTtl -1


For an Azure Cosmos DB for MongoDB API account, use the New-AzCosmosDBMongoDBCollection or Update-AzCosmosDBMongoDBCollection cmdlet with the -AnalyticalStorageTtl parameter.

Considerations for enabling analytical store support

Analytical store support can't be disabled without deleting the container. Setting the analytical store TTL value to 0 or null effectively disables the analytical store by no longer synchronizing new items to it from the operational store and deleting items already synchronized from the analytical store. After setting this value to 0, you can't re-enable analytical store support in the container.




Enable Cosmos DB account to use Azure Synapse Link - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/configure-azure-synapse-link-with-azure-cosmos-db/2-enable-cosmos-db-account-to-use
Enable Cosmos DB account to use Azure Synapse Link
5 minutes

Azure Synapse Link for Azure Cosmos DB is a cloud-native HTAP capability that enables integration between Azure Cosmos DB and Azure Synapse Analytics.

In the diagram above, the following key features of the Azure Synapse Link for Cosmos DB architecture are illustrated:

An Azure Cosmos DB container provides a row-based transactional store that is optimized for read/write operations.
The container also provides a column-based analytical store that is optimized for analytical workloads. A fully managed autosync process keeps the data stores in sync.
Azure Synapse Link provides a linked service that connects the analytical store enabled container in Azure Cosmos DB to an Azure Synapse Analytics workspace.
Azure Synapse Analytics provides Synapse SQL and Apache Spark runtimes in which you can run code to retrieve, process, and analyze data from the Azure Cosmos DB analytical store without impacting the transactional data store in Azure Cosmos DB.
Enabling Azure Synapse Link in Azure Cosmos DB

The first step in using Azure Synapse Link for Cosmos DB is to enable it in an Azure Cosmos DB account. Azure Synapse Link is supported in the following types of Azure Cosmos DB account:

Azure Cosmos DB for NoSQL
Azure Cosmos DB for MongoDB
Azure Cosmos DB for Apache Gremlin (preview)

You can enable Azure Synapse Link in the Azure portal page for your Cosmos DB account, or by using the Azure CLI or Azure PowerShell from a command line or in a script.

Using the Azure portal

In the Azure portal, you can enable Azure Synapse Link for a Cosmos DB account on the Azure Synapse Link page in the Integrations section, as shown below.

 Tip

For Azure Cosmos DB for NoSQL accounts, there's also a link on the Data Explorer page.

Using the Azure CLI

To enable Azure Synapse Link using the Azure CLI, run the az cosmosdb create command (to create a new Cosmos DB account) or az cosmosdb update command (to configure an existing Cosmos DB account) with the --enable-analytical-storage true parameter. For example, the following command updates an existing Cosmos DB account named my-cosmos-db to enable Azure Synapse Link.

az cosmosdb update --name my-cosmos-db --resource-group my-rg --enable-analytical-storage true


To enable Azure Synapse Link for an Azure Cosmos DB for Apache Gremlin account, include the --capabilities EnableGremlin parameter.

Using Azure PowerShell

To enable Azure Synapse Link using Azure PowerShell, run the New-AzCosmosDBAccount cmdlet (to create a new Cosmos DB account) or Update-AzCosmosDBAccount cmdlet (to configure an existing Cosmos DB account) with the -EnableAnalyticalStorage 1 parameter. For example, the following command updates an existing Cosmos DB account named my-cosmos-db to enable Azure Synapse Link.

Update-AzCosmosDBAccount -Name "my-cosmos-db" -ResourceGroupName "my-rg" -EnableAnalyticalStorage 1

Considerations for enabling Azure Synapse Link

When planning to enable Azure Synapse Link for a Cosmos DB account, consider the following facts:

After enabling Azure Synapse Link for an account, you can't disable it.

Enabling Azure Synapse Link doesn't start synchronization of operational data to an analytical store - you must also create or update a container with support for an analytical store.

When enabling Azure Synapse Link for a Cosmos DB for NoSQL account using the Azure CLI or PowerShell, you can use the --analytical-storage-schema-type (Azure CLI) or -AnalyticalStorageSchemaType (PowerShell) parameter to specify the schema type as WellDefined (default) or FullFidelity. For a Cosmos DB for MongoDB account, the default (and only supported) schema type is FullFidelity.

After a schema type has been assigned, you can't change it.

 Note

You'll learn more about the analytical store and its schema types in the next unit.




Introduction - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/configure-azure-synapse-link-with-azure-cosmos-db/1-introduction
Introduction
1 minute

Azure Synapse Analytics Link for Cosmos DB enables hybrid transactional/analytical processing (HTAP) integration between Azure Cosmos DB and Azure Synapse Analytics. By using this HTAP solution, organizations can make operational data in Azure Cosmos DB available for analysis and reporting in Azure Synapse Analytics in near-real time without the need to develop a complex ETL pipeline.

In this module, you'll learn how to:

Configure an Azure Cosmos DB account to use Azure Synapse Link.
Create an analytical store enabled container.
Create a linked service for Azure Cosmos DB.
Analyze linked data using Spark.
Analyze linked data using Synapse SQL.




Implement Azure Synapse Link with Azure Cosmos DB - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/configure-azure-synapse-link-with-azure-cosmos-db/

Implement Azure Synapse Link with Azure Cosmos DB
Module
9 Units
Feedback
Intermediate
Data Engineer
Azure Cosmos DB
Azure Synapse Analytics

Azure Synapse Link for Azure Cosmos DB enables HTAP integration between operational data in Azure Cosmos DB and Azure Synapse Analytics runtimes for Spark and SQL.

Learning objectives

After completing this module, you'll be able to:

Configure an Azure Cosmos DB Account to use Azure Synapse Link.
Create an analytical store enabled container.
Create a linked service for Azure Cosmos DB.
Analyze linked data using Spark.
Analyze linked data using Synapse SQL.
Add
Prerequisites

Before starting this module, you should have a basic knowledge of Azure Cosmos DB and Azure Synapse Analytics. Consider completing the following modules first:

Explore fundamentals of Azure Cosmos DB
Explore fundamentals of large-scale data warehousing
Introduction
min
Enable Cosmos DB account to use Azure Synapse Link
min
Create an analytical store enabled container
min
Create a linked service for Cosmos DB
min
Query Cosmos DB data with Spark
min
Query Cosmos DB with Synapse SQL
min
Exercise - Implement Azure Synapse Link for Cosmos DB
min
Knowledge check
min
Summary
min


Summary - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/design-hybrid-transactional-analytical-processing-using-azure-synapse-analytics/5-summary
Summary
1 minute

HTAP solutions enable extensive, near-realtime data analytics without impacting the performance of operational data stores. Azure Synapse Link offers multiple ways to create HTAP solutions that integrate operational data in commonly used data stores with Azure Synapse Analytics.

In this module, you learned to:

Describe Hybrid Transactional / Analytical Processing patterns.
Identify Azure Synapse Link services for HTAP.

For more information about Azure Synapse Link, see the following articles in the Azure Synapse Analytics documentation:

What is Azure Synapse Link for Azure Cosmos DB?
What is Azure Synapse Link for SQL?
What is Azure Synapse Link for Dataverse?




Knowledge check - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/design-hybrid-transactional-analytical-processing-using-azure-synapse-analytics/4-knowledge-check
Knowledge check
5 minutes
1. 

Which of the following descriptions matches a hybrid transactional/analytical processing (HTAP) architecture.

 

Business applications store data in an operational data store, which is also used to support analytical queries for reporting.

Business applications store data in an operational data store, which is synchronized with low latency to a separate analytical store for reporting and analysis.

Business applications store operational data in an analytical data store that is optimized for queries to support reporting and analysis.

2. 

You want to use Azure Synapse Analytics to analyze operational data stored in a Cosmos DB for NoSQL container. Which Azure Synapse Link service should you use?

 

Azure Synapse Link for SQL

Azure Synapse Link for Dataverse

Azure Synapse Link for Azure Cosmos DB

3. 

You plan to use Azure Synapse Link for Dataverse to analyze business data in your Azure Synapse Analytics workspace. Where is the replicated data from Dataverse stored?

 

In an Azure Synapse dedicated SQL pool

In an Azure Data Lake Gen2 storage container.

In an Azure Cosmos DB container.

Check your answers




Describe Azure Synapse Link - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/design-hybrid-transactional-analytical-processing-using-azure-synapse-analytics/3-azure-synapse-link
Describe Azure Synapse Link
6 minutes

HTAP solutions are supported in Azure Synapse Analytics through Azure Synapse Link; a general term for a set of linked services that support HTAP data synchronization into your Azure Synapse Analytics workspace.

Azure Synapse Link for Cosmos DB

Azure Cosmos DB is a global-scale NoSQL data service in Microsoft Azure that enables applications to store and access operational data by using a choice of application programming interfaces (APIs).

Azure Synapse Link for Azure Cosmos DB is a cloud-native HTAP capability that enables you to run near-real-time analytics over operational data stored in a Cosmos DB container. Azure Synapse Link creates a tight seamless integration between Azure Cosmos DB and Azure Synapse Analytics.

In the diagram above, the following key features of the Azure Synapse Link for Cosmos DB architecture are illustrated:

An Azure Cosmos DB container provides a row-based transactional store that is optimized for read/write operations.
The container also provides a column-based analytical store that is optimized for analytical workloads. A fully managed autosync process keeps the data stores in sync.
Azure Synapse Link provides a linked service that connects the analytical store enabled container in Azure Cosmos DB to an Azure Synapse Analytics workspace.
Azure Synapse Analytics provides Synapse SQL and Apache Spark runtimes in which you can run code to retrieve, process, and analyze data from the Azure Cosmos DB analytical store without impacting the transactional data store in Azure Cosmos DB.
Azure Synapse Link for SQL

Microsoft SQL Server is a popular relational database system that powers business applications in some of the world's largest organizations. Azure SQL Database is a cloud-based platform-as-a-service database solution based on SQL Server. Both of these relational database solutions are commonly used as operational data stores.

Azure Synapse Link for SQL enables HTAP integration between data in SQL Server or Azure SQL Database and an Azure Synapse Analytics workspace.

In the diagram above, the following key features of the Azure Synapse Link for SQL architecture are illustrated:

An Azure SQL Database or SQL Server instance contains a relational database in which transactional data is stored in tables.
Azure Synapse Link for SQL replicates the table data to a dedicated SQL pool in an Azure Synapse workspace.
The replicated data in the dedicated SQL pool can be queried in the dedicated SQL pool, or connected to as an external source from a Spark pool without impacting the source database.
Azure Synapse Link for Dataverse

Microsoft Dataverse is data storage service within the Microsoft Power Platform. You can use Dataverse to store business data in tables that are accessed by Power Apps, Power BI, Power Virtual Agents, and other applications and services across Microsoft 365, Dynamics 365, and Azure.

Azure Synapse Link for Dataverse enables HTAP integration by replicating table data to Azure Data Lake storage, where it can be accessed by runtimes in Azure Synapse Analytics - either directly from the data lake or through a Lake Database defined in a serverless SQL pool.

In the diagram above, the following key features of the Azure Synapse Link for Dataverse architecture are illustrated:

Business applications store data in Microsoft Dataverse tables.
Azure Synapse Link for Dataverse replicates the table data to an Azure Data Lake Gen2 storage account associated with an Azure Synapse workspace.
The data in the data lake can be used to define tables in a lake database and queried using a serverless SQL pool, or read directly from storage using SQL or Spark.




Understand hybrid transactional and analytical processing patterns - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/design-hybrid-transactional-analytical-processing-using-azure-synapse-analytics/2-understand-patterns
Understand hybrid transactional and analytical processing patterns
6 minutes

Many business application architectures separate transactional and analytical processing into separate systems with data stored and processed on separate infrastructures. These infrastructures are commonly referred to as OLTP (online transaction processing) systems working with operational data, and OLAP (online analytical processing) systems working with historical data, with each system is optimized for their specific task.

OLTP systems are optimized for dealing with discrete system or user requests immediately and responding as quickly as possible.

OLAP systems are optimized for the analytical processing, ingesting, synthesizing, and managing large sets of historical data. The data processed by OLAP systems largely originates from OLTP systems and needs to be loaded into the OLAP systems by ETL (Extract, Transform, and Load) batch processes.

Due to their complexity and the need to physically copy large amounts of data, this approach creates a delay in data being available to analyze in OLAP systems.

Hybrid Transactional / Analytical Processing (HTAP)

As more businesses move to digital processes, they increasingly recognize the value of being able to respond to opportunities by making faster and well-informed decisions. HTAP (Hybrid Transactional/Analytical processing) enables business to run advanced analytics in near-real-time on data stored and processed by OLTP systems.

The following diagram illustrates the generalized pattern of an HTAP architecture:

A business application processes user input and stores data in a transactional database that is optimized for a mix of data reads and writes based on the application's expected usage profile.
The application data is automatically replicated to an analytical store with low latency.
The analytical store supports data modeling, analytics, and reporting without impacting the transactional system.




Introduction - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/design-hybrid-transactional-analytical-processing-using-azure-synapse-analytics/1-introduction
Introduction
1 minute

Hybrid Transactional / Analytical Processing (HTAP) is a style of data processing that combines transactional data processing, such as is typically found in a business application, with analytical processing, such as is used in a business intelligence (BI) or reporting solution. The data access patterns and storage optimizations used in these two kinds of workload are very different, so usually a complex extract, transform, and load (ETL) process is required to copy data out of transactional systems and into analytical systems; adding complexity and latency to data analysis. In an HTAP solution, the transactional data is replicated automatically, with low-latency, to an analytical store, where it can be queried without impacting the performance of the transactional system.

In Azure Synapse Analytics, HTAP capabilities are provided by multiple Azure Synapse Link services, each connecting a commonly used transactional data store to your Azure Synapse Analytics workspace and making the data available for processing using Spark or SQL.

After completing this module, you'll be able to:

Describe Hybrid Transactional / Analytical Processing patterns.
Identify Azure Synapse Link services for HTAP.




Plan hybrid transactional and analytical processing using Azure Synapse Analytics - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/design-hybrid-transactional-analytical-processing-using-azure-synapse-analytics/

Plan hybrid transactional and analytical processing using Azure Synapse Analytics
Module
5 Units
Feedback
Intermediate
Data Engineer
Azure Synapse Analytics

Learn how hybrid transactional / analytical processing (HTAP) can help you perform operational analytics with Azure Synapse Analytics.

Learning objectives

After completing this module, you'll be able to:

Describe Hybrid Transactional / Analytical Processing patterns.
Identify Azure Synapse Link services for HTAP.
Add
Prerequisites

Before starting this module, you should have a basic knowledge of data analytics and Azure services for data. Consider completing the Azure Data Fundamentals certification first.

Introduction
min
Understand hybrid transactional and analytical processing patterns
min
Describe Azure Synapse Link
min
Knowledge check
min
Summary
min


Summary - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/use-spark-notebooks-azure-synapse-pipeline/7-summary
Summary
1 minute

Apache Spark offers data engineers a powerful platform for transforming and processing data. The ability to include Spark notebooks in a pipeline enables you to automate Spark processing and integrate it into a data integration workflow.

 Tip

To learn more about using Spark notebooks in an Azure Synapse Analytics pipeline, see Transform data by running a Synapse notebook in the Azure Synapse Analytics documentation.




Knowledge check - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/use-spark-notebooks-azure-synapse-pipeline/6-knowledge-check
Knowledge check
3 minutes
Check your knowledge
1. 

What kind of pool is required to run a Synapse notebook in a pipeline?

 

A Dedicated SQL pool

A Data Explorer pool

An Apache Spark pool

2. 

What kind of pipeline activity encapsulates a Synapse notebook?

 

Notebook activity

HDInsight Spark activity

Script activity

3. 

A notebook cell contains variable declarations. How can you use them as parameters?

 

Add a %%Spark magic at the beginning of the cell

Toggle the Parameters cell setting for the cell

Use the var keyword for each variable declaration

Check your answers




Exercise - Use an Apache Spark notebook in a pipeline - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/use-spark-notebooks-azure-synapse-pipeline/5-exercise-use-spark-notebooks-pipeline
Exercise - Use an Apache Spark notebook in a pipeline
30 minutes

Now it's your chance to integrate spark into an Azure Synapse Analytics pipeline. In this exercise, you'll create a pipeline that includes a notebook activity, and configure parameters for the notebook.

 Note

To complete this lab, you will need an Azure subscription in which you have administrative access.

Launch the exercise and follow the instructions.




Use parameters in a notebook - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/use-spark-notebooks-azure-synapse-pipeline/4-notebook-parameters
Use parameters in a notebook
5 minutes

Parameters enable you to dynamically pass values for variables in the notebook each time it's run. This approach provides flexibility, enabling you to adjust the logic encapsulated in the notebook for each run.

Create a parameters cell in the notebook

To define the parameters for a notebook, you declare and initialize variables in a cell, which you then configure as a Parameters cell by using the toggle option in the notebook editor interface.

Initializing a variable ensures that it has a default value, which will be used if the parameter isn't set in the notebook activity.

Set base parameters for the notebook activity

After defining a parameters cell in the notebook, you can set values to be used when the notebook is run by a notebook activity in a pipeline. To set parameter values, expand and edit the Base parameters section of the settings for the activity.

You can assign explicit parameter values, or use an expression to assign a dynamic value. For example, the expression @pipeline().RunId returns the unique identifier for the current run of the pipeline.




Use a Synapse notebook activity in a pipeline - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/use-spark-notebooks-azure-synapse-pipeline/3-use-notebook-activity
Use a Synapse notebook activity in a pipeline
3 minutes

To run a Spark notebook in a pipeline, you must add a notebook activity and configure it appropriately. You'll find the Notebook activity in the Synapse section of the activities pane in the Azure Synapse Analytics pipeline designer.

 Tip

You can also add a notebook to a pipeline from within the notebook editor.

To configure the notebook activity, edit the settings in the properties pane beneath the pipeline designer canvas. Notebook activity specific settings include:

Notebook: The notebook you want to run. You can select an existing notebook in your Azure Synapse Analytics workspace, or create a new one.
Spark pool: The Apache Spark pool on which the notebook should be run.
Executor size: The node size for the worker nodes in the pool, which determines the number of processor cores and the amount of memory allocated to worker nodes.
Dynamically allocate executors: Configures Spark dynamic allocation, enabling the pool to automatically scale up and down to support the workload.
Min executors: The minimum number of executors to be allocated.
Max executors: The maximum number of executors to be allocated.
Driver size: The node size for the driver node.




Understand Synapse Notebooks and Pipelines - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/use-spark-notebooks-azure-synapse-pipeline/2-understand-notebooks-pipelines
Understand Synapse Notebooks and Pipelines
5 minutes

Azure Synapse Pipelines enable you to create, run, and manage data integration and data flow activities. While many of these activities are built-into the Azure Synapse Pipeline platform and run natively in the integration runtime for your pipeline, you can also use external processing resources to perform specific tasks. One such external resource is an Apache Spark pool in your Azure Synapse Analytics workspace on which you can run code in a notebook.

It's common in big data analytics solutions for data engineers to use Spark notebooks for initial data exploration and interactive experimentation when designing data transformation processes. When the transformation logic has been completed, you can perform some final code optimization and refactoring for maintainability, and then include the notebook in a pipeline. The pipeline can then be run on a schedule or in response to an event (such as new data files being loaded into the data lake).

The notebook is run on a Spark pool, which you can configure with the appropriate compute resources and Spark runtime for your specific workload. The pipeline itself is run in an integration runtime that orchestrates the activities in the pipeline, coordinating the external services needed to run them.

 Tip

There are several best practices that can help make working with Spark notebooks more efficient and effective. Some of these include:

Keep your code organized: Use clear and descriptive variable and function names, and organize your code into small, reusable chunks.
Cache intermediate results: Spark allows you to cache intermediate results, which can significantly speed up the performance of your notebook.
Avoid unnecessary computations: Be mindful of the computations you are performing and try to avoid unnecessary steps. For example, if you only need a subset of your data, filter it out before running any further computations.
Avoid using collect() unless necessary: When working with large datasets, it is often better to perform operations on the entire dataset rather than bringing the data into the driver node using the collect() method.
Use Spark UI for monitoring and debugging: Spark's web-based user interface (UI) provides detailed information about the performance of your Spark jobs, including task execution times, input and output data sizes, and more.
Keep your dependencies version-consistent and updated: when working with Spark, it is important to keep dependencies version-consistent across your cluster and to use the latest version of Spark and other dependencies if possible.




Introduction - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/use-spark-notebooks-azure-synapse-pipeline/1-introduction
Introduction
1 minute

With Azure Synapse Analytics pipelines, you can orchestrate data transfer and transformation activities and build data integration solutions across multiple systems. When you're working with analytical data in a data lake, Apache Spark provides a scalable, distributed processing platform that you can use to process huge volumes of data efficiently.

The Synapse Notebook activity enables you to run data processing code in Spark notebooks as a task in a pipeline; making it possible to automate big data processing and integrate it into extract, transform, and load (ETL) workloads.




Use Spark Notebooks in an Azure Synapse Pipeline - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/use-spark-notebooks-azure-synapse-pipeline/

Use Spark Notebooks in an Azure Synapse Pipeline
Module
7 Units
Feedback
Intermediate
Data Engineer
Azure Synapse Analytics

Apache Spark provides data engineers with a scalable, distributed data processing platform, which can be integrated into an Azure Synapse Analytics pipeline.

Learning objectives

In this module, you will learn how to:

Describe notebook and pipeline integration.
Use a Synapse notebook activity in a pipeline.
Use parameters with a notebook activity.
Add
Prerequisites

Before starting this module, you should have experience of using Apache Spark in Azure Synapse Analytics. Consider completing the Analyze data with Apache Spark in Azure Synapse Analytics module first:

Introduction
min
Understand Synapse Notebooks and Pipelines
min
Use a Synapse notebook activity in a pipeline
min
Use parameters in a notebook
min
Exercise - Use an Apache Spark notebook in a pipeline
min
Knowledge check
min
Summary
min


Summary - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/build-data-pipeline-azure-synapse-analytics/8-summary
Summary
1 minute

Azure Synapse Analytics provides data integration services through the creation of pipelines. By using pipelines, you can implement complex extract, transform, and load (ETL) solutions that support enterprise data analytics.

 Tip

To learn more about developing and debugging pipelines, see Iterative development and debugging with Azure Data Factory and Synapse Analytics pipelines




Knowledge check - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/build-data-pipeline-azure-synapse-analytics/7-knowledge-check
Knowledge check
3 minutes
1. 

What does a pipeline use to access external data source and processing resources?

 

Data Explorer pools

Linked services

External tables

2. 

What kind of object should you add to a data flow to define a target to which data is loaded?

 

Source

Transformation

Sink

3. 

What must you create to run a pipeline at scheduled intervals?

 

A control flow

A trigger

An activity

Check your answers




Exercise - Build a data pipeline in Azure Synapse Analytics - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/build-data-pipeline-azure-synapse-analytics/6-exercise-build-data-pipeline-azure-synapse-analytics
Exercise - Build a data pipeline in Azure Synapse Analytics
45 minutes

Now it's your chance to build an Azure Synapse Analytics pipeline. In this exercise, you'll implement a run an Azure Synapse Analytics pipeline that transfers and transforms data.

 Note

To complete this lab, you will need an Azure subscription in which you have administrative access.

Launch the exercise and follow the instructions.




Run a pipeline - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/build-data-pipeline-azure-synapse-analytics/5-run-pipelines
Run a pipeline
5 minutes

When youre ready, you can publish a pipeline and use a trigger to run it. Triggers can be defined to run the pipeline:

Immediately
At explicitly scheduled intervals
In response to an event, such as new data files being added to a folder in a data lake.

You can monitor each individual run of a pipeline in the Monitor page in Azure Synapse Studio.

The ability to monitor past and ongoing pipeline runs is useful for troubleshooting purposes. Additionally, when combined with the ability to integrate Azure Synapse Analytics and Microsoft Purview, you can use pipeline run history to track data lineage data flows.

 Tip

To learn more about integration between Azure Synapse Analytics and Microsoft Purview, consider completing the Integrate Microsoft Purview and Azure Synapse Analytics module.




Define data flows - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/build-data-pipeline-azure-synapse-analytics/4-define-data-flows
Define data flows
5 minutes

A Data Flow is a commonly used activity type to define data flow and transformation. Data flows consist of:

Sources - The input data to be transferred.
Transformations  Various operations that you can apply to data as it streams through the data flow.
Sinks  Targets into which the data will be loaded.

When you add a Data Flow activity to a pipeline, you can open it in a separate graphical design interface in which to create and configure the required data flow elements.

An important part of creating a data flow is to define mappings for the columns as the data flows through the various stages, ensuring column names and data types are defined appropriately. While developing a data flow, you can enable the Data flow debug option to pass a subset of data through the flow, which can be useful to test that your columns are mapped correctly.

 Tip

To learn more about implementing a Data Flow activity, see Data Flow activity in Azure Data Factory and Azure Synapse Analytics in the Azure documentation.




Create a pipeline in Azure Synapse Studio - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/build-data-pipeline-azure-synapse-analytics/3-create-pipeline-azure-synapse-studio
Create a pipeline in Azure Synapse Studio
5 minutes

You can create a pipeline in Azure Synapse Studio by using shortcuts on the Home page, but the primary place where pipelines are created and managed is the Integrate page.

When you create a pipeline in Azure Synapse Studio, you can use the graphical design interface.

.

The pipeline designer includes a set of activities, organized into categories, which you can drag onto a visual design canvas. You can select each activity on the canvas and use the properties pane beneath the canvas to configure the settings for that activity.

To define the logical sequence of activities, you can connect them by using the Succeeded, Failed, and Completed dependency conditions, which are shown as small icons on the right-hand edge of each activity.

Defining a pipeline with JSON

While the graphical development environment is the preferred way to create a pipeline, you can also create or edit the underlying JSON definition of a pipeline. The following code example shows the JSON definition of a pipeline that includes a Copy Data activity:

{
  "name": "CopyPipeline",
  "properties": {
    "description": "Copy data from a blob to Azure SQL table",
    "activities": [
      {
        "name": "CopyFromBlobToSQL",
        "type": "Copy",
        "inputs": [
          {
            "name": "InputDataset"
          }
        ],
        "outputs": [
          {
            "name": "OutputDataset"
          }
        ],
        "typeProperties": {
          "source": {
            "type": "BlobSource"
          },
          "sink": {
            "type": "SqlSink",
            "writeBatchSize": 10000,
            "writeBatchTimeout": "60:00:00"
          }
        },
        "policy": {
          "retry": 2,
          "timeout": "01:00:00"
        }
      }
    ]
  }
}





Understand pipelines in Azure Synapse Analytics - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/build-data-pipeline-azure-synapse-analytics/2-understand-pipelines-azure-synapse-analytics
Understand pipelines in Azure Synapse Analytics
6 minutes

Pipelines in Azure Synapse Analytics encapsulate a sequence of activities that perform data movement and processing tasks. You can use a pipeline to define data transfer and transformation activities, and orchestrate these activities through control flow activities that manage branching, looping, and other typical processing logic. The graphical design tools in Azure Synapse Studio enable you to build complex pipelines with minimal or no coding required.

Core pipeline concepts

Before building pipelines in Azure Synapse Analytics, you should understand a few core concepts.

Activities

Activities are the executable tasks in a pipeline. You can define a flow of activities by connecting them in a sequence. The outcome of a particular activity (success, failure, or completion) can be used to direct the flow to the next activity in the sequence.

Activities can encapsulate data transfer operations, including simple data copy operations that extract data from a source and load it to a target (or sink), as well as more complex data flows that apply transformations to the data as part of an extract, transfer, and load (ETL) operation. Additionally, there are activities that encapsulate processing tasks on specific systems, such as running a Spark notebook or calling an Azure function. Finally, there are control flow activities that you can use to implement loops, conditional branching, or manage variable and parameter values.

Integration runtime

The pipeline requires compute resources and an execution context in which to run. The pipeline's integration runtime provides this context, and is used to initiate and coordinate the activities in the pipeline.

Linked services

While many of the activities are run directly in the integration runtime for the pipeline, some activities depend on external services. For example, a pipeline might include an activity to run a notebook in Azure Databricks or to call a stored procedure in Azure SQL Database. To enable secure connections to the external services used by your pipelines, you must define linked services for them.

 Note

Linked services are defined at the Azure Synapse Analytics workspace level, and can be shared across multiple pipelines.

Datasets

Most pipelines process data, and the specific data that is consumed and produced by activities in a pipeline is defined using datasets. A dataset defines the schema for each data object that will be used in the pipeline, and has an associated linked service to connect to its source. Activities can have datasets as inputs or outputs.

 Note

Similarly to linked services, datasets are defined at the Azure Synapse Analytics workspace level, and can be shared across multiple pipelines.




Introduction - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/build-data-pipeline-azure-synapse-analytics/1-introduction
Introduction
1 minute

With the wide range of data stores available in Azure, there's the need to manage and orchestrate the movement data between them. In fact, you'll usually want to automate extract, transform, and load (ETL) workloads as a regular process in a wider enterprise analytical solution. Pipelines are a mechanism for defining and orchestrating data movement activities. In this module, you'll be introduced to Azure Synapse Analytics pipelines, their component parts, and how to implement and run a pipeline in Azure Synapse Studio.

 Note

Azure Synapse Analytics pipelines are built on the same technology as Azure Data Factory, and offer a similar authoring experience. The authoring processes described in this module are also applicable to Azure Data Factory. For a detailed discussion of the differences between Azure Synapse Analytics pipelines and Azure Data Factory, see Data integration in Azure Synapse Analytics versus Azure Data Factory.




Build a data pipeline in Azure Synapse Analytics - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/build-data-pipeline-azure-synapse-analytics/

Build a data pipeline in Azure Synapse Analytics
Module
8 Units
Feedback
Intermediate
Data Engineer
Azure Data Factory
Azure Synapse Analytics

Pipelines are the lifeblood of a data analytics solution. Learn how to use Azure Synapse Analytics pipelines to build integrated data solutions that extract, transform, and load data across diverse systems.

Learning objectives

In this module, you will learn how to:

Describe core concepts for Azure Synapse Analytics pipelines.
Create a pipeline in Azure Synapse Studio.
Implement a data flow activity in a pipeline.
Initiate and monitor pipeline runs.
Add
Prerequisites

Before starting this module, you should be familiar with Azure Synapse Analytics and data analytics solutions in general. Consider completing the Introduction to Azure Synapse Analytics module first.

Introduction
min
Understand pipelines in Azure Synapse Analytics
min
Create a pipeline in Azure Synapse Studio
min
Define data flows
min
Run a pipeline
min
Exercise - Build a data pipeline in Azure Synapse Analytics
min
Knowledge check
min
Summary
min


Summary - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/secure-data-warehouse-azure-synapse-analytics/10-summary
Summary
3 minutes

In this module, you have learned how to approach and implement security to protect your data with Azure Synapse Analytics.

In this module, you have:

Understood network security options for Azure Synapse Analytics
Configured Conditional Access
Configured Authentication
Managed authorization through column and row level security
Managed sensitive data with Dynamic Data masking
Implemented encryption in Azure Synapse Analytics
Understood advanced data security options for Azure Synapse Analytics




Knowledge check - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/secure-data-warehouse-azure-synapse-analytics/9-knowledge-check
Knowledge check
3 minutes
1. 

You want to configure a private endpoint. You open up Azure Synapse Studio, go to the manage hub, and see that the private endpoints are greyed out. Why is the option not available?

 

Azure Synapse Studio doesn't support the creation of private endpoints.

A Conditional Access policy has to be defined first.

A managed virtual network hasn't been created.

2. 

You require an Azure Synapse Analytics Workspace to access an Azure Data Lake Store using the benefits of the security provided by Microsoft Entra ID. What is the best authentication method to use?

 

Storage account keys.

Shared access signatures.

Managed identities.

Check your answers




Implement encryption in Azure Synapse Analytics - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/secure-data-warehouse-azure-synapse-analytics/8-implement-encryption
Implement encryption in Azure Synapse Analytics
3 minutes

In this section, we will go through Transparent Data Encryption and TokenLibrary for Apache Spark.

What is transparent data encryption

Transparent data encryption (TDE) is an encryption mechanism to help you protect Azure Synapse Analytics. It will protect Azure Synapse Analytics against threats of malicious offline activity. The way TDE will do so is by encrypting data at rest. TDE performs real-time encryption as well as decryption of the database, associated backups, and transaction log files at rest without you having to make changes to the application. In order to use TDE for Azure Synapse Analytics, you will have to manually enable it.

What TDE does is performing I/O encryption and decryption of data at the page level in real time. When a page is read into memory, it is decrypted. It is encrypted before writing it to disk. TDE encrypts the entire database storage using a symmetric key called a Database Encryption Key (DEK). When you start up a database, the encrypted Database Encryption Key is decrypted. The DEK will then be used for decryption and re-encryption of the database files in the SQL Server database engine. The DEK is protected by the Transparent Data Encryption Protector. This protector can be either a service-managed certificated, which is referred to as service-managed transparent data encryption, or an asymmetric key that is stored in Azure Key Vault (customer-managed transparent data encryption).

What is important to understand is that for Azure Synapse Analytics, this TDE protector is set on the server level. There it is inherited by all the databases that are attached or aligned to that server. The term server refers both to server and instance.

Service-managed transparent data encryption

As stated above, the DEK that is protected by the Transparent Encryption protector can be service-managed certificates which we call service-managed TDE. When you look in Azure, that default setting means that the DEK is protected by a built-in certificate unique for each server with encryption algorithm AES256. When a database is in a geo-replicated relationship then primary and the geo-secondary database are protected by the primary database's parent server key. If the databases are connected to the same server, they will also have the same built-in AES 256 certificate. As Microsoft we automatically rotate the certificates in compliance with the internal security policy. The root key is protected by a Microsoft internal secret store. Microsoft also seamlessly moves and manages the keys as needed for geo-replication and restores.

Transparent data encryption with bring your own key for customer-managed transparent data encryption

As stated above, the DEK that is protected by the Transparent Data Encryption Protector can also be customer managed by bringing an asymmetric key that is stored in Azure Key Vault (customer-managed transparent data encryption). This is also referred to as Bring Your Own Key (BYOK) support for TDE. When this is the scenario that is applicable to you, the TDE Protector that encrypts the DEK is a customer-managed asymmetric key. It is stored in your own and managed Azure Key Vault. Azure Key Vault is Azure's cloud-based external key management system. This managed key never leaves the key vault. The TDE Protector can be generated by the key vault. Another option is to transfer the TDE Protector to the key vault from, for example, an on-premise hardware security module (HSM) device. Azure Synapse Analytics needs to be granted permissions to the customer-owned key vault in order to decrypt and encrypt the DEK. If permissions of the server to the key vault are revoked, a database will be inaccessible, and all data is encrypted.

By using Azure Key Vault integration for TDE, you have control over the key management tasks such as key rotations, key backups, and key permissions. It also enables you to audit and report on all the TDE protectors when using the Azure Key Vault functionality. The reason for using Key Vault is that it provides you with a central key management system where tightly monitored HSMs are leveraged. It also enables you to separate duties of management of keys and data in order to meet compliance with security policies.

Manage transparent data encryption in the Azure portal.

For Azure Synapse Analytics, you can manage TDE for the database in the Azure portal after you've signed in with the Azure Administrator or Contributor account. The TDE settings can be found under your user database.

It is by default that the service-managed TDE is used and therefore a TDE certificate is automatically generated for the server that contains that database.

Moving a transparent data encryption protected database

In some use cases you need to move a database that is protected with TDE. Within Azure, there is no need to decrypt the databases. The TDE settings on the source database or primary database, will be inherited on the target. Some of the operations within Azure that inherited the TDE are:

Geo-restore
Self-service point-in-time restore
Restoration of a deleted database
Active geo-replication
Creation of a database copy
Restore of backup file to Azure SQL Managed Instance

If you export a TDE-protected database, the exported content is not encrypted. This will be stored in an unencrypted BACPAC file. You need to make sure that you protect this BACPAC file and enable TDE as soon as the import of the bacpac file in the new database is finished.

Securing your credentials through linked services with TokenLibrary for Apache Spark

It is quite a common pattern to access data from external sources. Unless the external data source allows anonymous access, it is highly likely that you need to secure your connection with a credential, secret, or connection string.

Within Azure Synapse Analytics, the integration process is simplified by providing linked services. Doing so, the connection details can be stored in the linked service or an Azure Key Vault. If the Linked Service is created, Apache spark can reference the linked service to apply the connection information in your code. When you want to access files from the Azure Data Lake Storage Gen 2 within your Azure Synapse Analytics Workspace, it uses AAD passthrough for the authentication. Therefore, there is no need to use TokenLibrary. However, to connect to other linked services, you are enabled to make a direct call to the TokenLibrary.

An example can be found below: In order to connect to other linked services, you are enabled to make a direct call to TokenLibrary by retrieving the connection string. In order to retrieve the connection string, use the getConnectionString function and pass in the linked service name.

// Scala
// retrieve connectionstring from TokenLibrary

import com.microsoft.azure.synapse.tokenlibrary.TokenLibrary

val connectionString: String = TokenLibrary.getConnectionString("<LINKED SERVICE NAME>")
println(connectionString)

# Python
# retrieve connectionstring from TokenLibrary

from pyspark.sql import SparkSession

sc = SparkSession.builder.getOrCreate()
token_library = sc._jvm.com.microsoft.azure.synapse.tokenlibrary.TokenLibrary
connection_string = token_library.getConnectionString("<LINKED SERVICE NAME>")
print(connection_string)


If you want to Get the connection string as map and parse specific values from a key in the connection string, you can find an example below:

To parse specific values from a key=value pair in the connection string such as

DefaultEndpointsProtocol=https;AccountName=<AccountName>;AccountKey=<AccountKey>

use the getConnectionStringAsMap function and pass the key to return the value.

// Linked services can be used for storing and retreiving credentials (e.g, account key)
// Example connection string (for storage): "DefaultEndpointsProtocol=https;AccountName=<accountname>;AccountKey=<accountkey>"
import com.microsoft.azure.synapse.tokenlibrary.TokenLibrary

val accountKey: String = TokenLibrary.getConnectionStringAsMap("<LINKED SERVICE NAME">).get("<KEY NAME>")
println(accountKey)

# Linked services can be used for storing and retreiving credentials (e.g, account key)
# Example connection string (for storage): "DefaultEndpointsProtocol=https;AccountName=<accountname>;AccountKey=<accountkey>"
from pyspark.sql import SparkSession

sc = SparkSession.builder.getOrCreate()
token_library = sc._jvm.com.microsoft.azure.synapse.tokenlibrary.TokenLibrary
accountKey = token_library.getConnectionStringAsMap("<LINKED SERVICE NAME>").get("<KEY NAME>")
print(accountKey)





Manage sensitive data with Dynamic Data Masking - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/secure-data-warehouse-azure-synapse-analytics/7-manage-sensitive-data
Manage sensitive data with Dynamic Data Masking
8 minutes

Azure SQL Database, Azure SQL Managed Instance, and Azure Synapse Analytics support Dynamic Data Masking. Dynamic Data Masking ensures limited data exposure to nonprivileged users, such that they can't see the data that is being masked. It also helps you in preventing unauthorized access to sensitive information that has minimal impact on the application layer. Dynamic Data Masking is a policy-based security feature. It will hide the sensitive data in a result set of a query that runs over designated database fields.

Let's give you an example how it works. Let's say you work at a bank as a service representative in a call center. Due to compliance, any caller must identify themselves by providing several digits of their credit card number. In this scenario, the full credit card number shouldn't be fully exposed to the service representative in the call center. You can define a masking rule that masks all but the last four digits of a credit card number so that you would get a query that only gives as a result the last four digits of the credit card number. This is just one example that could be equally applied to a variety of personal data such that compliance isn't violated. For Azure Synapse Analytics, the way to set up a Dynamic Data Masking policy is using PowerShell or the REST API. The configuration of the Dynamic Data Masking policy can be done by the Azure SQL Database admin, server admin, or SQL Security Manager roles.

In Azure Synapse Analytics, you can find Dynamic Data Masking here;

Looking into Dynamic Data Masking Policies:

SQL users excluded from Dynamic Data Masking Policies

The following SQL users or Microsoft Entra identities can get unmasked data in the SQL query results. Users with administrator privileges are always excluded from masking, and will see the original data without any mask.

Masking rules - Masking rules are a set of rules that define the designated fields to be masked including the masking function that is used. The designated fields can be defined using a database schema name, table name, and column name.

Masking functions - Masking functions are a set of methods that control the exposure of data for different scenarios.

Set up Dynamic Data Masking for your database in Azure Synapse Analytics using PowerShell cmdlets

In this part, we're going to look into Dynamic Data Masking for a database in Azure Synapse Analytics using PowerShell cmdlets.

Data masking policies
Get-AzSqlDatabaseDataMaskingPolicy

The Get-AzSqlDatabaseDataMaskingPolicy gets the data masking policy for a database.

The syntax for the Get-AzSqlDatabaseDataMaskingPolicy in PowerShell is as follows:

Get-AzSqlDatabaseDataMaskingPolicy [-ServerName] <String> [-DatabaseName] <String>
 [-ResourceGroupName] <String> [-DefaultProfile <IAzureContextContainer>] [-WhatIf] [-Confirm]
 [<CommonParameters>]


What the Get-AzSqlDatabaseDataMaskingPolicy cmdlet does, is getting the data masking policy of an Azure SQL database.

To use this cmdlet in PowerShell, you'd have to specify the following parameters to identify the database:

ResourceGroupName: name of the resource group you deployed the database in
ServerName: sql server name
DatabaseName : name of the database

This cmdlet is also supported by the SQL Server Stretch Database service on Azure.

Set-AzSqlDatabaseDataMaskingPolicy

The Set-AzSqlDatabaseDataMaskingPolicy sets data masking for a database.

The syntax for the Set-AzSqlDatabaseDataMaskingPolicy in PowerShell is as follows:

Set-AzSqlDatabaseDataMaskingPolicy [-PassThru] [-PrivilegedUsers <String>] [-DataMaskingState <String>]
 [-ServerName] <String> [-DatabaseName] <String> [-ResourceGroupName] <String>
 [-DefaultProfile <IAzureContextContainer>] [-WhatIf] [-Confirm] [<CommonParameters>]


What the Set-AzSqlDatabaseDataMaskingPolicy cmdlet does is setting the data masking policy for an Azure SQL database.

To use this cmdlet in PowerShell, you'd have to specify the following parameters to identify the database:

ResourceGroupName: name of the resource group that you deployed the database in
ServerName : sql server name
DatabaseName : name of the database

In addition, you'll need to set the DataMaskingState parameter to specify whether data masking operations are enabled or disabled.

If the cmdlet succeeds and the PassThru parameter is used, it will return an object describing the current data masking policy in addition to the database identifiers.

Database identifiers can include, ResourceGroupName, ServerName, and DatabaseName.

This cmdlet is also supported by the SQL Server Stretch Database service on Azure.

Data masking rules
Get-AzSqlDatabaseDataMaskingRule

The Get-AzSqlDatabaseDataMaskingRule Gets the data masking rules from a database.

The syntax for the Get-AzSqlDatabaseDataMaskingRule in PowerShell is as follows:

Get-AzSqlDatabaseDataMaskingRule [-SchemaName <String>] [-TableName <String>] [-ColumnName <String>]
 [-ServerName] <String> [-DatabaseName] <String> [-ResourceGroupName] <String>
 [-DefaultProfile <IAzureContextContainer>] [-WhatIf] [-Confirm] [<CommonParameters>]


What the Get-AzSqlDatabaseDataMaskingRule cmdlet does it getting either a specific data masking rule or all of the data masking rules for an Azure SQL database.

To use this cmdlet in PowerShell, you'd have to specify the following parameters to identify the database:

ResourceGroupName: name of the resource group that you deployed the database in
ServerName : sql server name
DatabaseName : name of the database

You'd also have to specify the RuleId parameter to specify which rule this cmdlet returns.

If you don't provide RuleId, all the data masking rules for that Azure SQL database are returned.

This cmdlet is also supported by the SQL Server Stretch Database service on Azure.

New-AzSqlDatabaseDataMaskingRule

The New-AzSqlDatabaseDataMaskingRule creates a data masking rule for a database.

The syntax for the New-AzSqlDatabaseDataMaskingRule in PowerShell is as follows:

New-AzSqlDatabaseDataMaskingRule -MaskingFunction <String> [-PrefixSize <UInt32>] [-ReplacementString <String>]
 [-SuffixSize <UInt32>] [-NumberFrom <Double>] [-NumberTo <Double>] [-PassThru] -SchemaName <String>
 -TableName <String> -ColumnName <String> [-ServerName] <String> [-DatabaseName] <String>
 [-ResourceGroupName] <String> [-DefaultProfile <IAzureContextContainer>] [-WhatIf] [-Confirm]
 [<CommonParameters>]


What the New-AzSqlDatabaseDataMaskingRule cmdlet does is creating a data masking rule for an Azure SQL database.

To use this cmdlet in PowerShell, you'd have to specify the following parameters to identify the rule:

ResourceGroupName: name of the resource group that you deployed the database in
ServerName : sql server name
DatabaseName : name of the database

Providing the TableName and ColumnName is necessary in order to specify the target of the rule.

The MaskingFunction parameter is necessary to define how the data is masked.

If MaskingFunction has a value of Number or Text, you can specify the NumberFrom and NumberTo parameters, for number masking, or the PrefixSize, ReplacementString, and SuffixSize for text masking.

If the command succeeds and the PassThru parameter is used, the cmdlet returns an object describing the data masking rule properties in addition to the rule identifiers.

Rule identifiers can be, for example, ResourceGroupName, ServerName, DatabaseName, and RuleID.

This cmdlet is also supported by the SQL Server Stretch Database service on Azure.

Remove-AzSqlDatabaseDataMaskingRule

The Remove-AzSqlDatabaseDataMaskingRule removes a data masking rule from a database.

The syntax for the Remove-AzSqlDatabaseDataMaskingRule in PowerShell is as follows:

Remove-AzSqlDatabaseDataMaskingRule [-PassThru] [-Force] -SchemaName <String> -TableName <String>
 -ColumnName <String> [-ServerName] <String> [-DatabaseName] <String> [-ResourceGroupName] <String>
 [-DefaultProfile <IAzureContextContainer>] [-WhatIf] [-Confirm] [<CommonParameters>]


What the Remove-AzSqlDatabaseDataMaskingRule cmdlet does, is it removes a specific data masking rule from an Azure SQL database.

To use this cmdlet in PowerShell, you'd have to specify the following parameters to identify the rule that needs to be removed:

ResourceGroupName: name of the resource group that you deployed the database in
ServerName : sql server name
DatabaseName : name of the database
RuleId : identifier of the rule

This cmdlet is also supported by the SQL Server Stretch Database service on Azure.

Set-AzSqlDatabaseDataMaskingRule

The Set-AzSqlDatabaseDataMaskingRule Sets the properties of a data masking rule for a database.

The syntax for the Set-AzSqlDatabaseDataMaskingRule in PowerShell is as follows:

Set-AzSqlDatabaseDataMaskingRule [-MaskingFunction <String>] [-PrefixSize <UInt32>]
 [-ReplacementString <String>] [-SuffixSize <UInt32>] [-NumberFrom <Double>] [-NumberTo <Double>] [-PassThru]
 -SchemaName <String> -TableName <String> -ColumnName <String> [-ServerName] <String> [-DatabaseName] <String>
 [-ResourceGroupName] <String> [-DefaultProfile <IAzureContextContainer>] [-WhatIf] [-Confirm]
 [<CommonParameters>]


What the Set-AzSqlDatabaseDataMaskingRule cmdlet does is setting a data masking rule for an Azure SQL database.

To use this cmdlet in PowerShell, you'd have to specify the following parameters to identify the rule:

ResourceGroupName: name of the resource group that you deployed the database in
ServerName : sql server name
DatabaseName : name of the database
RuleId : identifier of the rule

You can provide any of the parameters of SchemaName, TableName, and ColumnName to retarget the rule.

Specify the MaskingFunction parameter to modify how the data is masked.

If you specify a value of Number or Text for MaskingFunction, you can specify the NumberFrom and NumberTo parameters for number masking or the PrefixSize, ReplacementString, and SuffixSize parameters for text masking.

If the command succeeds, and if you specify the PassThru parameter, the cmdlet returns an object that describes the data masking rule properties and the rule identifiers.

Rule identifiers can be, ResourceGroupName, ServerName, DatabaseName, and RuleId.

This cmdlet is also supported by the SQL Server Stretch Database service on Azure.

Set up Dynamic Data Masking for your database in Azure Synapse Analytics using the REST API

For setting up Dynamic Data Masking in Azure Synapse Analytics, you can also make use of the REST API. It will enable you to programmatically manage data masking policy and rules.

The REST API will support the following operations:

Data masking policies
Create Or Update

The Create Or Update masking policy using the REST API will create or update a database data masking policy.

In HTTP the following request can be made: > Note: The date of the API will change over time and the version you use will be determined by your needs and the funtionality requred.

GET https://management.azure.com/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Sql/servers/{serverName}/databases/{databaseName}/dataMaskingPolicies/Default?api-version=2021-06-01


The following parameters need to be passed through:

SubscriptionID: the ID of the subscription
ResourceGroupName: name of the resource group that you deployed the database in
ServerName : sql server name
DatabaseName : name of the database
dataMaskingPolicyName: the name of the data masking policy
api version: version of the api that is used.
Get

The Get policy, gets a database data masking policy.

In HTTP the following request can be made:

GET https://management.azure.com/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Sql/servers/{serverName}/databases/{databaseName}/dataMaskingPolicies/Default?api-version=2021-06-01


The following parameters need to be passed through:

SubscriptionID: the ID of the subscription
ResourceGroupName: name of the resource group that you deployed the database in
ServerName : sql server name
DatabaseName : name of the database
dataMaskingPolicyName: the name of the data masking policy
api version: version of the api that is used.

Data masking rules

Create Or Update

The Create or Update masking rule creates or updates a database data masking rule.

In HTTP the following request can be made:

PUT https://management.azure.com/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Sql/servers/{serverName}/databases/{databaseName}/dataMaskingPolicies/Default/rules/{dataMaskingRuleName}?api-version=2021-06-01


The following parameters need to be passed through:

SubscriptionID: the ID of the subscription
ResourceGroupName: name of the resource group that you deployed the database in
ServerName : sql server name
DatabaseName : name of the database
dataMaskingPolicyName: the name of the data masking policy
dataMaskingRuleName: the name of the rule for data masking
api version: version of the api that is used.
List By Database

The List By Database request gets a list of database data masking rules.

In HTTP the following request can be made:

GET https://management.azure.com/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Sql/servers/{serverName}/databases/{databaseName}/dataMaskingPolicies/Default/rules?api-version=2021-06-01


The following parameters need to be passed through:

SubscriptionID: the ID of the subscription
ResourceGroupName: name of the resource group that you deployed the database in
ServerName : sql server name
DatabaseName : name of the database
dataMaskingPolicyName: the name of the data masking policy
api version: version of the api that is used.




Exercise - Manage authorization through column and row level security - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/secure-data-warehouse-azure-synapse-analytics/6-exercise-manage-authorization-through-column-row-level-security
Exercise - Manage authorization through column and row level security
9 minutes

In this exercise, examples are shown how you can manage authorization through column and row level security.

An example of column level security

The following example shows how to restrict TestUser from accessing the SSN column of the Membership table:

Create Membership table with SSN column used to store social security numbers:

CREATE TABLE Membership
  (MemberID int IDENTITY,
   FirstName varchar(100) NULL,
   SSN char(9) NOT NULL,
   LastName varchar(100) NOT NULL,
   Phone varchar(12) NULL,
   Email varchar(100) NULL);


Allow TestUser to access all columns except for the SSN column, which has the sensitive data:

GRANT SELECT ON Membership(MemberID, FirstName, LastName, Phone, Email) TO TestUser;


Queries executed as TestUser will fail if they include the SSN column:

SELECT * FROM Membership;

-- Msg 230, Level 14, State 1, Line 12
-- The SELECT permission was denied on the column 'SSN' of the object 'Membership', database 'CLS_TestDW', schema 'dbo'.

An example of row level security

This scenario gives you an example for row level security on an Azure Synapse external table.

This short example creates three users and an external table with six rows. It then creates an inline table-valued function and a security policy for the external table. The example shows how select statements are filtered for the various users.

Prerequisites
You must have a SQL pool. See Create a Synapse SQL pool
The server hosting your SQL pool must be registered with AAD and you must have an Azure storage account with Storage Blog Contributor permissions. Follow the steps here.
Create a file system for your Azure Storage account. Use Storage Explorer to view your storage account. Right click on containers and select Create file system.

Once you have the prerequisites in place, create three user accounts that will demonstrate different access capabilities.

--run in master
CREATE LOGIN Manager WITH PASSWORD = '<user_password>'
GO
CREATE LOGIN Sales1 WITH PASSWORD = '<user_password>'
GO
CREATE LOGIN Sales2 WITH PASSWORD = '<user_password>'
GO

--run in master and your SQL pool database
CREATE USER Manager FOR LOGIN Manager;  
CREATE USER Sales1  FOR LOGIN Sales1;  
CREATE USER Sales2  FOR LOGIN Sales2 ;


Create a table to hold data.

CREATE TABLE Sales  
    (  
    OrderID int,  
    SalesRep sysname,  
    Product varchar(10),  
    Qty int  
    );  


Populate the table with six rows of data, showing three orders for each sales representative.

INSERT INTO Sales VALUES (1, 'Sales1', 'Valve', 5);
INSERT INTO Sales VALUES (2, 'Sales1', 'Wheel', 2);
INSERT INTO Sales VALUES (3, 'Sales1', 'Valve', 4);
INSERT INTO Sales VALUES (4, 'Sales2', 'Bracket', 2);
INSERT INTO Sales VALUES (5, 'Sales2', 'Wheel', 5);
INSERT INTO Sales VALUES (6, 'Sales2', 'Seat', 5);
-- View the 6 rows in the table  
SELECT * FROM Sales;


Create an Azure Synapse external table from the Sales table you just created.

CREATE MASTER KEY ENCRYPTION BY PASSWORD = '<user_password>';

CREATE DATABASE SCOPED CREDENTIAL msi_cred WITH IDENTITY = 'Managed Service Identity';

CREATE EXTERNAL DATA SOURCE ext_datasource_with_abfss WITH (TYPE = hadoop, LOCATION = 'abfss://<file_system_name@storage_account>.dfs.core.windows.net', CREDENTIAL = msi_cred);

CREATE EXTERNAL FILE FORMAT MSIFormat  WITH (FORMAT_TYPE=DELIMITEDTEXT);
  
CREATE EXTERNAL TABLE Sales_ext WITH (LOCATION='<your_table_name>', DATA_SOURCE=ext_datasource_with_abfss, FILE_FORMAT=MSIFormat, REJECT_TYPE=Percentage, REJECT_SAMPLE_VALUE=100, REJECT_VALUE=100)
AS SELECT * FROM sales;


Grant SELECT for the three users on the external table Sales_ext that you created.

GRANT SELECT ON Sales_ext TO Sales1;  
GRANT SELECT ON Sales_ext TO Sales2;  
GRANT SELECT ON Sales_ext TO Manager;


Create a new schema, and an inline table-valued function, you may have completed this in example A. The function returns 1 when a row in the SalesRep column is the same as the user executing the query (@SalesRep = USER_NAME()) or if the user executing the query is the Manager user (USER_NAME() = 'Manager').

CREATE SCHEMA Security;  
GO  
  
CREATE FUNCTION Security.fn_securitypredicate(@SalesRep AS sysname)  
    RETURNS TABLE  
WITH SCHEMABINDING  
AS  
    RETURN SELECT 1 AS fn_securitypredicate_result
WHERE @SalesRep = USER_NAME() OR USER_NAME() = 'Manager';  


Create a security policy on your external table using the inline table-valued function as a filter predicate. The state must be set to ON to enable the policy.

CREATE SECURITY POLICY SalesFilter_ext
ADD FILTER PREDICATE Security.fn_securitypredicate(SalesRep)
ON dbo.Sales_ext  
WITH (STATE = ON);


Now test the filtering predicate, by selecting from the Sales_ext external table. Sign in as each user, Sales1, Sales2, and manager. Run the following command as each user.

SELECT * FROM Sales_ext;


The Manager should see all six rows. The Sales1 and Sales2 users should only see their sales.

Alter the security policy to disable the policy.

ALTER SECURITY POLICY SalesFilter_ext  
WITH (STATE = OFF);  


Now the Sales1 and Sales2 users can see all six rows.

Connect to the Azure Synapse database to clean up resources

DROP USER Sales1;
DROP USER Sales2;
DROP USER Manager;

DROP SECURITY POLICY SalesFilter_ext;
DROP TABLE Sales;
DROP EXTERNAL TABLE Sales_ext;
DROP EXTERNAL DATA SOURCE ext_datasource_with_abfss ;
DROP EXTERNAL FILE FORMAT MSIFormat;
DROP DATABASE SCOPED CREDENTIAL msi_cred; 
DROP MASTER KEY;


Connect to logical master to clean up resources.

DROP LOGIN Sales1;
DROP LOGIN Sales2;
DROP LOGIN Manager;





Manage authorization through column and row level security - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/secure-data-warehouse-azure-synapse-analytics/5-manage-authorization-through-column-row-level-security
Manage authorization through column and row level security
6 minutes

In this topic, we'll go through how you can manage authorization through column and row level security within Azure Synapse Analytics. We'll start off by talking about column level security in Azure Synapse Analytics, and finish with row level security.

Column level security in Azure Synapse Analytics

Generally speaking, column level security is simplifying a design and coding for the security in your application. It allows you to restrict column access in order to protect sensitive data. For example, if you want to ensure that a specific user 'Leo' can only access certain columns of a table because he's in a specific department. The logic for 'Leo' only to access the columns specified for the department he works in, is a logic that is located in the database tier, rather than on the application level data tier. If he needs to access data from any tier, the database should apply the access restriction every time he tries to access data from another tier. The reason for doing so is to make sure that your security is reliable and robust since we're reducing the surface area of the overall security system. Column level security will also eliminate the necessity for the introduction of view, where you would filter out columns, to impose access restrictions on 'Leo'

The way to implement column level security is by using the GRANT T-SQL statement. Using this statement, SQL and Microsoft Entra ID support the authentication.

Syntax

The syntax to use for implementing column level security looks as follows:

GRANT <permission> [ ,...n ] ON
    [ OBJECT :: ][ schema_name ]. object_name [ ( column [ ,...n ] ) ] // specifying the column access
    TO <database_principal> [ ,...n ]
    [ WITH GRANT OPTION ]
    [ AS <database_principal> ]
<permission> ::=
    SELECT
  | UPDATE
<database_principal> ::=
      Database_user // specifying the database user
    | Database_role // specifying the database role 
    | Database_user_mapped_to_Windows_User
    | Database_user_mapped_to_Windows_Group


So when would you use column-level security? Let's say that you are a financial services firm, and can only have an account manager allowed to have access to a customer's social security number, phone number, or other personally identifiable information. It is imperative to distinguish the role of an account manager versus the manager of the account managers.

Another use case might be related to the Healthcare Industry. Let's say you have a specific health care provider. This healthcare provider only wants doctors and nurses to be able to access medical records. The billing department should not have access to view this data. Column-level security might be the option to use.

So how does column level security distinguishes from row-level security? Let's look into that.

Row level security in Azure Synapse Analytics

Row-level security (RLS) can help you to create a group membership or execution context in order to control not just columns in a database table, but actually, the rows. RLS, just like column-level security, can simply help and enable your design and coding of your application security. However, compared to column-level security where it's focused on the columns (parameters), RLS helps you implement restrictions on data row access. Let's say that your employee can only access rows of data that are important to the department, you should implement RLS. If you want to restrict, for example, customer data access that is only relevant to the company, you can implement RLS. The restriction on the access of the rows is a logic that is located in the database tier, rather than on the application level data tier. If 'Leo' needs to access data from any tier, the database should apply the access restriction every time he tries to access data from another tier. The reason for doing so is to make sure that your security is reliable and robust since we're reducing the surface area of the overall security system.

The way to implement RLS is by using the CREATE SECURITY POLICY[!INCLUDEtsql] statement. The predicates are created as inline table-valued functions. It is imperative to understand that within Azure Synapse, only supports filter predicates. If you need to use a block predicate, you won't be able to find support at this moment within Azure synapse.

Description of row level security in relation to filter predicates

RLS within Azure Synapse supports one type of security predicates, which are Filter predicates, not block predicates.
What filter predicates do, is silently filtering the rows that are available for reading operations such as SELECT, UPDATE, DELETE.

The access to row-level data in a table is restricted as an inline table-valued function, which is a security predicate. This table-valued function will then be invoked and enforced by the security policy that you need. An application is not aware of rows that are filtered from the result set for filter predicates. So what will happen is that if all rows are filtered, a null set is returned.

When you are using filter predicates, it will be applied when data is read from the base table. The filter predicate affects all get operations such as SELECT, DELETE, UPDATE. You are unable to select or delete rows that have been filtered. It is not possible for you to update a row that has been filtered. What you can do, is update rows in a way that they will be filtered afterward.

Use cases

We've already mentioned some use cases for RLS. Another use case might where you have created a multi-tenant application where you create a policy where logical separations of a tenant's data rows from another tenant's data rows are enforced. In order to implement this efficiently, it is highly recommended to store data for many tenants in a single table.

When we look at RLS filter predicates, they are functionally equivalent to appending a WHERE clause. The predicate can be as sophisticated as business practices dictate, or the clause can be as simple as WHERE TenantId = 42.

When we look at RLS more formally, RLS introduces predicate based access control. The reason why RLS can be used for predicate access control is that it is a flexible, centralized, predicate-based evaluation. The filter predicate can be based on metadata or any other criteria you would determine as appropriate. The predicate is used as a criterion to determine if the user has the appropriate access to the data based on user attributes. Label-based access control can be implemented by using predicate-based access control.

Permissions

If you want to create, alter or drop the security policies, you would have to use the ALTER ANY SECURITY POLICY permission. The reason for that is when you are creating or dropping a security policy it requires ALTER permissions on the schema.

In addition to that, there are other permissions required for each predicate that you would add:

SELECT and REFERENCES permissions on the inline table-valued function being used as a predicate.

REFERENCES permission on the table that you target to be bound to the policy.

REFERENCES permission on every column from the target table used as arguments.

Once you've set up the security policies, they will apply to all the users (including dbo users in the database) Even though DBO users can alter or drop security policies, their changes to the security policies can be audited. If you have special circumstances where highly privileged users, like a sysadmin or db_owner, need to see all rows to troubleshoot or validate data, you would still have to write the security policy in order to allow that.

If you have created a security policy where SCHEMABINDING = OFF, in order to query the target table, the user must have the SELECT or EXECUTE permission on the predicate function. They also need permissions to any additional tables, views, or functions used within the predicate function. If a security policy is created with SCHEMABINDING = ON (the default), then these permission checks are bypassed when users query the target table.

Best practices

There are some best practices to take in mind when you want to implement RLS. We recommended creating a separate schema for the RLS objects. RLS objects in this context would be the predicate functions, and security policies. Why is that a best practice? It helps to separate the permissions that are required on these special objects from the target tables. In addition to that, separation for different policies and predicate functions may be needed in multi-tenant-databases. However, it is not a standard for every case.

Another best practice to bear in mind is that the ALTER ANY SECURITY POLICY permission should only be intended for highly privileged users (such as a security policy manager). The security policy manager should not require SELECT permission on the tables they protect.

In order to avoid potential runtime errors, you should take into mind type conversions in predicate functions that you write. Also, you should try to avoid recursion in predicate functions. The reason for this is to avoid performance degradation. Even though the query optimizer will try to detect the direct recursions, there is no guarantee to find the indirect recursions. With indirect recursion, we mean where a second function calls the predicate function.

It would also be recommended to avoid the use of excessive table joins in predicate functions. This would maximize performance.

Generally speaking when it comes to the logic of predicates, you should try to avoid logic that depends on session-specific SET options. Even though this is highly unlikely to be used in practical applications, predicate functions whose logic depends on certain session-specific SET options can leak information if users are able to execute arbitrary queries. For example, a predicate function that implicitly converts a string to datetime could filter different rows based on the SET DATEFORMAT option for the current session.




Configure authentication - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/secure-data-warehouse-azure-synapse-analytics/4-configure-authentication

Learn  Training  Browse  Work with Data Warehouses using Azure Synapse Analytics  Secure a data warehouse in Azure Synapse Analytics 
Add
Previous
Unit 4 of 10
Next
Configure authentication
Completed
100 XP
8 minutes

Authentication is the process of validating credentials as you access resources in a digital infrastructure. This ensures that you can validate that an individual, or a service that wants to access a service in your environment can prove who they are. Azure Synapse Analytics provides several different methods for authentication.

What needs to be authenticated

There are a variety of scenarios that means that authentication must take place to protect the data that is stored in your Azure Synapse Analytics estate.

The common form of authentication is that of individuals who want to access the data in the service. This is typically seen as an individual providing a username and password to authenticate against a service. However, this is also becoming more sophisticated with authentication requests working in combination with Conditional Access policies to further secure the authentication process with additional security steps.

What is less obvious is the fact that services must authenticate with other services so that they can operate seamlessly. An example of this is using an Azure Synapse Spark or serverless SQL pool to access data in an Azure Data Lake store. An authentication mechanism must take place in the background to ensure that Azure Synapse Analytics can access the data in the data lake in an authenticated manner.

Finally, there are situations where users and services operate together at the same time. Here you have a combination of both user and service authentication taking place under the hood to ensure that the user is getting access to the data seamlessly. An example of this is using Power BI to view reports in a dashboard that is being serviced by a dedicated SQL pool. Here you have multiple levels of authentication taking place that needs to be managed.

Types of security

The following are the types of authentication that you should be aware of when working with Azure Synapse Analytics.

Microsoft Entra ID

Microsoft Entra ID is a directory service that allows you to centrally maintain objects that can be secured. The objects can include user accounts and computer accounts. An employee of an organization will typically have a user account that represents them in the organizations Microsoft Entra tenant, and they then use the user account with a password to authenticate against other resources that are stored within the directory using a process known as single sign-on.

The power of Microsoft Entra ID is that they only have to log in once, and Microsoft Entra ID will manage access to other resources based on the information held within it using pass through authentication. If a user and an instance of Azure Synapse Analytics are part of the same Microsoft Entra ID, it is possible for the user to access Azure Synapse Analytics without an apparent login. If managed correctly, this process is seamless as the administrator would have given the user authorization to access Azure Synapse Analytics dedicated SQL pool as an example.

In this situation, it is normal for an Azure Administrator to create the user accounts and assign them to the appropriate roles and groups in Microsoft Entra ID. The Data Engineer will then add the user, or a group to which the user belongs to access a dedicated SQL pool.

Managed identities

Managed identity for Azure resources is a feature of Microsoft Entra ID. The feature provides Azure services with an automatically managed identity in Microsoft Entra ID. You can use the Managed Identity capability to authenticate to any service that supports Microsoft Entra authentication.

Managed identities for Azure resources are the new name for the service formerly known as Managed Service Identity (MSI). A system-assigned managed identity is created for your Azure Synapse workspace when you create the workspace.

Azure Synapse also uses the managed identity to integrate pipelines. The managed identity lifecycle is directly tied to the Azure Synapse workspace. If you delete the Azure Synapse workspace, then the managed identity is also cleaned up.

The workspace managed identity needs permissions to perform operations in the pipelines. You can use the object ID or your Azure Synapse workspace name to find the managed identity when granting permissions.

You can retrieve the managed identity in the Azure portal. Open your Azure Synapse workspace in Azure portal and select Overview from the left navigation. The managed identity's object ID is displayed to in the main screen.

The managed identity information will also show up when you create a linked service that supports managed identity authentication from Azure Synapse Studio.

Launch Azure Synapse Studio and select the Manage tab from the left navigation. Then select Linked services and choose the + New option to create a new linked service.

In the New linked service window, type Azure Data Lake Storage Gen2. Select the Azure Data Lake Storage Gen2 resource type from the list below and choose Continue.

In the next window, choose Managed Identity for Authentication method. You'll see the managed identity's Name and Object ID.

SQL Authentication

For user accounts that are not part of a Microsoft Entra ID, then using SQL Authentication will be an alternative. In this instance, a user is created in the instance of a dedicated SQL pool. If the user in question requires administrator access, then the details of the user are held in the master database. If administrator access is not required, you can create a user in a specific database. A user then connects directly to the Azure Synapse Analytics dedicated SQL pool where they are prompted to use a username and password to access the service.

This approach is typically useful for external users who need to access the data, or if you are using third party or legacy applications against the Azure Synapse Analytics dedicated SQL pool

Multifactor authentication

Synapse SQL support connections from SQL Server Management Studio (SSMS) using Active Directory Universal Authentication.

This enables you to operate in environments that use Conditional Access policies that enforce multifactor authentication as part of the policy.

Keys

If you are unable to use a managed identity to access resources such as Azure Data Lake then you can use storage account keys and shared access signatures.

With a storage account key. Azure creates two of these keys (primary and secondary) for each storage account you create. The keys give access to everything in the account. You'll find the storage account keys in the Azure portal view of the storage account. Just select Settings, and then click Access keys.

As a best practice, you shouldn't share storage account keys, and you can use Azure Key Vault to manage and secure the keys.

Azure Key Vault is a secret store: a centralized cloud service for storing app secrets - configuration values like passwords and connection strings that must remain secure at all times. Key Vault helps you control your apps' secrets by keeping them in a single central location and providing secure access, permissions control, and access logging.

The main benefits of using Key Vault are:

Separation of sensitive app information from other configuration and code, reducing risk of accidental leaks
Restricted secret access with access policies tailored to the apps and individuals that need them
Centralized secret storage, allowing required changes to happen in only one place
Access logging and monitoring to help you understand how and when secrets are accessed

Secrets are stored in individual vaults, which are Azure resources used to group secrets together. Secret access and vault management is accomplished via a REST API, which is also supported by all of the Azure management tools as well as client libraries available for many popular languages. Every vault has a unique URL where its API is hosted.

Shared access signatures

If an external third-party application needs access to your data, you'll need to secure their connections without using storage account keys. For untrusted clients, use a shared access signature (SAS). A shared access signature is a string that contains a security token that can be attached to a URI. Use a shared access signature to delegate access to storage objects and specify constraints, such as the permissions and the time range of access. You can give a customer a shared access signature token.

Types of shared access signatures

You can use a service-level shared access signature to allow access to specific resources in a storage account. You'd use this type of shared access signature, for example, to allow an app to retrieve a list of files in a file system or to download a file.

Use an account-level shared access signature to allow access to anything that a service-level shared access signature can allow, plus additional resources and abilities. For example, you can use an account-level shared access signature to allow the ability to create file systems.

Next unit: Manage authorization through column and row level security

Continue

Having an issue? We can help!

For issues related to this module, explore existing questions using the #azure training tag or Ask a question on Microsoft Q&A .
For issues related to Certifications and Exams, post on Credentials Support Forum or visit our Credentials Help.

Feedback

Was this page helpful?

Yes
No


Configure Conditional Access - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/secure-data-warehouse-azure-synapse-analytics/3-configure-conditional-access

Learn  Training  Browse  Work with Data Warehouses using Azure Synapse Analytics  Secure a data warehouse in Azure Synapse Analytics 
Add
Previous
Unit 3 of 10
Next
Configure Conditional Access
Completed
100 XP
7 minutes

Conditional Access is a feature that enables you to define the conditions under which a user can connect to your Azure subscription and access services. Conditional Access provides an additional layer of security that can be used in combination with authentication to strengthen the security access to your network.

Conditional Access policies at their simplest are if-then statements, if a user wants to access a resource, then they must complete an action. As an example, if a Data Engineer wishes to access services in Azure Synapse Analytics, they may be requested by the Conditional Access policy to perform an additional step of multifactor authentication (MFA) to complete the authentication to get onto the service

Conditional Access policies use signals as a basis to determine if Conditional Access should first be applied. Common signals include:

User or group membership names
IP address information
Device platforms or type
Application access requests
Real-time and calculated risk detection
Microsoft Cloud App Security (MCAS)

Based on these signals, you can then choose to block access. The alternative is you can grant access, and at the same time request that the user perform an additional action including:

Perform multifactor authentication
Use a specific device to connect

Given the amount of data that could potentially be stored, Azure Synapse Analytics dedicated SQL pools supports Conditional Access to provide protection for your data. It does require that Azure Synapse Analytics is configured to support Microsoft Entra ID, and that if you chose multifactor authentication, that the tool you are using support it.

To configure Conditional Access, you can perform the following steps:

Sign in to the Azure portal, select Microsoft Entra ID, and then select Conditional Access.

In the Conditional Access-Policies blade, click New policy, provide a name, and then click Configure rules.

Under Assignments, select Users and groups, check Select users and groups, and then select the user or group for Conditional Access. Click Select, and then click Done to accept your selection.

Select Cloud apps, click Select apps. You see all apps available for Conditional Access. Select Azure SQL Database, at the bottom click Select, and then click Done.

If you can't find Azure SQL Database listed in the following third screenshot, complete the following steps:

Connect to your database in Azure SQL Database by using SSMS with a Microsoft Entra admin account.
Execute CREATE USER [user@yourtenant.com] FROM EXTERNAL PROVIDER.
Sign into Microsoft Entra ID and verify that Azure SQL Database, SQL Managed Instance, or Azure Synapse are listed in the applications in your Microsoft Entra instance.

Select Access controls, select Grant, and then check the policy you want to apply. For this example, we select Require multifactor authentication.

Next unit: Configure authentication

Continue

Having an issue? We can help!

For issues related to this module, explore existing questions using the #azure training tag or Ask a question on Microsoft Q&A .
For issues related to Certifications and Exams, post on Credentials Support Forum or visit our Credentials Help.

Feedback

Was this page helpful?

Yes
No


Understand network security options for Azure Synapse Analytics - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/secure-data-warehouse-azure-synapse-analytics/2-understand-network-security-options
Understand network security options for Azure Synapse Analytics
5 minutes

There are a range of network security steps that you should consider to secure Azure Synapse Analytics. One of the first aspects that you will consider is securing access to the service itself. This can be achieved by creating the following network objects including:

Firewall rules
Virtual networks
Private endpoints
Firewall rules

Firewall rules enable you to define the type of traffic that is allowed or denied access to an Azure Synapse workspace using the originating IP address of the client that is trying to access the Azure Synapse Workspace. IP firewall rules configured at the workspace level apply to all public endpoints of the workspace including dedicated SQL pools, serverless SQL pool, and the development endpoint.

You can choose to allow connections from all IP addresses as you are creating the Azure Synapse Workspaces, although this is not recommended as it does not allow for control access to the workspace. Instead, within the Azure portal, you can configure specific IP address ranges and associate them with a rule name so that you have greater control.

Make sure that the firewall on your network and local computer allows outgoing communication on TCP ports 80, 443 and 1443 for Synapse Studio.

Also, you need to allow outgoing communication on UDP port 53 for Synapse Studio. To connect using tools such as SSMS and Power BI, you must allow outgoing communication on TCP port 1433.

Virtual networks

Azure Virtual Network (VNet) enables private networks in Azure. VNet enables many types of Azure resources, such as Azure Synapse Analytics, to securely communicate with other virtual networks, the internet, and on-premises networks. When you create your Azure Synapse workspace, you can choose to associate it to a Microsoft Azure Virtual Network. The Virtual Network associated with your workspace is managed by Azure Synapse. This Virtual Network is called a Managed workspace Virtual Network.

Using a managed workspace virtual network provides the following benefits:

With a Managed workspace Virtual Network, you can offload the burden of managing the Virtual Network to Azure Synapse.
You don't have to configure inbound NSG rules on your own Virtual Networks to allow Azure Synapse management traffic to enter your Virtual Network. Misconfiguration of these NSG rules causes service disruption for customers.
You don't need to create a subnet for your Spark clusters based on peak load.
Managed workspace Virtual Network along with Managed private endpoints protects against data exfiltration. You can only create Managed private endpoints in a workspace that has a Managed workspace Virtual Network associated with it.
it ensures that your workspace is network isolated from other workspaces.

If your workspace has a Managed workspace Virtual Network, Data integration and Spark resources are deployed in it. A Managed workspace Virtual Network also provides user-level isolation for Spark activities because each Spark cluster is in its own subnet.

Dedicated SQL pool and serverless SQL pool are multi-tenant capabilities and therefore reside outside of the Managed workspace Virtual Network. Intra-workspace communication to dedicated SQL pool and serverless SQL pool use Azure private links. These private links are automatically created for you when you create a workspace with a Managed workspace Virtual Network associated to it.

You can only choose to enable managed virtual networks as you are creating the Azure Synapse Workspaces.

Private endpoints

Azure Synapse Analytics enables you to connect up its various components through endpoints. You can set up managed private endpoints to access these components in a secure manner known as private links. This can only be achieved in an Azure Synapse workspace with a Managed workspace Virtual Network. Private link enables you to access Azure services (such as Azure Storage and Azure Cosmos DB) and Azure hosted customer/partner services from your Azure Virtual Network securely.

When you use a private link, traffic between your Virtual Network and workspace traverses entirely over the Microsoft backbone network. Private Link protects against data exfiltration risks. You establish a private link to a resource by creating a private endpoint.

Private endpoint uses a private IP address from your Virtual Network to effectively bring the service into your Virtual Network. Private endpoints are mapped to a specific resource in Azure and not the entire service. Customers can limit connectivity to a specific resource approved by their organization. You can manage the private endpoints in the Azure Synapse Studio manage hub.




Introduction - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/secure-data-warehouse-azure-synapse-analytics/1-introduction
Introduction
3 minutes

In this module, you will learn how to approach and implement security to protect your data with Azure Synapse Analytics.

In this module, you will:

Understand network security options for Azure Synapse Analytics
Configure Conditional Access
Configure Authentication
Manage authorization through column and row level security
Manage sensitive data with Dynamic Data masking
Implement encryption in Azure Synapse Analytics
Understand advanced data security options for Azure Synapse Analytics
Prerequisites

Before taking this module, it is recommended that the student is able to:

Log into the Azure portal
Create a Synapse Analytics Workspace
Create an Azure Synapse Analytics SQL Pool




Secure a data warehouse in Azure Synapse Analytics - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/secure-data-warehouse-azure-synapse-analytics/

Secure a data warehouse in Azure Synapse Analytics
Module
10 Units
Feedback
Intermediate
Data Engineer
Azure Synapse Analytics

Learn how to approach and implement security to protect your data with Azure Synapse Analytics.

Learning objectives

In this module, you will:

Understand network security options for Azure Synapse Analytics
Configure Conditional Access
Configure Authentication
Manage authorization through column and row level security
Manage sensitive data with Dynamic Data masking
Implement encryption in Azure Synapse Analytics
Add
Prerequisites
Before taking this module, it is recommended that you complete Data Fundamentals.
Introduction
min
Understand network security options for Azure Synapse Analytics
min
Configure Conditional Access
min
Configure authentication
min
Manage authorization through column and row level security
min
Exercise - Manage authorization through column and row level security
min
Manage sensitive data with Dynamic Data Masking
min
Implement encryption in Azure Synapse Analytics
min
Knowledge check
min
Summary
min


Summary - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/manage-monitor-data-warehouse-activities-azure-synapse-analytics/8-summary
Summary
3 minutes

In this module, you have learned some of the features you can use to manage and monitor Azure Synapse Analytics, including:

How to Scale compute resources in Azure Synapse Analytics
Pausing the compute in Azure Synapse Analytics
How to manage workloads in Azure Synapse Analytics
Using the Azure Advisor to review recommendations
Using Dynamic Management Views to identify and troubleshoot query performance




Knowledge check - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/manage-monitor-data-warehouse-activities-azure-synapse-analytics/7-knowledge-check
Knowledge check
3 minutes
1. 

Which ALTER DATABASE statement parameter allows a dedicated SQL pool to scale?

 

SCALE.

MODIFY

CHANGE.

2. 

Which workload management feature influences the order in which a request gets access to resources?

 

Workload classification.

Workload importance.

Workload isolation.

3. 

Which Dynamic Management View enables the view of the active connections against a dedicated SQL pool?

 

sys.dm_pdw_exec_requests.

sys.dm_pdw_dms_workers.

DBCC PDW_SHOWEXECUTIONPLAN.

Check your answers




Use dynamic management views to identify and troubleshoot query performance - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/manage-monitor-data-warehouse-activities-azure-synapse-analytics/6-use-dynamic-management-views-to-identify-troubleshoot-query-performance
Use dynamic management views to identify and troubleshoot query performance
8 minutes

Dynamic Management Views provide a programmatic experience for monitoring the Azure Synapse Analytics SQL pool activity by using the Transact-SQL language. The views that are provided, not only enable you to troubleshoot and identify performance bottlenecks with the workloads working on your system, but they are also used by other services such as Azure Advisor to provide recommendations about Azure Synapse Analytics.

There are over 90 Dynamic Management Views that can queried against dedicated SQL pools to retrieve information about the following areas of the service:

Connection information and activity
SQL execution requests and queries
Index and statistics information
Resource blocking and locking activity
Data movement service activity
Errors

The following is an example of monitoring query execution of the Azure Synapse Analytics SQL pools. The first step involves checking the connections against the server first, before checking the query execution activity.

Monitoring connections

All logins to your data warehouse are logged to sys.dm_pdw_exec_sessions. The session_id is the primary key and is assigned sequentially for each new logon.

-- Other Active Connections
SELECT * FROM sys.dm_pdw_exec_sessions where status <> 'Closed' and session_id <> session_id();

Monitor query execution

All queries executed on SQL pool are logged to sys.dm_pdw_exec_requests. The request_id uniquely identifies each query and is the primary key for this DMV. The request_id is assigned sequentially for each new query and is prefixed with QID, which stands for query ID. Querying this DMV for a given session_id shows all queries for a given logon.

Step 1

The first step is to identify the query you want to investigate

-- Monitor active queries
SELECT *
FROM sys.dm_pdw_exec_requests
WHERE status not in ('Completed','Failed','Cancelled')
  AND session_id <> session_id()
ORDER BY submit_time DESC;

-- Find top 10 queries longest running queries
SELECT TOP 10 *
FROM sys.dm_pdw_exec_requests
ORDER BY total_elapsed_time DESC;


From the preceding query results, note the Request ID of the query that you would like to investigate.

Queries in the Suspended state can be queued due to a large number of active running queries. These queries also appear in the sys.dm_pdw_waits waits query with a type of UserConcurrencyResourceType. For information on concurrency limits, see Memory and concurrency limits or Resource classes for workload management. Queries can also wait for other reasons such as for object locks. If your query is waiting for a resource, see Investigating queries waiting for resources further down in this article.

To simplify the lookup of a query in the sys.dm_pdw_exec_requests table, use LABEL to assign a comment to your query, which can be looked up in the sys.dm_pdw_exec_requests view.

-- Query with Label
SELECT *
FROM sys.tables
OPTION (LABEL = 'My Query')
;

-- Find a query with the Label 'My Query'
-- Use brackets when querying the label column, as it it a key word
SELECT  *
FROM    sys.dm_pdw_exec_requests
WHERE   [label] = 'My Query';

Step 2

Use the Request ID to retrieve the queries distributed SQL (DSQL) plan from sys.dm_pdw_request_steps

-- Find the distributed query plan steps for a specific query.
-- Replace request_id with value from Step 1.

SELECT * FROM sys.dm_pdw_request_steps
WHERE request_id = 'QID####'
ORDER BY step_index;


When a DSQL plan is taking longer than expected, the cause can be a complex plan with many DSQL steps or just one step taking a long time. If the plan is many steps with several move operations, consider optimizing your table distributions to reduce data movement.

The Table distribution article explains why data must be moved to solve a query. The article also explains some distribution strategies to minimize data movement.

To investigate further details about a single step, the operation_type column of the long-running query step and note the Step Index:

Proceed with Step 3 for SQL operations: OnOperation, RemoteOperation, ReturnOperation.
Proceed with Step 4 for Data Movement operations: ShuffleMoveOperation, BroadcastMoveOperation, TrimMoveOperation, PartitionMoveOperation, MoveOperation, CopyOperation.
Step 3

Use the Request ID and the Step Index to retrieve details from sys.dm_pdw_sql_requests, which contains execution information of the query step on all of the distributed databases.

-- Find the distribution run times for a SQL step.
-- Replace request_id and step_index with values from Step 1 and 3.

SELECT * FROM sys.dm_pdw_sql_requests
WHERE request_id = 'QID####' AND step_index = 2;


When the query step is running, DBCC PDW_SHOWEXECUTIONPLAN can be used to retrieve the SQL Server estimated plan from the SQL Server plan cache for the step running on a particular distribution.

-- Find the SQL Server execution plan for a query running on a specific SQL pool or control node.
-- Replace distribution_id and spid with values from previous query.

DBCC PDW_SHOWEXECUTIONPLAN(1, 78);

Step 4

Use the Request ID and the Step Index to retrieve information about a data movement step running on each distribution from sys.dm_pdw_dms_workers.

-- Find information about all the workers completing a Data Movement Step.
-- Replace request_id and step_index with values from Step 1 and 3.

SELECT * FROM sys.dm_pdw_dms_workers
WHERE request_id = 'QID####' AND step_index = 2;

Check the total_elapsed_time column to see if a particular distribution is taking longer than others for data movement.
For the long-running distribution, check the rows_processed column to see if the number of rows being moved from that distribution is larger than others. If so, this finding might indicate skew of your underlying data. One cause for data skew is distributing on a column with many NULL values (whose rows will all land in the same distribution). Prevent slow queries by avoiding distribution on these types of columns or filtering your query to eliminate NULLs when possible.

If the query is running, you can use DBCC PDW_SHOWEXECUTIONPLAN to retrieve the SQL Server estimated plan from the SQL Server plan cache for the currently running SQL Step within a particular distribution.

-- Find the SQL Server estimated plan for a query running on a specific SQL pool Compute or control node.
-- Replace distribution_id and spid with values from previous query.

DBCC PDW_SHOWEXECUTIONPLAN(55, 238);


Dynamic Management Views (DMV) only contains 10,000 rows of data. On heavily utilized systems this means that data held in this table may be lost with hours, or even minutes as data is managed in a first in, first out system. As a result you can potentially lose meaningful information that can help you diagnose query performance issues on your system. In this situation, you should use the Query Store.

You can also monitor additional aspects of Azure Synapse SQL pools including:

Monitoring waits
Monitoring tempdb
Monitoring memory
Monitoring transaction log
Monitoring PolyBase

You can view information about monitoring these areas here




Use Azure Advisor to review recommendations - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/manage-monitor-data-warehouse-activities-azure-synapse-analytics/5-use-azure-advisor-to-review-recommendations
Use Azure Advisor to review recommendations
9 minutes

Azure Advisor provides you with personalized messages that provide information on best practices to optimize the setup of your Azure services. It analyzes your resource configuration and usage telemetry and then recommends solutions that can help you improve the cost effectiveness, performance, Reliability (formerly called High availability), and security of your Azure resources.

The Advisor may appear when you log into the Azure portal, but you can also access the Advisor by selecting Advisor in the navigation menu.

On accessing Advisor, a dashboard is presented that provides recommendations in the following areas:

Cost
Security
Reliability
Operational excellence
Performance

You can click on any of the dashboard items for more information. In the following example, the performance dashboard item is showing more information on two high impact items in Azure Synapse Analytics.

You can also click on each item to get even more information that can help you resolve the issue. In the following example, this is the information that is shown when clicking on the Create statistics on table columns recommendation.

In this screen, you can click on the view impacted tables to see which tables are being impacted specifically, and there are also links to the help in the Azure documentation that you can use to get more understanding of the issue.

How Azure Synapse Analytics works with Azure Advisor

Azure Advisor recommendations are free, and the recommendations are based on telemetry data that is generated by Azure Synapse Analytics. The telemetry data that is captured by Azure Synapse Analytics include

Data Skew and replicated table information.
Column statistics data.
TempDB utilization data.
Adaptive Cache.

Azure Advisor recommendations are checked every 24 hours, as the recommendation API is queried against the telemetry generated from with Azure Synapse Analytics, and the recommendation dashboards are then updated to reflect the information that the telemetry has generated. This can then be viewed in the Azure Advisor dashboard.




Manage workloads in Azure Synapse Analytics - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/manage-monitor-data-warehouse-activities-azure-synapse-analytics/4-manage-workloads
Manage workloads in Azure Synapse Analytics
10 minutes

Azure Synapse Analytics allows you to create, control and manage resource availability when workloads are competing. This allows you to manage the relative importance of each workload when waiting for available resources.

To facilitate faster load times, you can create a workload classifier for the load user with the importance set to above_normal or High. Workload importance ensures that the load takes precedence over other waiting tasks of a lower importance rating. Use this in conjunction with your own workload group definitions for workload isolation to manage minimum and maximum resource allocations during peak and quiet periods.

Dedicated SQL pool workload management in Azure Synapse consists of three high-level concepts:

Workload Classification
Workload Importance
Workload Isolation

These capabilities give you more control over how your workload utilizes system resources.

Workload classification

Workload management classification allows workload policies to be applied to requests through assigning resource classes and importance.

While there are many ways to classify data warehousing workloads, the simplest and most common classification is load and query. You load data with insert, update, and delete statements. You query the data using selects. A data warehousing solution will often have a workload policy for load activity, such as assigning a higher resource class with more resources. A different workload policy could apply to queries, such as lower importance compared to load activities.

You can also subclassify your load and query workloads. Subclassification gives you more control of your workloads. For example, query workloads can consist of cube refreshes, dashboard queries or ad-hoc queries. You can classify each of these query workloads with different resource classes or importance settings. Load can also benefit from subclassification. Large transformations can be assigned to larger resource classes. Higher importance can be used to ensure key sales data is loaded before weather data or a social data feed.

Not all statements are classified as they do not require resources or need importance to influence execution. DBCC commands, BEGIN, COMMIT, and ROLLBACK TRANSACTION statements are not classified.

Workload importance

Workload importance influences the order in which a request gets access to resources. On a busy system, a request with higher importance has first access to resources. Importance can also ensure ordered access to locks. There are five levels of importance: low, below_normal, normal, above_normal, and high. Requests that don't set importance are assigned the default level of normal. Requests that have the same importance level have the same scheduling behavior that exists today.

Workload isolation

Workload isolation reserves resources for a workload group. Resources reserved in a workload group are held exclusively for that workload group to ensure execution. Workload groups also allow you to define the amount of resources that are assigned per request, much like resource classes do. Workload groups give you the ability to reserve or cap the amount of resources a set of requests can consume. Finally, workload groups are a mechanism to apply rules, such as query timeout, to requests.

You can perform the following steps to implement workload management

Create a workload classifier to add importance to certain queries

Your organization has asked you if there is a way to mark queries executed by the CEO as more important than others, so they don't appear slow due to heavy data loading or other workloads in the queue. You decide to create a workload classifier and add importance to prioritize the CEO's queries.

Select the Develop hub.

From the Develop menu, select the + button (1) and choose SQL Script (2) from the context menu.

In the toolbar menu, connect to the SQL Pool database to execute the query.

In the query window, replace the script with the following to confirm that there are no queries currently being run by users logged in as asa.sql.workload01, representing the CEO of the organization or asa.sql.workload02 representing the data analyst working on the project:

--First, let's confirm that there are no queries currently being run by users logged in workload01 or workload02

SELECT s.login_name, r.[Status], r.Importance, submit_time, 
start_time ,s.session_id FROM sys.dm_pdw_exec_sessions s 
JOIN sys.dm_pdw_exec_requests r ON s.session_id = r.session_id
WHERE s.login_name IN ('asa.sql.workload01','asa.sql.workload02') and Importance
is not NULL AND r.[status] in ('Running','Suspended') 
--and submit_time>dateadd(minute,-2,getdate())
ORDER BY submit_time ,s.login_name


Select Run from the toolbar menu to execute the SQL command.

Now that we have confirmed that there are no running queries, we need to flood the system with queries and see what happens for asa.sql.workload01 and asa.sql.workload02. To do this, we'll run a Azure Synapse Pipeline which triggers queries.

Select the Integrate hub.

Select the Lab 08 - Execute Data Analyst and CEO Queries Pipeline (1), which will run / trigger the asa.sql.workload01 and asa.sql.workload02 queries. Select Add trigger (2), then Trigger now (3). In the dialog that appears, select OK.

Let's see what happened to all the queries we just triggered as they flood the system. In the query window, replace the script with the following:

SELECT s.login_name, r.[Status], r.Importance, submit_time, start_time ,s.session_id FROM sys.dm_pdw_exec_sessions s 
JOIN sys.dm_pdw_exec_requests r ON s.session_id = r.session_id
WHERE s.login_name IN ('asa.sql.workload01','asa.sql.workload02') and Importance
is not NULL AND r.[status] in ('Running','Suspended') and submit_time>dateadd(minute,-2,getdate())
ORDER BY submit_time ,status


Select Run from the toolbar menu to execute the SQL command.

You should see an output similar to the following:

Notice that the Importance level for all queries is set to normal.

We will give our asa.sql.workload01 user queries priority by implementing the Workload Importance feature. In the query window, replace the script with the following:

IF EXISTS (SELECT * FROM sys.workload_management_workload_classifiers WHERE name = 'CEO')
BEGIN
    DROP WORKLOAD CLASSIFIER CEO;
END
CREATE WORKLOAD CLASSIFIER CEO
  WITH (WORKLOAD_GROUP = 'largerc'
  ,MEMBERNAME = 'asa.sql.workload01',IMPORTANCE = High);


We are executing this script to create a new Workload Classifier named CEO that uses the largerc Workload Group and sets the Importance level of the queries to High.

Select Run from the toolbar menu to execute the SQL command.

Let's flood the system again with queries and see what happens this time for asa.sql.workload01 and asa.sql.workload02 queries. To do this, we'll run an Azure Synapse Pipeline which triggers queries. Select the Integrate Tab, run the Lab 08 - Execute Data Analyst and CEO Queries Pipeline, which will run / trigger the asa.sql.workload01 and asa.sql.workload02 queries.

In the query window, replace the script with the following to see what happens to the asa.sql.workload01 queries this time:

SELECT s.login_name, r.[Status], r.Importance, submit_time, start_time ,s.session_id FROM sys.dm_pdw_exec_sessions s 
JOIN sys.dm_pdw_exec_requests r ON s.session_id = r.session_id
WHERE s.login_name IN ('asa.sql.workload01','asa.sql.workload02') and Importance
is not NULL AND r.[status] in ('Running','Suspended') and submit_time>dateadd(minute,-2,getdate())
ORDER BY submit_time ,status desc


Select Run from the toolbar menu to execute the SQL command.

You should see an output similar to the following:

Notice that the queries executed by the asa.sql.workload01 user have a high importance.

Select the Monitor hub.

Select Pipeline runs (1), and then select Cancel recursive (2) for each running Lab 08 pipelines, marked In progress (3). This will help speed up the remaining tasks.

Reserve resources for specific workloads through workload isolation

Workload isolation means resources are reserved, exclusively, for a workload group. Workload groups are containers for a set of requests and are the basis for how workload management, including workload isolation, is configured on a system. A simple workload management configuration can manage data loads and user queries.

In the absence of workload isolation, requests operate in the shared pool of resources. Access to resources in the shared pool is not guaranteed and is assigned on an importance basis.

Given the workload requirements provided by Tailwind Traders, you decide to create a new workload group called CEODemo to reserve resources for queries executed by the CEO.

Let's start by experimenting with different parameters.

In the query window, replace the script with the following:

IF NOT EXISTS (SELECT * FROM sys.workload_management_workload_groups where name = 'CEODemo')
BEGIN
    Create WORKLOAD GROUP CEODemo WITH  
    ( MIN_PERCENTAGE_RESOURCE = 50        -- integer value
    ,REQUEST_MIN_RESOURCE_GRANT_PERCENT = 25 --  
    ,CAP_PERCENTAGE_RESOURCE = 100
    )
END


The script creates a workload group called CEODemo to reserve resources exclusively for the workload group. In this example, a workload group with a MIN_PERCENTAGE_RESOURCE set to 50% and REQUEST_MIN_RESOURCE_GRANT_PERCENT set to 25% is guaranteed 2 concurrency.

Select Run from the toolbar menu to execute the SQL command.

In the query window, replace the script with the following to create a Workload Classifier called CEODreamDemo that assigns a workload group and importance to incoming requests:

IF NOT EXISTS (SELECT * FROM sys.workload_management_workload_classifiers where  name = 'CEODreamDemo')
BEGIN
    Create Workload Classifier CEODreamDemo with
    ( Workload_Group ='CEODemo',MemberName='asa.sql.workload02',IMPORTANCE = BELOW_NORMAL);
END


This script sets the Importance to BELOW_NORMAL for the asa.sql.workload02 user, through the new CEODreamDemo Workload Classifier.

Select Run from the toolbar menu to execute the SQL command.

In the query window, replace the script with the following to confirm that there are no active queries being run by asa.sql.workload02 (suspended queries are OK):

SELECT s.login_name, r.[Status], r.Importance, submit_time,
start_time ,s.session_id FROM sys.dm_pdw_exec_sessions s
JOIN sys.dm_pdw_exec_requests r ON s.session_id = r.session_id
WHERE s.login_name IN ('asa.sql.workload02') and Importance
is not NULL AND r.[status] in ('Running','Suspended')
ORDER BY submit_time, status


Select Run from the toolbar menu to execute the SQL command.

Select the Integrate hub.

Select the Lab 08 - Execute Business Analyst Queries Pipeline (1), which will run / trigger asa.sql.workload02 queries. Select Add trigger (2), then Trigger now (3). In the dialog that appears, select OK.

In the query window, replace the script with the following to see what happened to all the asa.sql.workload02 queries we just triggered as they flood the system:

SELECT s.login_name, r.[Status], r.Importance, submit_time,
start_time ,s.session_id FROM sys.dm_pdw_exec_sessions s
JOIN sys.dm_pdw_exec_requests r ON s.session_id = r.session_id
WHERE s.login_name IN ('asa.sql.workload02') and Importance
is not NULL AND r.[status] in ('Running','Suspended')
ORDER BY submit_time, status


Select Run from the toolbar menu to execute the SQL command.

You should see an output similar to the following that shows the importance for each session set to below_normal:

Notice that the running scripts are executed by the asa.sql.workload02 user (1) with an Importance level of below_normal (2). We have successfully configured the business analyst queries to execute at a lower importance than the CEO queries. We can also see that the CEODreamDemo Workload Classifier works as expected.

Select the Monitor hub.

Select Pipeline runs (1), and then select Cancel recursive (2) for each running Lab 08 pipelines, marked In progress (3). This will help speed up the remaining tasks.

Return to the query window under the Develop hub. In the query window, replace the script with the following to set 3.25% minimum resources per request:

IF  EXISTS (SELECT * FROM sys.workload_management_workload_classifiers where group_name = 'CEODemo')
BEGIN
    Drop Workload Classifier CEODreamDemo
    DROP WORKLOAD GROUP CEODemo
    --- Creates a workload group 'CEODemo'.
        Create  WORKLOAD GROUP CEODemo WITH  
    (MIN_PERCENTAGE_RESOURCE = 26 -- integer value
        ,REQUEST_MIN_RESOURCE_GRANT_PERCENT = 3.25 -- factor of 26 (guaranteed more than 4 concurrencies)
    ,CAP_PERCENTAGE_RESOURCE = 100
    )
    --- Creates a workload Classifier 'CEODreamDemo'.
    Create Workload Classifier CEODreamDemo with
    (Workload_Group ='CEODemo',MemberName='asa.sql.workload02',IMPORTANCE = BELOW_NORMAL);
END


 Note

Configuring workload containment implicitly defines a maximum level of concurrency. With a CAP_PERCENTAGE_RESOURCE set to 60% and a REQUEST_MIN_RESOURCE_GRANT_PERCENT set to 1%, up to a 60-concurrency level is allowed for the workload group. Consider the method included below for determining the maximum concurrency: [Max Concurrency] = [CAP_PERCENTAGE_RESOURCE] / [REQUEST_MIN_RESOURCE_GRANT_PERCENT]

Select Run from the toolbar menu to execute the SQL command.

Let's flood the system again and see what happens for asa.sql.workload02. To do this, we will run an Azure Synapse Pipeline which triggers queries. Select the Integrate Tab. Run the Lab 08 - Execute Business Analyst Queries Pipeline, which will run / trigger asa.sql.workload02 queries.

In the query window, replace the script with the following to see what happened to all of the asa.sql.workload02 queries we just triggered as they flood the system:

SELECT s.login_name, r.[Status], r.Importance, submit_time,
start_time ,s.session_id FROM sys.dm_pdw_exec_sessions s
JOIN sys.dm_pdw_exec_requests r ON s.session_id = r.session_id
WHERE s.login_name IN ('asa.sql.workload02') and Importance
is  not NULL AND r.[status] in ('Running','Suspended')
ORDER BY submit_time, status


Select Run from the toolbar menu to execute the SQL command.

After several moments (up to a minute), we should see several concurrent executions by the asa.sql.workload02 user running at below_normal importance. We have validated that the modified Workload Group and Workload Classifier works as expected.

Select the Monitor hub.

Select Pipeline runs (1), and then select Cancel recursive (2) for each running Lab 08 pipelines, marked In progress (3). This will help speed up the remaining tasks.




Pause compute in Azure Synapse Analytics - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/manage-monitor-data-warehouse-activities-azure-synapse-analytics/3-pause-compute
Pause compute in Azure Synapse Analytics
3 minutes

When performing the batch movement of data to populate a data warehouse, it is typical for the data engineer to understand the schedule on which the data loads take place. In these circumstances, you may be able to predict the periods of downtime in the data loading and querying process and take advantage of the pause operations to minimize your costs.

In the Azure portal you can use the Pause command within the dedicated SQL pool

And this can also be used within Azure Synapse Studio for Apache Spark pools too, in the Manage hub.

Which allows you to enable it, and set the number of minutes idle




Scale compute resources in Azure Synapse Analytics - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/manage-monitor-data-warehouse-activities-azure-synapse-analytics/2-scale-compute-resources
Scale compute resources in Azure Synapse Analytics
8 minutes

One of the key management features that you have at your disposal within Azure Synapse Analytics, is the ability to scale the compute resources for SQL or Spark pools to meet the demands of processing your data. In SQL pools, the unit of scale is an abstraction of compute power that is known as a data warehouse unit. Compute is separate from storage, which enables you to scale compute independently of the data in your system. This means you can scale up and scale down the compute power to meet your needs.

You can scale a Synapse SQL pool either through the Azure portal, Azure Synapse Studio or programmatically using TSQL or PowerShell.

In the Azure portal, you can click on scale icon

And then you can adjust the slider to scale the SQL Pool

Another option to scale is within Azure Synapse Studio, click on the scale icon:

And then move the slider as follows:

You can also make the modification using Transact-SQL

ALTER DATABASE mySampleDataWarehouse
MODIFY (SERVICE_OBJECTIVE = 'DW300c');


Or by using PowerShell

Set-AzSqlDatabase -ResourceGroupName "resourcegroupname" -DatabaseName "mySampleDataWarehouse" -ServerName "sqlpoolservername" -RequestedServiceObjectiveName "DW300c"

Scaling Apache Spark pools in Azure Synapse Analytics

Apache Spark pools for Azure Synapse Analytics uses an Autoscale feature that automatically scales the number of nodes in a cluster instance up and down. During the creation of a new Spark pool, a minimum and maximum number of nodes can be set when Autoscale is selected. Autoscale then monitors the resource requirements of the load and scales the number of nodes up or down. To enable the Autoscale feature, complete the following steps as part of the normal pool creation process:

On the Basics tab, select the Enable autoscale checkbox.
Enter the desired values for the following properties:
Min number of nodes.
Max number of nodes.

The initial number of nodes will be the minimum. This value defines the initial size of the instance when it's created. The minimum number of nodes can't be fewer than three.

You can also modify this in the Azure portal, you can click on auto-scale settings icon

Choose the node size and the number of nodes

and for Azure Synapse Studio as follows

And Choose the node size and the number of nodes

Autoscale continuously monitors the Spark instance and collects the following metrics:

Expand table
Metric	Description
Total Pending CPU	The total number of cores required to start execution of all pending nodes.
Total Pending Memory	The total memory (in MB) required to start execution of all pending nodes.
Total Free CPU	The sum of all unused cores on the active nodes.
Total Free Memory	The sum of unused memory (in MB) on the active nodes.
Used Memory per Node	The load on a node. A node on which 10 GB of memory is used, is considered under more load than a worker with 2 GB of used memory.

The following conditions will then autoscale the memory or CPU

Expand table
Scale-up	Scale-down
Total pending CPU is greater than total free CPU for more than 1 minute.	Total pending CPU is less than total free CPU for more than 2 minutes.
Total pending memory is greater than total free memory for more than 1 minute.	Total pending memory is less than total free memory for more than 2 minutes.

The scaling operation can take between 1 -5 minutes. During an instance where there is a scale down process, Autoscale will put the nodes in decommissioning state so that no new executors can launch on that node.

The running jobs will continue to run and finish. The pending jobs will wait to be scheduled as normal with fewer available nodes.




Introduction - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/manage-monitor-data-warehouse-activities-azure-synapse-analytics/1-introduction
Introduction
3 minutes

In this module, you will learn some of the features you can use to manage and monitor Azure Synapse Analytics.

At the end of this module, you will

Scale compute resources in Azure Synapse Analytics
Pause compute in Azure Synapse Analytics
Manage workloads in Azure Synapse Analytics
Use Azure Advisor to review recommendations
Use Dynamic Management Views to identify and troubleshoot query performance
Prerequisites

Before taking this module, it is recommended that the student is able to:

Log into the Azure portal
Create a Synapse Analytics Workspace
Create an Azure Synapse Analytics SQL Pool




Manage and monitor data warehouse activities in Azure Synapse Analytics - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/manage-monitor-data-warehouse-activities-azure-synapse-analytics/

Manage and monitor data warehouse activities in Azure Synapse Analytics
Module
8 Units
Feedback
Beginner
Data Engineer
Azure Synapse Analytics

Learn how to manage and monitor Azure Synapse Analytics.

Learning objectives

In this module, you will:

Scale compute resources in Azure Synapse Analytics
Pause compute in Azure Synapse Analytics
Manage workloads in Azure Synapse Analytics
Use Azure Advisor to review recommendations
Use Dynamic Management Views to identify and troubleshoot query performance
Add
Prerequisites
Before taking this module, it is recommended that you complete Data Fundamentals.
Introduction
min
Scale compute resources in Azure Synapse Analytics
min
Pause compute in Azure Synapse Analytics
min
Manage workloads in Azure Synapse Analytics
min
Use Azure Advisor to review recommendations
min
Use dynamic management views to identify and troubleshoot query performance
min
Knowledge check
min
Summary
min


Summary - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/load-optimize-data-into-relational-data-warehouse/10-summary
Summary
1 minute

Loading data warehouse tables is a core task for data engineers. This module introduced some common SQL-based techniques that you can use to stage and load data in a relational data warehouse that's hosted in a dedicated SQL pool in Azure Synapse Analytics.




