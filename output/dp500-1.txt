Summary - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/build-reports-using-power-bi-azure-synapse-analytics/11-summary
Summary
3 minutes

You have just learned how to build reports using Power BI within Azure Synapse Analytics.

In this module, you have:

learned how to integrate a Synapse workspace and Power BI
learned how to optimize the integration with Power BI
experimented with query performance improvements with materialized views and result-set caching
visualized data with SQL serverless and created a Power BI report using serverless SQL Pool.




Knowledge check - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/build-reports-using-power-bi-azure-synapse-analytics/10-knowledge-check
Knowledge check
3 minutes
1. 

What data source connection method loads all of the data into the Power BI cache, and runs queries against the ingested data?

 

DirectQuery.

Import Connection.

Hybrid Connection

2. 

What can be considered a benefit of materialized views?

 

A different data distribution compared to the base tables.

Decreased query performance for complex queries with JOINs and aggregate functions, but increased performance for simple queries.

Instant query response times for repetitive query patterns.

3. 

What is a limitation of result-set caching?

 

The maximum size of the result set cache is 1 TB per database.

Users cant manually empty the result set cache.

Pausing the database will clean the cache.

Check your answers




Visualize data with serverless SQL pools - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/build-reports-using-power-bi-azure-synapse-analytics/9-visualize-data-serverless-sql-pools

Learn  Training  Browse  Build reports using Power BI within Azure Synapse Analytics 
Add
Previous
Unit 9 of 11
Next
Visualize data with serverless SQL pools
Completed
100 XP
6 minutes

In the Azure portal, navigate to your Synapse Workspace. In the Overview (1) tab, copy the Serverless SQL endpoint (2):

Switch back to Power BI Desktop. Create a new report, then click Get data.

Select Azure (1) on the left-hand menu, then select Azure Synapse Analytics (SQL DW) (2). Finally, click Connect (3):

Paste the endpoint to the Serverless SQL endpoint identified on the first step into the Server field (1), enter demo for the Database (2), select DirectQuery (3), then paste the query below (4) into the expanded Advanced options section of the SQL Server database dialog. Finally, click OK (5).

SQL
Copy
SELECT TOP (100) [Year]
,[Month]
,[Day]
,[TotalAmount]
,[ProfitAmount]
,[TransactionsCount]
FROM [dbo].[2019Q1Sales]


(If prompted) Select the Microsoft account (1) option on the left, Sign in (2) (with the same credentials you use for connecting to the Synapse workspace) and click Connect (3).

Select Load in the preview data window and wait for the connection to be configured.

After the data loads, select Line chart from the Visualizations menu.

Select the line chart visualization and configure it as follows to show Profit, Amount, and Transactions count by day:

Axis: Day
Values: ProfitAmount, TotalAmount
Secondary values: TransactionsCount

Select the line chart visualization and configure it to sort in ascending order by the day of transaction. To do this, select More options next to the chart visualization.

Select Sort ascending.

Select More options next to the chart visualization again.

Select Sort by, then Day.

Click Save in the top-left corner.

Specify a file name (1), such as synapse-sql-serverless, then click Save (2).

Click Publish above the saved report. Make sure that, in Power BI Desktop, you are signed in with the same account you use in the Power BI portal and in Synapse Studio. You can switch to the proper account from the right topmost corner of the window. In the Publish to Power BI dialog, select the workspace you linked to Synapse (for example, synapse-training), then click Select.

Wait until the publish operation successfully completes.

In Azure Synapse Studio, navigate to the Develop hub.

Expand the Power BI group, expand your Power BI linked service (for example, synapse-training), right-click on Power BI reports and select Refresh (1) to update the list of reports. You should see the two Power BI reports you created in this lab (synapse-lab and synapse-sql-serverless (2)).

Select the synapse-lab report. You can view and edit the report directly within Synapse Studio!

Select the synapse-sql-serverless report. You should be able to view and edit this report as well.

Next unit: Knowledge check

Continue

Having an issue? We can help!

For issues related to this module, explore existing questions using the #azure training tag or Ask a question ​on Microsoft Q&A .
For issues related to Certifications and Exams, post on Credentials Support Forum or visit our Credentials Help.

Feedback

Was this page helpful?

Yes
No


Exercise - Improve performance with materialized views and result-set caching - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/build-reports-using-power-bi-azure-synapse-analytics/8-exercise-improve-performance-materialized-views-result-set-caching
Exercise - Improve performance with materialized views and result-set caching
7 minutes
Task 1: Improve performance with materialized views

In Azure Synapse Studio, select Develop from the left-hand menu.

Select +, then SQL script.

Connect to SQLPool01, then execute the following query to get an estimated execution plan and observe the total cost and number of operations:

SQL
Copy
EXPLAIN
SELECT * FROM
(
    SELECT
    FS.CustomerID
    ,P.Seasonality
    ,D.Year
    ,D.Quarter
    ,D.Month
    ,avg(FS.TotalAmount) as AvgTotalAmount
    ,avg(FS.ProfitAmount) as AvgProfitAmount
    ,sum(FS.TotalAmount) as TotalAmount
    ,sum(FS.ProfitAmount) as ProfitAmount
FROM
    wwi.SaleSmall FS
    JOIN wwi.Product P ON P.ProductId = FS.ProductId
    JOIN wwi.Date D ON FS.TransactionDateId = D.DateId
GROUP BY
    FS.CustomerID
    ,P.Seasonality
    ,D.Year
    ,D.Quarter
    ,D.Month
) T


The results should look similar to this:

XML
Copy
<?xml version="1.0" encoding="utf-8"?>
<dsql_query number_nodes="1" number_distributions="60" number_distributions_per_node="60">
    <sql>SELECT count(*) FROM
(
    SELECT
    FS.CustomerID
    ,P.Seasonality
    ,D.Year
    ,D.Quarter
    ,D.Month
    ,avg(FS.TotalAmount) as AvgTotalAmount
    ,avg(FS.ProfitAmount) as AvgProfitAmount
    ,sum(FS.TotalAmount) as TotalAmount
    ,sum(FS.ProfitAmount) as ProfitAmount
FROM
    wwi.SaleSmall FS
    JOIN wwi.Product P ON P.ProductId = FS.ProductId
    JOIN wwi.Date D ON FS.TransactionDateId = D.DateId
GROUP BY
    FS.CustomerID
    ,P.Seasonality
    ,D.Year
    ,D.Quarter
    ,D.Month
) T</sql>
    <dsql_operations total_cost="10.61376" total_number_operations="12">


Replace the query with the following to create a materialized view that can support the above query:

SQL
Copy
IF EXISTS(select * FROM sys.views where name = 'mvCustomerSales')
    DROP VIEW wwi_perf.mvCustomerSales
    GO

CREATE MATERIALIZED VIEW
    wwi_perf.mvCustomerSales
WITH
(
    DISTRIBUTION = HASH( CustomerId )
)
AS
SELECT
    S.CustomerId
    ,D.Year
    ,D.Quarter
    ,D.Month
    ,SUM(S.TotalAmount) as TotalAmount
    ,SUM(S.ProfitAmount) as TotalProfit
FROM
    [wwi_perf].[Sale_Partition02] S
    join [wwi].[Date] D on
        S.TransactionDateId = D.DateId
GROUP BY
    S.CustomerId
    ,D.Year
    ,D.Quarter
    ,D.Month
GO


This query will take between 30 and 120 seconds to complete.

We first drop the view if it exists, since we create it in an earlier lab.

Run the following query to check that it actually hits the created materialized view.

SQL
Copy
EXPLAIN
SELECT * FROM
(
SELECT
FS.CustomerID
,P.Seasonality
,D.Year
,D.Quarter
,D.Month
,avg(FS.TotalAmount) as AvgTotalAmount
,avg(FS.ProfitAmount) as AvgProfitAmount
,sum(FS.TotalAmount) as TotalAmount
,sum(FS.ProfitAmount) as ProfitAmount
FROM
    wwi_pbi.SaleSmall FS
    JOIN wwi_pbi.Product P ON P.ProductId = FS.ProductId
    JOIN wwi_pbi.Date D ON FS.TransactionDateId = D.DateId
GROUP BY
    FS.CustomerID
    ,P.Seasonality
    ,D.Year
    ,D.Quarter
    ,D.Month
) T



Switch back to the Power BI Desktop report, then click on Transform data.

In the Power Query editor, open the settings page of the Source (1) step in the query. Expand the Advanced options (2) section, paste the following query (3) to use the new materialized view, then click OK (4).

SQL
Copy
SELECT [CustomerID]
,[Seasonality]
,[Year]
,[Quarter]
,[Month]
,[TotalAmount]
,[ProfitAmount]
,[cb]
FROM [wwi].[mvCustomerSales]


Select Close & Apply on the topmost left corner of the editor window to apply the query and fetch the initial schema in the Power BI designer window.

Click the Refresh button above the report to submit the query against the new materialized view.

Notice that the data refresh only takes a few seconds now, compared to before.

Check the duration of the query again in Synapse Studio, in the monitoring hub (1), under SQL requests (2). Notice that the Power BI queries using the new materialized view run much faster (Duration ~ 10s) (3).

Task 2: Improve performance with result-set caching

In Azure Synapse Studio, select Develop from the left-hand menu.

Select +, then SQL script.

Connect to SQLPool01, then execute the following query to check if result set caching is turned on in the current SQL pool:

SQL
Copy
SELECT
    name
    ,is_result_set_caching_on
FROM
    sys.databases


If False is returned for SQLPool01, execute the following query to activate it (you need to run it on the master database):

SQL
Copy
ALTER DATABASE [SQLPool01]
SET RESULT_SET_CACHING ON


Connect to SQLPool01 and use the master database:

 Important

The operations to create result set cache and retrieve data from the cache happen on the control node of a Synapse SQL pool instance. When result set caching is turned ON, running queries that return large result set (for example, >1GB) can cause high throttling on the control node and slow down the overall query response on the instance. Those queries are commonly used during data exploration or ETL operations. To avoid stressing the control node and cause performance issue, users should turn OFF result set caching on the database before running those types of queries.

Next move back to the Power BI Desktop report and hit the Refresh button to submit the query again.

After the data refreshes, hit Refresh once more to ensure we hit the result set cache.

Check the duration of the query again in Synapse Studio, in the Monitoring hub (1) - SQL Requests (2) page. Notice that now it runs almost instantly (Duration = 0s) (4).




Describe Power BI optimization options - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/build-reports-using-power-bi-azure-synapse-analytics/7-describe-optimization-options
Describe Power BI optimization options
7 minutes

As the amount of data stored and queried continues to rise, it becomes increasingly important to have the most price-performant data warehouse. To improve the performance of applications and reports without adding any more cost, result-set caching and materialized views are second to none.

Result-set caching

Result-set caching helps with predictable and repetitive query patterns and enables instant query response times while reducing time-to-insight for data analysts and reporting users. Result-set caching is an effective solution for interactive dashboard performance improvement.

With result-set caching enabled, Azure Synapse Analytics automatically caches results from repetitive queries, causing subsequent query executions to return results from the persisted cache that omits complete query execution. In addition to saving compute cycles, queries satisfied by result-set cache do not use any concurrency slots and thus do not count against existing concurrency limits. For security reasons, only users with the appropriate security credentials can access the result sets in cache.

Materialized views to improve performance

Another feature that significantly enhances query performance for a broad set of queries is materialized view support. A materialized view improves complex queries' performance (typically queries with joins and aggregations) while offering simple maintenance operations.

When materialized views are created, the Azure Synapse Analytics dedicated SQL Pool query optimizer transparently and automatically rewrite user queries to leverage deployed materialized views, leading to improved query performance. Best of all, as the data gets loaded into base tables, the query optimizer automatically maintains and refreshes materialized views, providing easier maintenance and management. As the user queries use materialized views, queries run significantly faster and use fewer system resources. A materialized view pre-computes, stores, and maintains its data in a dedicated SQL pool just like a table. There's no recomputation needed each time a materialized view is used. That's why queries that use all or a subset of the data in a materialized view can get faster performance. The more complex and expensive the query within the view is, the more significant potential for execution-time savings.

A properly designed materialized view provides the following benefits:

Reduce the execution time for complex queries with JOINs and aggregate functions. The more complex the query, the higher the potential for execution-time saving. The most benefit is gained when a query's computation cost is high and the resulting data set is small.
The optimizer in the dedicated SQL pool can automatically use deployed materialized views to improve query execution plans. This process is transparent to users providing faster query performance and doesn't require queries to make direct reference to the materialized views.
Materialized views require low maintenance. All incremental data changes from the base tables are automatically added to the materialized views in a synchronous manner. This design allows querying materialized views to return the same data as directly querying the base tables.
The data in a materialized view can be distributed differently from the base tables.
Data in materialized views gets the same high availability and resiliency benefits as data in regular tables.

The materialized views implemented in a dedicated SQL pool also provide the following additional benefits:

Comparing to other data warehouse providers, the materialized views implemented in Azure Synapse Analytics also provide the following additional benefits:

Automatic and synchronous data refresh with data changes in base tables. No user action is required.
Broad aggregate function support. See CREATE MATERIALIZED VIEW AS SELECT (Transact-SQL).
The support for query-specific materialized view recommendation. See EXPLAIN (Transact-SQL).




Exercise - Create a new Power BI report in Synapse Studio - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/build-reports-using-power-bi-azure-synapse-analytics/6-exercise-create-new-report-synapse-studio

Learn  Training  Browse  Build reports using Power BI within Azure Synapse Analytics 
Add
Previous
Unit 6 of 11
Next
Exercise - Create a new Power BI report in Synapse Studio
Completed
100 XP
6 minutes

In Azure Synapse Studio, select Develop from the left menu.

Select + (1), then SQL script (2).

Connect to SQLPool01, then execute the following query to get an approximation of its execution time. This will be the query we'll use to bring data in the Power BI report you'll build later in this exercise.

SQL
Copy
SELECT count(*) FROM
(
    SELECT
        FS.CustomerID
        ,P.Seasonality
        ,D.Year
        ,D.Quarter
        ,D.Month
        ,avg(FS.TotalAmount) as AvgTotalAmount
        ,avg(FS.ProfitAmount) as AvgProfitAmount
        ,sum(FS.TotalAmount) as TotalAmount
        ,sum(FS.ProfitAmount) as ProfitAmount
    FROM
        wwi.SaleSmall FS
        JOIN wwi.Product P ON P.ProductId = FS.ProductId
        JOIN wwi.Date D ON FS.TransactionDateId = D.DateId
    GROUP BY
        FS.CustomerID
        ,P.Seasonality
        ,D.Year
        ,D.Quarter
        ,D.Month
) T


You should see a query result of 194683820.

To connect to your data source, open the downloaded .pbids file in Power BI Desktop. Select the Microsoft account (1) option on the left, Sign in (2) (with the same credentials you use for connecting to the Synapse workspace) and click Connect (3).

In the Navigator dialog, right-click on the root database node and select Transform data.

Select the DirectQuery (1) option in the connection settings dialog since our intention is not to bring a copy of the data into Power BI but to be able to query the data source while working with the report visualizations. Click OK (2) and wait a few seconds while the connection is configured.

In the Power Query editor, open the settings page of the Source (1) step in the query. Expand the Advanced options (2) section, paste the following query and click OK (3).

SQL
Copy
SELECT * FROM
(
    SELECT
        FS.CustomerID
        ,P.Seasonality
        ,D.Year
        ,D.Quarter
        ,D.Month
        ,avg(FS.TotalAmount) as AvgTotalAmount
        ,avg(FS.ProfitAmount) as AvgProfitAmount
        ,sum(FS.TotalAmount) as TotalAmount
        ,sum(FS.ProfitAmount) as ProfitAmount
    FROM
        wwi.SaleSmall FS
        JOIN wwi.Product P ON P.ProductId = FS.ProductId
        JOIN wwi.Date D ON FS.TransactionDateId = D.DateId
    GROUP BY
        FS.CustomerID
        ,P.Seasonality
        ,D.Year
        ,D.Quarter
        ,D.Month
) T


 Note

This step will take at least 40-60 seconds to execute since it submits the query directly on the Synapse SQL Pool connection.

Select Close & Apply on the topmost left corner of the editor window to apply the query and fetch the initial schema in the Power BI designer window.

Back to the Power BI report editor, expand the Visualizations menu on the right, then select the Line and stacked column chart visualization.

Select the newly created chart to expand its properties pane. Using the expanded Fields menu, configure the visualization as follows:

Shared axis: Year, Quarter
Column series: Seasonality
Column values: TotalAmount
Line values: ProfitAmount

 Note

It will take around 40-60 seconds for the visualization to render due to the live query execution on the Synapse dedicated SQL pool.

Switching back to Azure Synapse Studio, you can check the query executed while configuring the visualization in the Power BI Desktop application. Open the Monitor hub, and under the Activities section, open the SQL requests monitor. Make sure you select SQLPool01 in the Pool filter, as by default, SQL on-demand is selected.

Identify the query behind your visualization in the topmost requests you see in the log and observe the duration, which is about 30 seconds (1). Use the Request content (2) option to look into the actual query submitted from Power BI Desktop.

Switch back to the Power BI Desktop application, then click Save in the top-left corner.

Specify a file name, such as synapse-lab (1), then click Save (2).

Click Publish above the saved report. Make sure that, in Power BI Desktop, you are signed in with the same account you use in the Power BI portal and in Synapse Studio. You can switch to the proper account from the topmost right corner of the window. In the Publish to Power BI dialog, select the workspace you linked to Synapse (for example, synapse-training), then click Select.

Wait until the publish operation successfully completes.

After the operation successfully completes, you should be able to see this report (2) published in the Power BI portal, as well as in Synapse Studio. To view it in Synapse Studio, navigate to the Develop hub and refresh (1) the Power BI reports node.

Next unit: Describe Power BI optimization options

Continue

Having an issue? We can help!

For issues related to this module, explore existing questions using the #azure training tag or Ask a question ​on Microsoft Q&A .
For issues related to Certifications and Exams, post on Credentials Support Forum or visit our Credentials Help.

Feedback

Was this page helpful?

Yes
No


Exercise - Create a new data source to use in Power BI - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/build-reports-using-power-bi-azure-synapse-analytics/5-exercise-create-new-data-source-to-use

Learn  Training  Browse  Build reports using Power BI within Azure Synapse Analytics 
Add
Previous
Unit 5 of 11
Next
Exercise - Create a new data source to use in Power BI
Completed
100 XP
3 minutes

Beneath Power BI, under the linked Power BI workspace, select Power BI datasets (1).

Select New Power BI dataset (2) from the top actions menu.

Select Start and make sure you have Power BI Desktop installed on your environment machine.

Select SQLPool01, then select Continue.

Next, select Download to download the .pbids file.

Select Continue, then Close and refresh to close the publishing dialog.

Now, you have a new Power BI Dataset.

Next unit: Exercise - Create a new Power BI report in Synapse Studio

Continue

Having an issue? We can help!

For issues related to this module, explore existing questions using the #azure training tag or Ask a question ​on Microsoft Q&A .
For issues related to Certifications and Exams, post on Credentials Support Forum or visit our Credentials Help.

Feedback

Was this page helpful?

Yes
No


Exercise - Connect to Power BI from Synapse - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/build-reports-using-power-bi-azure-synapse-analytics/3-exercise-connect-to-from-synapse
Exercise - Connect to Power BI from Synapse
4 minutes
Task 1: Create a Power BI workspace

In a new browser tab, navigate to https://powerbi.microsoft.com/.

Sign in with the same account used to sign in to Azure by selecting the Sign in link on the upper-right corner.

If this is your first time signing into this account, complete the setup wizard with the default options.

Select Workspaces, then select Create a workspace.

If you are prompted to upgrade to Power BI Pro, select Try free.

Select Got it to confirm the pro subscription.

Set the name to synapse-training, then select Save.

Task 2: Connect to Power BI from Synapse

Open Synapse Studio (https://web.azuresynapse.net/), and then navigate to the Manage hub.

Select Linked services on the left-hand menu, then select + New.

Select Power BI, then select Continue.

In the dataset properties form, complete the following:

Expand table
Field	Value
Name (1)	enter handson_powerbi
Workspace name (2)	select synapse-training

Select Create (3).

Task 3: Explore the Power BI linked service in Synapse Studio

In Azure Synapse Studio and navigate to the Develop hub using the left menu option.

Expand Power BI, expand SynapseDemos (or synapse-training, named after your resource group) and observe that you have access to your Power BI datasets and reports, directly from Synapse Studio.

New reports can be created by selecting + at the top of the Develop tab. Existing reports can be edited by selecting the report name. Any saved changes will be written back to the Power BI workspace.




Understand Power BI data sources - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/build-reports-using-power-bi-azure-synapse-analytics/4-understand-data-sources
Understand Power BI data sources
6 minutes

Power BI reports can connect to a number of data sources. Depending on how data is used, different data sources are available. There is a large number of varied data sources available for Power BI reports, encompassing:

Online services (Salesforce, Dynamics 365, others)
Databases (SQL Server, Access, Amazon Redshift, others)
Simple files (Excel, JSON, others)
Other data sources (Spark, Web sites, Microsoft Exchange, others)

You can connect to all sorts of different data sources when using Power BI Desktop or the Power BI service, and make those data connections in different ways. You can import data to Power BI, which is the most common way to get data, or connect directly to data in the original source repository, which is known as DirectQuery.

Import connections

You can get data from any of the data sources in Power BI by selecting Get Data in the bottom-left corner of the page.

After you select Get Data, you can choose the data you want to access

For import, when using Get Data in Power BI Desktop to connect to a data source like SQL Server, the behavior of that connection is as follows:

During the initial Get Data experience, the set of tables selected each define a query that will return a set of data. Those queries can be edited before loading the data, for example, to apply filters, or aggregate the data, or join different tables.
Upon load, all of the data defined by those queries will be imported into the Power BI cache.
Upon building a visual within Power BI Desktop, the imported data will be queried. The Power BI store ensures the query will be fast. All changes to the visual are reflected immediately.
Any changes to the underlying data aren't reflected in any visuals. It's necessary to Refresh to reimport data.
Upon publishing the report as a .pbix file to the Power BI service, a dataset is created and uploaded to the Power BI service. The imported data is included with that dataset. It's then possible to schedule refresh of that data, for example, to reimport the data every day. Depending upon the location of the original data source, it might be necessary to configure an on-premises data gateway.
When opening an existing report in the Power BI service, or authoring a new report, the imported data is queried again, ensuring interactivity.
Visuals, or entire report pages, can be pinned as dashboard tiles. The tiles automatically refresh whenever the underlying dataset refreshes.
DirectQuery connections

For DirectQuery, when using Get Data in Power BI Desktop to connect to a data source, the behavior of that connection is as follows:

During the initial Get Data experience, the source is selected. For relational sources, a set of tables are selected and each still define a query that logically returns a set of data. For multidimensional sources, like SAP BW, only the source is selected.
However, upon load, no data is imported into the Power BI store. Instead, upon building a visual within Power BI Desktop, queries are sent to the underlying data source to retrieve the necessary data. The time taken to refresh the visual depends on the performance of the underlying data source.
Any changes to the underlying data aren't immediately reflected in any existing visuals. It's still necessary to refresh. The necessary queries are resent for each visual, and the visual is updated as necessary.
Upon publishing the report to the Power BI service, it will again result in a dataset in the Power BI service, the same as for import. However, no data is included with that dataset.
When opening an existing report in the Power BI service, or authoring a new one, the underlying data source is again queried to retrieve the necessary data. Depending upon the location of the original data source, it might be necessary to configure an on-premises data gateway, as is needed for import mode if the data is refreshed.
Visuals, or entire report pages, can be pinned as Dashboard tiles. To ensure that opening a dashboard is fast, the tiles are automatically refreshed on a schedule, for example, every hour. The frequency of this refresh can be controlled, to reflect how frequently the data is changing, and how important it's to see the latest data. When opening a dashboard, the tiles reflect the data at the time of the last refresh, and not necessarily the latest changes made to the underlying source. You can refresh an open dashboard to ensure it's current.
Live connections

When connecting to SQL Server Analysis Services, there's an option to either import data from or connect live to, the selected data model. If you use import, you define a query against that external SQL Server Analysis Services source, and the data is imported as normal. If you use connect live, there's no query defined, and the entire external model is shown in the field list.




Describe the Power BI and Synapse workspace integration - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/build-reports-using-power-bi-azure-synapse-analytics/2-describe-synapse-workspace-integration
Describe the Power BI and Synapse workspace integration
5 minutes

The combination of Azure Synapse Analytics and Power BI opens exciting possibilities for bridging the gap between massive volumes of structured or unstructured data and actionable insights. You can create new reports or consume your existing Power BI datasets and reports, all within Synapse by simply linking your Power BI and Synapse workspaces. You can make any edits to your reports in Synapse and the changes will instantly propagate to all users. This tight integration between Power BI and Azure Synapse greatly reduces time to insights as Data Analysts, Data Scientists, Data Engineers and business users can collaborate easily in a single environment.

A data warehouse is a centralized repository of integrated data from one or more disparate sources. Data warehouses store current and historical data and are used for reporting and analysis of the data. A Power BI linked service in Azure Synapse Analytics helps your reports access data in a data lake through serverless SQL pools or taps into tables in a dedicated SQL Pool quickly. This capability provides a large selection of data from multiple sources. Moreover, having the authoring tools in Azure Synapse Studio helps to craft the connection between data in the data lake or dedicated SQL pool much more intuitively.

To move data into a data warehouse, data is periodically extracted from various sources that contain essential business information. As the data is moved, it can be formatted, cleaned, validated, summarized, and reorganized with Azure Synapse Pipelines and Dataflows. Alternatively, the data can be stored in the lowest level of detail, with aggregated materialized views provided in Azure Synapse for reporting. In either case, Azure Synapse becomes a permanent data store for reporting, analysis, and business intelligence (BI), helping ‘align and combine’ data from multiple sources and enforcing consistent data standards when needed.




Introduction - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/build-reports-using-power-bi-azure-synapse-analytics/1-introduction
Introduction
3 minutes

In this lesson, you will learn how to integrate Power BI with your Azure Synapse workspace to build Power BI reports. You will create a new data source and Power BI report in Azure Synapse Studio. Then you will learn how to improve query performance with materialized views and result-set caching. Finally, you will explore the data lake with serverless SQL pools and create visualizations against that data in Power BI.

After completing the lesson, you will be able to:

Integrate an Azure Synapse workspace and Power BI
Optimize integration with Power BI
Improve query performance with materialized views and result-set caching
Visualize data with serverless SQL pools and create a Power BI report

Before taking this lesson, it is recommended that you can:

Login to the Azure portal
Create a Azure Synapse Analytics Workspace
Create and connect to an Azure Synapse Analytics SQL Pool
Create a Power BI Workspace




Build reports using Power BI within Azure Synapse Analytics - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/build-reports-using-power-bi-azure-synapse-analytics/

Build reports using Power BI within Azure Synapse Analytics
Module
11 Units
Feedback
Intermediate
Data Analyst
Data Engineer
Azure

In this module, you will learn how you can build Power BI reports from within Azure Synapse Analytics.

Learning objectives

In this module, you'll:

Describe the Power BI and Synapse workspace integration
Understand Power BI data sources
Describe optimization options
Visualize data with serverless SQL pools
Add
Prerequisites
It's recommended that students understand how Azure Synapse Analytics and Power BI work.
Introduction
min
Describe the Power BI and Synapse workspace integration
min
Exercise - Connect to Power BI from Synapse
min
Understand Power BI data sources
min
Exercise - Create a new data source to use in Power BI
min
Exercise - Create a new Power BI report in Synapse Studio
min
Describe Power BI optimization options
min
Exercise - Improve performance with materialized views and result-set caching
min
Visualize data with serverless SQL pools
min
Knowledge check
min
Summary
min


settings.png (624×351)
https://learn.microsoft.com/en-us/training/modules/power-bi-admin-reach/media/settings.png#lightbox


app-template-settings.png (335×336)
https://learn.microsoft.com/en-us/training/modules/power-bi-admin-reach/media/app-template-settings.png#lightbox


appsource.png (1092×717)
https://learn.microsoft.com/en-us/training/modules/power-bi-admin-reach/media/appsource.png#lightbox


admin-portal.png (624×384)
https://learn.microsoft.com/en-us/training/modules/power-bi-admin-reach/media/admin-portal.png#lightbox


dataflow-components.png (1061×654)
https://learn.microsoft.com/en-us/training/modules/power-bi-admin-reach/media/dataflow-components.png#lightbox


dataflow.png (946×505)
https://learn.microsoft.com/en-us/training/modules/power-bi-admin-reach/media/dataflow.png#lightbox


developer-settings.png (857×617)
https://learn.microsoft.com/en-us/training/modules/power-bi-admin-reach/media/developer-settings.png#lightbox


sharepoint-online.png (1025×311)
https://learn.microsoft.com/en-us/training/modules/power-bi-admin-reach/media/sharepoint-online.png#lightbox


power-bi-teams.png (518×429)
https://learn.microsoft.com/en-us/training/modules/power-bi-admin-reach/media/power-bi-teams.png#lightbox


share-content.png (975×375)
https://learn.microsoft.com/en-us/training/modules/power-bi-admin-reach/media/share-content.png#lightbox


Summary - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/power-bi-admin-reach/13-summary
Summary
1 minute

 You learned about the following topics in this module:    

Describe the various embedding scenarios that allow you to broaden the reach of Power BI

Understand the options for developers to customize Power BI solutions

Learn to provision and optimize Power BI embedded capacity and create and deploy dataflows

Build custom Power BI solutions template apps




Check your knowledge - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/power-bi-admin-reach/12-check
Check your knowledge
4 minutes
Answer the following questions to see what you've learned.
1. 

Which of the following embed scenario allows you to easily embed interactive Power BI content in blog posts, websites, emails, or social media?

 

Embed in PowerApps

SharePoint Online web part

Publish to web

Embed in Microsoft Teams

2. 

In what resource would you find client tools that provide the ability to produce and consume .NET packages?

 

.NET resource library

SharePoint Online gallery

REST API directory

NuGet gallery

3. 

Which of the following is NOT a recognized option to create or build on top of a dataflow?

 

Use define new tables

Use linked tables

Use entity tables

Use a computed table

4. 

Power BI Template Apps allow Power BI Pro and Power BI Premium users to gain immediate insights through prepackaged dashboards and reports that can be connected to what source?

 

SharePoint Online

.NET gallery

O365

Live data sources

Check your answers




Template app governance - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/power-bi-admin-reach/10-template-app-governance
Template app governance
1 minute

The primary governance role regarding template apps is to determine who can install first- and third-party applications from both AppSource and Direct links.

By default AppSource installation settings are set to on in the Admin portal, and direct links settings by default are set to off.

The last key governance task entails controlling who can publish template apps outside of the organization. This setting in the admin portal is set to off by default.




Template apps - installed entities - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/power-bi-admin-reach/9-template-apps-entities
Template apps - installed entities
1 minute

When using a template app, the app will be transferred to your Power BI tenant and you can use it like any other report or Power BI entity. Plus, if there is a newer version of an app you're using, you'll get a notification. You can either install the new version side by side or you can overwrite the previous one.

Every time you install another version of an app you will be getting more workspace.




Template apps - install packages - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/power-bi-admin-reach/8-template-apps
Template apps - install packages
1 minute

The new Power BI template apps enable Power BI partners to build Power BI apps with little or no coding and deploy them to any Power BI customer. Power BI Template Apps allow Power BI Pro or Power BI Premium users to gain immediate insights through prepackaged dashboards and reports that can be connected to live data sources. 

Many Power BI Apps are already available in the Power BI Apps marketplace.

In addition to providing insights for your organization, there are many apps that have been published for usage monitoring.

There are other aspects associated with template apps that should be noted. For example:

In the pro user experience, you can customize everything. Once an app is installed in your tenant, it is similar to any other content you have in Power BI.

You will get continuous updates to content and you can either install side by side or overwrite it.

Administrators have authority to define who in the organization can install apps.




Dataflow capabilities on Power BI Premium - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/power-bi-admin-reach/7-dataflow-power-bi-premium
Dataflow capabilities on Power BI Premium
1 minute

There are certain dataflow capabilities that are only available in Power BI Premium. The chart below provides a listing and comparison of capabilities between Power BI Pro and Power BI Premium.




Create a Dataflow - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/power-bi-admin-reach/6-create-dataflow
Create a Dataflow
1 minute

To create a dataflow, launch the Power BI service in a browser then select a workspace (dataflows are not available in my-workspace in the Power BI service) from the navigation pane on the left. You can also create a new workspace in which to create your new dataflow.

There are multiple ways to create or build on top of a new dataflow:

Create a dataflow using define new tables - Using the Define new tables option allows you to define a new table and connect to a new data source.

Create a dataflow using linked tables - Creating a dataflow using linked tables enables you to reference an existing table, defined in another dataflow, in a read-only fashion.

Create a dataflow using import/export - Creating a dataflow using import/export lets you import a dataflow from a file. This is useful if you want to save a dataflow copy offline or move a dataflow from one workspace to another.

Create a dataflow by attaching a common data model folder - Creating a dataflow from a CDM folder allows you to reference a table that has been written by another application in the Common Data Model (CDM) format. Using this method has several requirements including:

Creator must possess the storage blob data owner role of the storage account.

Creator must have read access and execute access control lists (ACLs) on both the CDM and any files or folders within it.




Dataflow explained - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/power-bi-admin-reach/5-dataflow-explained
Dataflow explained
1 minute

Dataflows are created and managed in workspaces by using the Power BI service. Data is stored as entities in the Common Data Model in Azure Data Lake Storage Gen2.

With dataflows, you can ingest data from both on-premises or cloud data sources, and you have the ability map, edit, and extend standard entities. You can also create custom entities and to create Power BI datasets using a dataflow. Lastly, it's required that a dataflow is refreshed before it can be consumed in a dataset inside Power BI desktop or referenced as a linked or computed table.

For more information on configuring and consuming dataflows, see this article.




Dataflow introduction - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/power-bi-admin-reach/4-dataflow
Dataflow introduction
3 minutes

A dataflow is a collection of tables that are created and managed in workspaces in the Power BI service. A table is a set of columns that are used to store data, much like a table within a database. You can add and edit tables in your dataflow, and manage data refresh schedules, directly from the workspace in which your dataflow was created.

Dataflows are designed to support the following scenarios:

Create reusable transformation logic that can be shared by many datasets and reports inside Power BI. Dataflows promote reusability of the underlying data elements, preventing the need to create separate connections with your cloud or on-premises data sources.

Expose the data in your own Azure Data Lake Gen 2 storage, enabling you to connect other Azure services to the raw underlying data.

Create a single source of the truth by forcing analysts to connect to the dataflows, rather than connecting to the underlying systems, providing you with control over which data is accessed, and how data is exposed to report creators. You can also map the data to industry standard definitions, enabling you to create tidy curated views, which can work with other services and products in Microsoft Power Platform.

If you want to work with large data volumes and perform ETL at scale, dataflows with Power BI Premium scales more efficiently and gives you more flexibility. Dataflows supports a wide range of cloud and on-premises sources.

Prevent analysts from having direct access to the underlying data source. Since report creators can build on top of dataflows, it may be more convenient for you to allow access to underlying data sources only to a few individuals, and then provide access to the dataflows for analysts to build on top of. This approach reduces the load to the underlying systems and gives administrators finer control of when the systems get loaded from refreshes.

Once you've created a dataflow, you can use Power BI Desktop and the Power BI service to create datasets, reports, dashboards, and apps that use the Common Data Model to drive deep insights into your business activities. Dataflow refresh scheduling is managed directly from the workspace in which your dataflow was created, just like your datasets.

To create a dataflow, launch the Power BI service in a browser then select a workspace (dataflows are not available in my-workspace in the Power BI service) from the navigation pane on the left, as shown in the following screen. You can also create a new workspace in which to create your new dataflow.

The next two steps are to schedule a refresh and build the dataset using your Power BI desktop.

For more information or instructions on how to create a dataflow, see the following article.




Provision a Power BI embedded capacity - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/power-bi-admin-reach/3-embedded-capacity
Provision a Power BI embedded capacity
1 minute

Power BI Embedded simplifies Power BI capabilities by helping users quickly add stunning visuals, reports, and dashboards to apps. A Power BI embedded capacity is created in the Azure portal and assigned in the Power BI service. Requirements to provision Power BI embedded capacity include the need to be signed in as a user in the Power BI tenant and have an active Azure subscription. By default, user is signed in as a capacity administrator and therefore can add other users. Once you've provisioned Power BI embedded capacity, you can add more as your needs grow. You can use any combination of Premium capacity SKUs (P1 through P3) within your organization.

For additional information on how to provision a Power BI embedded capacity, view the following video.




REST API custom development - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/power-bi-admin-reach/2-rest-api-custom
REST API custom development
1 minute

There are several REST APIs developers can use to build solutions. Azure Active Directory (AD) REST APIs and AD libraries for multiple platforms can be used for authentication. Power BI REST APIs provide programmatic access to objects in a Power BI Report Server catalog and the .NET and JavaScript SDKs can be used to embed reports into an application for your organization.

NuGet packages

The NuGet client tools provide the ability to produce and consume .NET packages. The NuGet Gallery is the central package repository used by all package authors and consumers.

There are several NuGet packages available to Power BI developers, including:

Azure AD Authentication Library - Microsoft.IdentityModel.Clients.ActiveDirectory: This package includes the AD Authentication Library (ADAL) and provides the authentication functionality for a .NET client.

Power BI .NET SDK - Microsoft.PowerBI.API: This package is a .NET client library for Microsoft Power BI public REST endpoints and provides access to workspaces, content identifiers (GUIDS) for datasets, reports, dashboards, and tiles.

JavaScript SDK - Microsoft.PowerBI.JavaScript: This package is a suite of JavaScript web components for integrating Power BI into apps, including the powerbi.js script, referenced by web pages to enable client-side functionality.




Introduction - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/power-bi-admin-reach/1-introduction
Introduction
3 minutes

There are various ways to share your Power BI reports beyond your tenant. The following is a list of embedding scenarios you may elect to use. We will examine each option through the remainder this module.

Publish to web

Embed in Microsoft Teams

SharePoint Online web part

Embed in PowerApps

There is also a special version of Power BI that can be initiated through the Azure portal called Power BI Embedded (PBIE). PBIE allows application developers to embed fully interactive reports into their applications. Microsoft developed Power BI Embedded for independent software vendors (ISVs) wanting to embed visuals into their applications to help their customers make analytic decisions and spares ISVs from having to build their own analytics' solutions. 

For more information about PBIE, see this article.

Publish to web

With the Power BI Publish to web option, you can easily embed interactive Power BI content in blog posts, websites, emails, or social media. You can also easily edit, update, refresh, or stop sharing your published visuals.

 Note

Publish to web is disabled by default. Power BI admins can enable this feature in the Admin portal > tenant settings > export and sharing settings.

 Warning

Proceed with caution. When you use Publish to web, anyone on the Internet can view your published report or visual. Viewing requires no authentication. These factors of Publish to web make it an excellent option for publishing public relations material. It includes viewing detail-level data that your reports aggregate. Before publishing a report, make sure it's okay for you to share the data and visualizations publicly. Don't publish confidential or proprietary information. If in doubt, check your organization's policies before publishing.

Access the publish to web option through the Power BI admin portal.

For more information on the publish to web option, see this article.

Embed in Microsoft Teams

You can embed Power BI reports in Microsoft Teams. In your Microsoft Teams channel, select the + sign and choose the Power BI option as a tab. Microsoft Teams automatically detects all the reports in your Power BI groups and in My Workspace. You can choose which reports you want to show in your Power BI tab in your channel.

SharePoint Online web part

With the new Power BI report web part for SharePoint Online, you can easily embed interactive Power BI reports in SharePoint Online pages.

When using the new Embed in SharePoint Online option, the embedded reports respect all item permissions and data security through row-level security (RLS), so you can easily create secure internal portals.

Embedding a report in SharePoint Online requires no coding knowledge, does not work with on-premises SharePoint and requires the user to log into SharePoint to access reports.

For more information regarding embedding reports in a SharePoint Online web part, see this article.

Embed in PowerApps

To embed dashboards and reports in PowerApps, the embed content in apps under developer settings on the portal must be enabled. In this setting, you will also select which users can embed content in apps. You have the option to enable the entire organization or restrict the capability to specific security groups.




Broaden the reach of Power BI - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/power-bi-admin-reach/

Broaden the reach of Power BI
Module
12 Units
Feedback
Intermediate
Administrator
Microsoft Power Platform
Power BI

You can broaden the reach of Power BI by sharing your reports beyond your Power BI environment. You can publish reports to the public internet, embed reports in Microsoft Teams or in PowerApps, and place BI reports in a SharePoint online web part. There's also a special version of Power BI service called Microsoft Power BI Embedded (PBIE) which allows application developers to embed fully interactive reports into their applications without having to build their own data visualizations and controls from scratch.

Learning objectives

By the end of this module, you'll be able to:

Describe the various embedding scenarios that allow you to broaden the reach of Power BI
Understand the options for developers to customize Power BI solutions
Learn to provision and optimize Power BI embedded capacity and create and deploy dataflows
Build custom Power BI solutions template apps
Add
Prerequisites
Familiarity with Power BI and governance practices in a BI environment. 
We recommend completing the Dashboard in a Day learning path before beginning this module. 
Introduction
min
REST API custom development
min
Provision a Power BI embedded capacity
min
Dataflow introduction
min
Dataflow explained
min
Create a Dataflow
min
Dataflow capabilities on Power BI Premium
min
Template apps - install packages
min
Template apps - installed entities
min
Template app governance
min
Check your knowledge
min
Summary
min


Summary - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/power-bi-admin-monitor/7-summary
Summary
1 minute

You learned about the following topics in this module:

Usage metrics available through the Power BI admin portal

Optimizing the use of usage metrics for dashboards and reports

The difference between audit logs and the activity log




4-log.png (1174×736)
https://learn.microsoft.com/en-us/training/modules/power-bi-admin-monitor/media/4-log.png#lightbox


3-usage.png (990×526)
https://learn.microsoft.com/en-us/training/modules/power-bi-admin-monitor/media/3-usage.png#lightbox


2-metrics.png (1296×723)
https://learn.microsoft.com/en-us/training/modules/power-bi-admin-monitor/media/2-metrics.png#lightbox


1-portal.png (806×562)
https://learn.microsoft.com/en-us/training/modules/power-bi-admin-monitor/media/1-portal.png#lightbox


Check your knowledge - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/power-bi-admin-monitor/6-check
Check your knowledge
3 minutes
Answer the following questions to see what you've learned.
1. 

Which one of the following is NOT a Power BI tenant metric available through the O365 admin portal?

 

Number of user dashboards

Number of user reports

Report peak usage time during the day

Number of user datasets

2. 

What's the easiest way to convert a usage metrics report into a full featured editable Power BI report?

 

Run the report through the Power BI report converter

Use the "save as" command

Set up your report request to render an editable Power BI object

The usage metrics report is rendered in an editable format

3. 

Which one of the following statements accurately describes searching through a Power BI activity log?

 

It is currently a manual task

You can use the Microsoft 365 security center to search an audit log

You can use the Microsoft 365 compliance center to search an audit log

Export the activity log to a searchable file format, for example Excel workbook

Check your answers




Activity log - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/power-bi-admin-monitor/5-activity-log
Activity log
1 minute

There is a new Power BI API called the activity log, which enables Power BI service admins to track user and admin activities within a Power BI tenant. It is comparable to the unified Office 365 audit log in the sense that both logs contain a complete copy of the Power BI auditing data, but there are also several key differences that are important for Power BI service admins, as the following table reveals.

Table with Unified Audit Log versus Power BI Activity Log. The unified audit log includes events from SharePoint Online, Exchange Online, Dynamics 365, and other services in addition to the Power BI auditing events. Only users with View-Only Audit Logs or Audit Logs permissions have access, such as global admins and auditors. Global admins and auditors can search the unified audit log by using Office 365 Security & Compliance Center, the Microsoft 365 Security Center, and the Microsoft 365 Compliance Center. Global admins and auditors can download audit log entries by using Office 365 Management APIs and cmdlets. By contrast, the Power BI activity log only includes the Power BI auditing events. Global admins and Power BI service admins have access. There's no user interface to search the activity log yet. Global admins and Power BI service admins can download activity log entries by using a Power BI REST API and management cmdlet.

To distinguish the Power BI-specific log from the unified audit log, Microsoft uses the name activity log for Power BI, but the Power BI auditing data within both logs is identical. In this way, global admins and auditors can continue to use the Security and Compliance Centers for all their auditing needs, while Power BI service admins now have a straightforward way to access and download the data they need to monitor the actions of their users and administrators in their Power BI tenant.

For more information regarding the new Power BI activity log cmdlet, see Introducing the Power BI Activity Log | Microsoft Power BI Blog | Microsoft Power BI blog post.




Audit logs - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/power-bi-admin-monitor/4-logs
Audit logs
2 minutes

Knowing who is taking what action on which item in your Power BI tenant can be critical in helping your organization fulfill its requirements, like meeting regulatory compliance and records management. With Power BI, you have two options to track user activity: The Power BI activity log and the unified audit log. The Power BI activity log will be covered in Unit 5 of this course.

As the tenant admin, it's likely that at some point you'll be required to answer questions about a specific instance or activity in the tenant. Questions such as who is accessing my report, or when did they access the report will require investigation by the tenant admin. These investigations can be done through the O365 cloud. But these queries produce event activity records on all products in the O365 container, including:

SharePoint Online
Exchange Online
Dynamics 365
Other services
Power BI auditing events

Access to audit logs is restricted to Microsoft 365 global administrators and require an Exchange Online license. You can use a PowerShell cmdlet to download the data for investigations and analysis, but the data is only retained for 90 days. A good practice may be to log regularly scheduled queries in a repository that can be reserved for access beyond the 90-day period.

The following is an example of an audit log search.

The Power BI audit log search is primarily a manual task. The Power BI activity log is the set of audit events specific to Power BI.

For additional information about Power BI audit logs, review Track user activities in Power BI.




Usage metrics for dashboards and reports - new version - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/power-bi-admin-monitor/3-metrics-new

Learn  Training  Browse  Implement and manage an analytics environment  Monitor and audit usage 
Add
Previous
Unit 3 of 7
Next
Usage metrics for dashboards and reports - new version
Completed
100 XP
1 minute

Enhancements have been introduced in the new version of usage metrics reports. These enhancements are only available in v2 workspaces. In addition to the previously available user activity metrics, dashboard and report performance metrics have been added. For example, here is a sampling of new performance metrics available in v2 workspaces:

Report load time
Average report load time
Report views
Unique viewers
Unused reports

The following is an example of one page of the Usage report v2 - new design.

The new version of the usage metric reports system is built on top of a brand-new infrastructure. That's why the collection and storage of usage information is more reliable and scalable. And it's based on audit events, eliminating previous inconsistencies between the audit log and usage metrics reports.

Next unit: Audit logs

Continue




Usage metrics for dashboards and reports - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/power-bi-admin-monitor/2-metrics
Usage metrics for dashboards and reports
2 minutes

Usage metrics let you see the usage of individual dashboards and reports. Some of the metrics identify who the users are and how frequently the users are accessing the dashboard or report.

Following is an example of a dashboard usage metrics report.

Reported usage metrics may differ from audit log numbers because of the way in which different reporting systems collect the metrics.

 Note

Several screenshot examples shown in this module include personally identifiable information (PII.) An important privacy feature is that Admins can hide user details from usage metrics via tenant settings. For additional information regarding hiding user details, see Exclude user information from usage metrics reports.

Usage metrics reports are read-only. However, you can personalize a usage metrics report by using "Save as." This creates a brand-new dataset and converts the read-only report to a full-featured Power BI report that can be edited. Not only does the personalized report contain metrics for the selected dashboard or report, removing the default filter allows access to usage metrics for all dashboards or all reports in the selected workspace. Using this practice also allows you to capture the names of your end users. Lastly, the admin settings that control who can access reports allow admins to reflect which users are shown by name. This may be important for compliance, as in some countries/regions such as Germany, restrict by law the ability to show who is doing what as part of their job. For these cases Power BI gives the admin a way to prevent showing a group of users by name.

For additional information about usage metrics for dashboards and reports, view this article.




Introduction - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/power-bi-admin-monitor/1-introduction
Introduction
1 minute

Usage metrics are available through the Power BI admin portal. This view provides a high-level record of what's going on in a tenant. Interestingly, it's sometimes more important to realize what's not happening in a tenant. For example, if a company has many Power BI Pro licenses that aren't used effectively, corrective action can then be taken. These reports tend not to be time bound, or time sensitive. They'll present what objects are being accessed by how many users. However, the report won't reflect the times during the day or reporting period when usage was highest.

New usage metrics are being regularly added to this report.

For additional information about Power BI usage metrics, review the Usage Metrics section of this article.




Monitor and audit usage - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/power-bi-admin-monitor/

Monitor and audit usage
Module
7 Units
Feedback
Intermediate
Administrator
Microsoft Power Platform
Power BI

Usage metrics help you understand the impact of your dashboards and reports. When you run either dashboard usage metrics or report usage metrics, you discover how those dashboards and reports are being used throughout your organization, who's using them, and for what purpose. Knowing who is taking what action on which item in your Power BI tenant can be critical in helping your organization fulfill its requirements, like meeting regulatory compliance and records management. This module outlines what is available in usage metrics reports and audit logs.

Learning objectives

In this module, you will:

Discover what usage metrics are available through the Power BI admin portal
Optimize use of usage metrics for dashboards and reports
Distinguish between audit logs and the activity logs
Add
Prerequisites

We recommend completing Dashboard in a Day before beginning this module.

Introduction
min
Usage metrics for dashboards and reports
min
Usage metrics for dashboards and reports - new version
min
Audit logs
min
Activity log
min
Check your knowledge
min
Summary
min


6-support.png (752×472)
https://learn.microsoft.com/en-us/training/modules/power-bi-admin-governance/media/6-support.png#lightbox


6-help.png (657×634)
https://learn.microsoft.com/en-us/training/modules/power-bi-admin-governance/media/6-help.png#lightbox


5-embed.png (705×288)
https://learn.microsoft.com/en-us/training/modules/power-bi-admin-governance/media/5-embed.png#lightbox


4-visuals.png (750×315)
https://learn.microsoft.com/en-us/training/modules/power-bi-admin-governance/media/4-visuals.png#lightbox


3-portal.png (750×407)
https://learn.microsoft.com/en-us/training/modules/power-bi-admin-governance/media/3-portal.png#lightbox


2-elements.png (752×423)
https://learn.microsoft.com/en-us/training/modules/power-bi-admin-governance/media/2-elements.png#lightbox


1-it.png (732×344)
https://learn.microsoft.com/en-us/training/modules/power-bi-admin-governance/media/1-it.png#lightbox


Summary - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/power-bi-admin-governance/8-summary
Summary
1 minute

You learned about the following topics in this module:

Key components of an effective BI governance model

Key elements associated with data governance

How to configure, deploy, and manage elements of a BI governance strategy

How to set up BI help and support settings




Check your knowledge - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/power-bi-admin-governance/7-check
Check your knowledge
4 minutes
Answer the following questions to see what you've learned.
1. 

Which of the following is NOT a common component of an effective BI governance model?

 

Data engineering

Service and support

Training and education

Enterprise reporting

2. 

Which element of data governance defines policies for use of data in a Power BI system?

 

Visibility

Control

Support

Compliance

3. 

Which of the following administrator roles is NOT responsible for configuring tenant settings?

 

Global administrator

Power BI service administrator

Microsoft Power Platform service administrator

Power BI billing administrator

4. 

Which link is NOT available for customization in the custom help and support menu?

 

Training documentation

Embed code list

Help desk

Licensing requests

Check your answers




Help and support settings - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/power-bi-admin-governance/6-help
Help and support settings
1 minute

Power BI allows you to customize help and support links in the help menu to direct your users to specific organizational content.

It's encouraged that you set up internal Power BI-related sites using Microsoft Teams, or some other collaboration platform. These sites can be used to store training documentation, host discussions, make requests for licenses, or respond to help.

If you do so, it's recommended that you enable the Publish "Get Help" information setting for the entire organization. It's found in the Help and support settings group. You can set URLs for your:

Training documentation

Discussion forum

Licensing requests

Help desk

These URLs will become available as links in the Power BI help menu.

To set up your custom help and support menu in your Power BI tenant, access the admin portal, select tenant settings, then select Publish "Get Help" information. Toggle the Disabled button to Enabled, and then provide appropriate URLs to your company's sites for training documentation, discussion forums, licensing requests, and help desk.

For Power BI Support and status use https://support.powerbi.com




Manage embed codes - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/power-bi-admin-governance/5-embed
Manage embed codes
1 minute

With the Power BI Publish to web option, you can easily embed interactive Power BI content in blog posts, websites, emails, or social media. You can also easily edit, update, refresh, or stop sharing your published visuals.

Once you create a Publish to web embed code, you can manage your codes from the Settings menu in Power BI. Managing embed codes includes the ability to remove the destination visual or report for a code (rendering the embed code unusable) or getting the embed code.

As a Power BI admin, you can control all embed codes in the tenant through the Power BI admin portal.




Deploy organizational visuals - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/power-bi-admin-governance/4-deploy
Deploy organizational visuals
1 minute

As a Power BI admin for your organization, you can establish the approval process for usage of custom visuals and control which type of Power BI visuals, including custom visuals, users can access across the organization.

To manage Power BI visuals, you must be a Global Admin in Office 365, or have been assigned the Power BI service administrator role. Organizational visuals are managed and can be distributed through the Power BI admin portal.

From the Power BI admin portal, you can manage the list of Power BI visuals available in your organization's organizational store. The Organizational visuals tab in the Admin portal allows you to add and remove visuals, and decide which visuals will automatically display in the visualization pane of your organization's users. You can add to the list any type of visual including uncertified visuals and .pbiviz visuals, even if they contradict the tenant settings of your organization.

Organizational visuals settings are automatically deployed to Power BI Desktop. For additional information about organizational visuals, you may refer to this article.




Configure tenant settings - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/power-bi-admin-governance/3-configure
Configure tenant settings
1 minute

Tenant settings enable fine-grained control over the features that are made available to your organization and are managed in the Admin portal. Tenant settings can be configured by a global admin, Power BI service administrator, or Microsoft Power Platform service administrator. Many tenant settings can restrict capabilities and features to a limited set of users by assigning one of three states.

Disabled for the entire organization: No one in your organization can use this feature.

Enabled for the entire organization: Everyone in your organization can use this feature.

Enabled for a subset of the organization: Specific security groups in your organization can use this feature.

For more information about creating, editing, or deleting security groups in Microsoft 365 Admin Center, review this article.

An example of a commonly enabled tenant setting is - Publish to web. The Publish to web setting gives you options that let users create embed codes to publish reports to the web. This functionality makes the report and its data available to anyone on the web.

 Note

For security reasons, Publish to web is turned off by default.




Elements of data governance - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/power-bi-admin-governance/2-elements
Elements of data governance
1 minute

Data governance is the process of governing and managing data and ensures the accuracy, completeness, integrity, and timeliness of data.

The first element associated with BI data governance is visibility. As an administrator it's important to know what's going on in the Power BI environment.

Key information required by an administrator includes things like, what solutions are being deployed, whether users have access to them, and knowing what users are doing when using the solutions in their BI environment.

Another element of data governance is control. This includes the enforcement of policies dictating acceptable use of the system by defining who in the organization has authority and control over data assets and how those data assets may be used.

A third component of data governance is compliance. This means that processes are in place to control your data and ensure that all regulations are met in all your organization's data practices. Compliance starts with configuring your system to achieve compliance requirements.




Introduction - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/power-bi-admin-governance/1-introduction
Introduction
2 minutes

A critical role of a BI administrator is to enforce the governance of the BI environment.

There are three key groups that contribute to building the governance model in a normal BI ecosystem. Each group has assigned tasks and responsibilities. The three groups are the IT/BI team, the business users, and central IT.

The IT/BI team is responsible for designing models and processes for collecting, cleaning, and cataloging data. The IT/BI team is also responsible for developing rules that determine what reports are created and how they're distributed to the organization.

The business users' primary responsibilities are to research and document the BI system requirements, including needs for ad hoc analysis and reporting, and how users of the system will work together towards shared or common goals.

The third group, central IT, contributes to the development of several components of a BI governance model. Starting with training and education, which teaches users, administrators, auditors, and governors of the system how to optimize the output of the environment. Another component is data governance, or the process of governing and managing data to ensure the accuracy, completeness, integrity, and timeliness of data. A third component in our model is the enablement of business process and access controls to define the approval processes for who should be granted access to the system, what they can do in the system, and when they should have access to it. The last component is procurement and administration, which supports the purchasing and procurement of the BI system elements.




Provide governance in a Power BI environment - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/power-bi-admin-governance/

Provide governance in a Power BI environment
Module
8 Units
Feedback
Intermediate
Administrator
Microsoft Power Platform
Power BI

Power BI governance is a set of rules, regulations, and policies that define and ensure the effective, controlled, and valuable operation of a BI environment. In this module, you'll learn the fundamental components and practices necessary to govern a Power BI tenant.

Learning objectives

In this module, you will:

Define the key components of an effective BI governance model
Describe the key elements associated with data governance
Configure, deploy, and manage elements of a BI governance strategy
Set up BI help and support settings
Add
Prerequisites

Familiarity with Power BI and governance practices in a BI environment.

Introduction
min
Elements of data governance
min
Configure tenant settings
min
Deploy organizational visuals
min
Manage embed codes
min
Help and support settings
min
Check your knowledge
min
Summary
min


04-save-ss.png (342×373)
https://learn.microsoft.com/en-us/training/modules/create-paginated-reports-power-bi/media/04-save-ss.png#lightbox


04-chart-sections-ss.png (418×380)
https://learn.microsoft.com/en-us/training/modules/create-paginated-reports-power-bi/media/04-chart-sections-ss.png#lightbox


04-added-chart-ss.png (308×210)
https://learn.microsoft.com/en-us/training/modules/create-paginated-reports-power-bi/media/04-added-chart-ss.png#lightbox


04-chart-type-ss.png (571×511)
https://learn.microsoft.com/en-us/training/modules/create-paginated-reports-power-bi/media/04-chart-type-ss.png#lightbox


04-insert-chart-2-ssm.png (360×315)
https://learn.microsoft.com/en-us/training/modules/create-paginated-reports-power-bi/media/04-insert-chart-2-ssm.png#lightbox


04-insert-chart-ssm.png (416×282)
https://learn.microsoft.com/en-us/training/modules/create-paginated-reports-power-bi/media/04-insert-chart-ssm.png#lightbox


04-dataset-properties-parameter-ss.png (657×645)
https://learn.microsoft.com/en-us/training/modules/create-paginated-reports-power-bi/media/04-dataset-properties-parameter-ss.png#lightbox


04-report-parameter-properties-ssm.png (571×476)
https://learn.microsoft.com/en-us/training/modules/create-paginated-reports-power-bi/media/04-report-parameter-properties-ssm.png#lightbox


04-add-parameter-ssm.png (1789×962)
https://learn.microsoft.com/en-us/training/modules/create-paginated-reports-power-bi/media/04-add-parameter-ssm.png#lightbox


04-display-report-ss.png (563×803)
https://learn.microsoft.com/en-us/training/modules/create-paginated-reports-power-bi/media/04-display-report-ss.png#lightbox


04-run-button-ss.png (142×110)
https://learn.microsoft.com/en-us/training/modules/create-paginated-reports-power-bi/media/04-run-button-ss.png#lightbox


04-add-fields-ss.png (1012×316)
https://learn.microsoft.com/en-us/training/modules/create-paginated-reports-power-bi/media/04-add-fields-ss.png#lightbox


04-insert-tab-ss.png (876×128)
https://learn.microsoft.com/en-us/training/modules/create-paginated-reports-power-bi/media/04-insert-tab-ss.png#lightbox


04-dataset-properties-ssm.png (650×716)
https://learn.microsoft.com/en-us/training/modules/create-paginated-reports-power-bi/media/04-dataset-properties-ssm.png#lightbox


04-connection-properties-ssm.png (376×527)
https://learn.microsoft.com/en-us/training/modules/create-paginated-reports-power-bi/media/04-connection-properties-ssm.png#lightbox


Summary - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/create-paginated-reports-power-bi/8-summary
Summary
3 minutes

You have learned about how to create a Power BI paginated report. Now, you can connect to a data source, extract data into a dataset, and populate a table or chart with that data. After you've finished creating the report, you can format your report to make it visually appealing and informative, and then publish it to Power BI.




04-data-source-properties-build-button-ssm.png (658×489)
https://learn.microsoft.com/en-us/training/modules/create-paginated-reports-power-bi/media/04-data-source-properties-build-button-ssm.png#lightbox


04-power-bi-premium-ss.png (1084×1289)
https://learn.microsoft.com/en-us/training/modules/create-paginated-reports-power-bi/media/04-power-bi-premium-ss.png#lightbox


04-add-data-source-part-1-ssm.png (1789×961)
https://learn.microsoft.com/en-us/training/modules/create-paginated-reports-power-bi/media/04-add-data-source-part-1-ssm.png#lightbox


04-power-bi-report-builder-ssm.png (674×708)
https://learn.microsoft.com/en-us/training/modules/create-paginated-reports-power-bi/media/04-power-bi-report-builder-ssm.png#lightbox


Check your knowledge - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/create-paginated-reports-power-bi/7-check
Check your knowledge
3 minutes
Answer the following questions to see what you've learned.
1. 

Why are parameters important in Power BI paginated reports?

 

They allow the report developer to control the refresh interval of the report.

They allow the user to control aspects of how the report is rendered when the report is run.

They are required so that Power BI can call the paginated report.

2. 

Power BI paginated reports are created by using which tool?

 

Power BI Desktop

Power BI service

Power BI Report Builder

3. 

Power BI paginated reports is an evolved technology that was built from which original tool?

 

SQL Server Analysis Services

SQL Server Reporting Services

Microsoft SharePoint

Check your answers




Publish the report - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/create-paginated-reports-power-bi/5-publish
Publish the report
2 minutes

To publish your report, select File > Save as and then select Power BI Service. Your report will now appear in Power BI service. For more information, go to Publish datasets and reports from Power BI Desktop.

Best practices

Creating a report is meant to inform and drive action on the part of the report user. It isn't enough to create a report with sales information on it; the report author should always ask themselves several questions:

What purpose is this report for?

Who is using the report?

How can I help people do a better job?

What is the most important information and how can I highlight it?

Is this report readable?

Can people change the elements that they need to if their questions change?

Do I have visuals that are distracting from the core message of the report?

Is this report staying focused in a single topic or only a few topics?

Am I providing all information that the user expects to see in the report?

Creating good headers and footers is an excellent way to help the user interpret the report. You can provide guidance to the user by documenting why this report was created. Adding a report implementation date and time is an excellent practice. Occasionally, reports are run and then saved. People who are looking at a report won't know that they're looking at an older version unless that fact is highlighted in a footer.

Target the report for your appropriate region. English speakers read top-down, left-to-right. Putting important information, like totals, at the top of the report will highlight that information for English speakers. The way people read dates varies across different countries/regions. For example, Europeans and Americans have different conventions for reading dates. Localize data formats to the appropriate target user.

In addition to focusing on the visual aspects of the report, a good report author will consider report delivery and data source usage. Good delivery focuses on how the user wants to see the report. Therefore, report authors should ask themselves the following questions to test the appropriate delivery format and ensure that the report is rendering correctly in that format:

Does the user want the report sent to them in an email message?

Does the user want the report in a printable format?

Does the user read the report in a web browser?

Pay attention to the height and width of the report page. Verify that the report isn't running off the page when the report renders for the user.

A good report author creates reports that are easy on the data source. If you continue to recall data that you don't need from a data source, you will overburden the data source and affect performance in unpredictable ways. Focusing on only getting pertinent data will help you be a responsible teammate to others who are using the same data.




Work with charts on the report - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/create-paginated-reports-power-bi/4-charts
Work with charts on the report
1 minute

Two ways to add a chart to your report are: Select the Chart button, select Insert Chart, and then draw your table on the canvas.

Right-click the report canvas, select Insert, and then select Chart.

Next, choose the type and style of your chart.

After you have selected a chart type, the chart will be added to the design surface.

When you select the chart, a new window appears to the right. The Chart Data screen allows you to format the chart according to the values and axis properties.

Select the plus (+) sign beside each section to select the required columns.

For more information on working with charts, you can search Microsoft documentation regarding SSRS reports. All of the material in the SSRS documentation will apply to Power BI paginated reports.




Create a paginated report - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/create-paginated-reports-power-bi/3-paginated-report
Create a paginated report
3 minutes

To create a report, you must add a visual to the design surface, similar to what you would do in Power BI Desktop. Select the Insert tab from the ribbon at the top to see your options for adding a visual.

For the purposes of this example, a table visual has been added to the design surface.

When you select the Table drop-down menu, you can choose from two options: Insert Table or Table Wizard. Select Insert Table. You can now draw a table on the design surface. From the Report data window, drag fields from the dataset to the table on the design surface.

When you have finished, notice that the field is added to the lower portion of the table in the square brackets. The header will also be added. You can rename or apply formatting to the headers, such as bolding or underlining the text.

To test this simple report, select the Run button from the Home tab in the ribbon.

The report will run and display actual data in the table.

Notice that some items have changed in the report: a title was entered at the top, table headers were renamed and are in bold font, and a background color was selected on the header. These changes were implemented to help make using the report easier for the report reader.

If you want to return to the design surface, select the Design button.

Another aspect of creating a report is to add a parameter. Parameters can be used for different reasons, for example, when you want the user to enter information that displays on a visual on the report. The most popular reason to add a parameter is to affect which data is retrieved from the data source.

Consider the scenario where you are creating a report that retrieves data from a sales database. You only want sales data from between a begin date and an end date. In this case, you would create two parameters and then modify the dataset query to include those parameters in the WHERE clause of the query. Your first step in this situation is to add a parameter.

Add parameters

To add a parameter, right-click Parameters and select Add Parameter.

On the General tab, name the parameter, select the data type, and then choose the prompt that the user will see.

On the Available Values tab, enter options that the user can choose from. The Default Values tab has the initial value of the parameter when the report loads, but it can be changed by the user.

You can also get parameter values from a query. For more information, see the Microsoft documentation on parameters.

After you have created a parameter, you can use it to interact with the report. If you return to the dataset, you can connect that parameter with the query.

The parameter reference starts with the at (@) symbol. Add the parameter name to the query text. Now, when the report refreshes, the data will be pulled from the data source according to the WHERE clause and the parameter value.




Introduction to paginated reports - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/create-paginated-reports-power-bi/1-introduction
Introduction to paginated reports
2 minutes

Paginated reports allow report developers to create Power BI artifacts that have tightly controlled rendering requirements. Paginated reports are ideal for creating sales invoices, receipts, purchase orders, and tabular data. This module will teach you how to create reports, add parameters, and work with tables and charts in paginated reports.

Paginated reports defined

Paginated reports give a pixel-perfect view of the data. Pixel perfect means that you have total control of how the report renders. If you want a footer on every sales receipt that you create, a paginated report is the appropriate solution. If you want a certain customer's name to always appear in green font on a report, you can do that in a paginated report.

Power BI paginated reports are descendants of SQL Server Reporting Services (SSRS), which was first introduced in 2004. Power BI paginated reports and SSRS have a lot in common. If you're looking for information on paginated reports and can't find it, searching the internet and Microsoft documentation on SSRS is an excellent idea because you'll find numerous blog posts, videos, and documentation available to you.

When paginated reports are the right fit

You can use paginated reports for operational reports with tables of details and optional headers and footers.

Additionally, you can use paginated reports when you expect to print the report on paper or when you want an e-receipt, a purchase order, or an invoice. Paginated reports also render tabular data exceedingly well. You can have customized sort orders, clickable-headers, and URLs in results, which allows for simple integration with custom applications.

Power BI paginated reports can also display all of your data in a single report element, such as a table. If you have 25,000 records, and you want the reports to print over 100 pages, you can do that. If you want every third record to be printed with a light pink background, you can do that as well.

Power BI paginated reports are not created in Power BI Desktop; they are built by using Power BI Report Builder. You can share paginated reports with either a Power BI Pro or a Power BI premium license.

In this module, you will:

Get data.

Create a paginated report.

Work with charts and tables on the report.

Publish the report.




Get data - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/create-paginated-reports-power-bi/2-get-data
Get data
3 minutes

The first step in creating a report is to get data from a data source. Though this process might seem similar to getting data in Power BI, it is different. Power BI paginated reports do not use Power Query when connecting to data sources.

Getting data in a Power BI paginated report does not involve data cleaning steps. In fact, data is not stored in a Power BI paginated report dataset. When data is refreshed on the report, it is retrieved in an unaltered form from the data source, according to the query that was used to retrieve it.

Data can be collected from multiple data sources, including Microsoft Excel, Oracle, SQL Server, and many more. However, after the data has been collected, the different data sources cannot be merged into a single semantic model. Each source must be used for a different purpose. For instance, data from an Excel source can be used for a chart, while data from SQL Server can be used for a table on a single report. Paginated reports have an expression language that can be used to look up data in different datasets, but it is nothing like Power Query.

Power BI paginated reports can use a dataset from Power BI service. These datasets have used Power Query to clean and alter the data. The difference is that this work was done in Power BI Desktop or SQL Server Data Tools prior to using Power BI Report Builder, which doesn't have that tool in the user interface.

Create and configure a data source

To retrieve data, open Power BI Report Builder. From the Getting Started screen, select New Report. You can choose whether to create a report with a table on it, a chart, or a blank report. For the purposes of this example, a blank report has been selected. These choices create a default visual on your new report, which can be changed at any time. Next, go to the Report Data window, which is typically on the left side of the tool, though it can be moved around.

Right-click the Data Sources folder and select Add Data Source.

On the General tab, name the data source.

After naming the data source, choose the correct connection string by selecting the Build button.

After you have selected Build, the Connection Properties screen appears. The properties on this screen will be unique for each data source. The following figure is an example of what you might see in the screen. The figure shows the properties of a SQL Server connection that you, the report author, will enter:

Server name

Database name

A button for testing the connection

Select OK to continue

You can also enter username and password information on the Connection Properties screen, or you can leave it on the default setting and use your Windows credentials. Select OK again.

You've now created a data source.

Generally, authentication is beyond the scope of this course. Typically, you will receive the connection information from your IT department, application specialist, or the software vendor.

Create and configure a dataset

A data source is the connection information to a particular resource, like SQL Server. A dataset is the saved information of the query against the data source, not the data. The data always resides in its original location.

Right-click Datasets in the Report View window and select Add Dataset. Ensure that the correct data source is selected. This action will run the query against the correct data source.

From the window that displays, you can:

Name the query.

Choose whether to use a text command or a stored procedure.

Enter a query into the text box.




Create paginated reports - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/create-paginated-reports-power-bi/

Create paginated reports
Module
7 Units
Feedback
Intermediate
Data Analyst
Power BI
Microsoft Power Platform

Paginated reports allow report developers to create Power BI artifacts that have tightly controlled rendering requirements. Paginated reports are ideal for creating sales invoices, receipts, purchase orders, and tabular data. This module will teach you how to create reports, add parameters, and work with tables and charts in paginated reports.

Learning objectives

In this module, you will:

Get data.
Create a paginated report.
Work with charts and tables on the report.
Publish the report.
Add
Prerequisites

Power BI paginated reports are a feature of Power BI Premium

Introduction to paginated reports
min
Get data
min
Create a paginated report
min
Work with charts on the report
min
Publish the report
min
Check your knowledge
min
Summary
min


Exercise: Monitor data in real-time with Power BI - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/monitor-data-real-time-power-bi/5a-exercise-monitor-data-real-time-power-bi?launch-lab=true
Exercise: Monitor data in real-time with Power BI
45 minutes

 Note

A virtual machine (VM) containing the client tools you need is provided, along with the exercise instructions. Use the button above to open the VM. A limited number of concurrent sessions are available - if the hosted environment is unavailable, try again later.

When you finish the exercise, end the lab to close the VM. Don't forget to come back and complete the knowledge check to earn points for completing this module!




Exercise: Monitor data in real-time with Power BI - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/monitor-data-real-time-power-bi/5a-exercise-monitor-data-real-time-power-bi

Learn  Training  Browse  Implement advanced data visualization techniques by using Power BI  Monitor data in real-time with Power BI 
Add
Previous
Unit 6 of 8
Next
Exercise: Monitor data in real-time with Power BI
Completed
100 XP
45 minutes

This unit includes a lab to complete.

Use the free resources provided in the lab to complete the exercises in this unit. You will not be charged for the lab environment; however, you may need to bring your own subscription depending on the lab.

Microsoft provides this lab experience and related content for educational purposes. All presented information is owned by Microsoft and intended solely for learning about the covered products and services in this Microsoft Learn module.

Sign in to launch the lab

 Note

A virtual machine (VM) containing the client tools you need is provided, along with the exercise instructions. Use the button above to open the VM. A limited number of concurrent sessions are available - if the hosted environment is unavailable, try again later.

When you finish the exercise, end the lab to close the VM. Don't forget to come back and complete the knowledge check to earn points for completing this module!

Next unit: Knowledge check

Continue




Summary - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/monitor-data-real-time-power-bi/7-summary
Summary
3 minutes

Power BI offers features to support real-time analytics in Power BI reports, dashboards, and paginated reports.

You should carefully consider latency tolerances and required functionality before selecting the artifact type. Also, when working with Premium features, you should also determine whether licenses can be purchased.

In general, you should strive to deliver real-time Power BI visualizations with Power BI reports. They provide the greatest design flexibility, can be filtered, and are highly interactive. Considering creating real-time Power BI dashboards when you can’t achieve the requirements with Power BI reports, like when alerting is a requirement.




Knowledge check - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/monitor-data-real-time-power-bi/6-knowledge-check
Knowledge check
5 minutes

Choose the best response for each of the questions below. Then select Check your answers.

Check your knowledge
1. 

Gladys is a developer at Adventure Works who needs to create a real-time Power BI solution to monitor their call center. When there are excessive numbers of callers waiting on hold, Power BI should notify the call center manager. What type of Power BI artifact should Gladys develop?

 

Dashboard.

Power BI report.

Streaming dataflow.

2. 

Benny is a report author at Adventure Works who needs to set up a Power BI report page to show real-time manufacturing data with latency of 5 minutes. The report connects to a dataset that represents a DirectQuery model. Because the DirectQuery source is an operational system, it should be queried only when necessary. What should Benny do?

 

Set the AutoRefresh property to a value greater than zero.

Enable automatic page fresh with a change detection measure.

Set up incremental refresh for the dataset and enable the DirectQuery partition.

3. 

Fiafia is a developer at Adventure Works who needs to deliver a real-time Power BI dashboard. It should present real-time manufacturing data with the lowest possible latency. The dashboard only shows manufacturing events for the past hour. The solution should use the minimum of Power BI resources. What type of dataset should Fiafia develop?

 

DirectQuery dataset.

Push dataset.

Streaming dataset.

Check your answers




Set-up auto-refresh paginated reports - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/monitor-data-real-time-power-bi/5-set-up-auto-refresh-paginated-reports
Set-up auto-refresh paginated reports
2 minutes

Power BI paginated reports support a real-time capability, but only when the report in rendered in HTML.

At design time, you can set the report’s AutoRefresh property. By default, it’s set to zero, which means that the report won’t automatically refresh. When you set this property to a value greater than zero, it determines the rate in seconds at which the report automatically refreshes.




Create real-time dashboards - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/monitor-data-real-time-power-bi/4-create-real-time-dashboards
Create real-time dashboards
12 minutes

Power BI dashboards always present the latest data that the Power BI service is aware of. For this reason, dashboards are ideal to support real-time monitoring scenarios.

There are four special types of Power BI datasets that support real-time dashboards. They are:

Streaming dataset
Push dataset
Hybrid dataset
PubNub streaming dataset

There aren’t any requirements that real-time dashboards or their related datasets be stored in a dedicated capacity.

 Tip

Remember that dashboards can refresh as frequently as every 15 minutes when its tiles connect to a regular dataset that represents a DirectQuery model. For more information, see Unit 3.

Dashboards offer an interesting capability that reports don’t. You can set dashboard tiles (that display a single numeric value) to alert you when data values reach specific thresholds. At Adventure Works, their manufacturing plant is set up with many IoT devices. A Power BI dashboard presents real-time manufacturing metrics and notifies maintenance staff when devices require attention.

Use streaming datasets

A streaming dataset specifically delivers real-time data with latency of about one second. Real-time dashboard tile updates are smooth and fluid.

The streaming dataset caches data in a temporary cache for up to 60 minutes. Because there’s no data model, it’s not possible to create Power BI reports that connect to a streaming dataset. Instead, to visualize streaming data, you add streaming tiles to the dashboard. A streaming tile specifically visualizes real-time data sources from a streaming dataset. They’re easy to identify in a dashboard because they’re adorned with a lightning strike icon.

Streaming datasets are simple in their design because they represent a single table. Table columns are either text, number, or date. You can create a streaming dataset in the Power BI service or programmatically by using the Power BI REST API.

Developers can programmatically insert rows into the dataset by sending a JSON document to the dataset’s endpoint. The endpoint URL includes an authentication key.

Here’s a methodology to create a real-time dashboard by using a streaming dataset:

Create a streaming dataset.

Add a streaming tile to a dashboard.

Programmatically insert rows into the streaming dataset.

However, there are some important limitations to bear in mind:

Data retention is 60 minutes, so it’s not possible to monitor on history beyond that time.

The maximum ingestion rate is 5 requests/second.

The packet size of a request to add new rows can’t exceed 15 KB.

Power BI reports can’t connect to a streaming dataset.

Because dashboards don’t support filtering, you can’t filter streaming tiles.

Use push datasets

A push dataset specifically delivers real-time data with latency of between 3-5 seconds. While structurally it’s closely related to a Power BI import dataset, push datasets can’t be created by using Power BI Desktop. A push dataset can include multiple tables, relationships, and measures. However, it can’t include some model objects, like hierarchies and security roles.

As the dataset type name suggests, data is pushed into dataset tables. When data is pushed, Power BI immediately refreshes related dashboard tiles.

You can create a push dataset in the Power BI service (or as a hybrid dataset as described later) or programmatically by using the Power BI REST API. Developers must first acquire a Microsoft Entra access token to work with the REST API operations. They can use the API to push rows of data to a specific table. There are also API operations they can use to modify the dataset schema, delete all rows from a table, and delete the dataset.

It's also possible use Azure Stream Analytics (ASA) to create a push dataset and output rows of data to it. As a complex event-processing engine, ASA can push high volumes of fast streaming data, even from thousands of IoT devices. For more information, see Power BI output from Azure Stream Analytics.

 Tip

ASA can integrate with Azure Machine Learning (AML), allowing output of machine learning predictions. For example, an ASA job output could predict that an IoT device requires maintenance. You can set up a Power BI dashboard tile to alert you when maintenance is required. For more information, see Integrate Azure Stream Analytics with Azure Machine Learning.

Here’s a methodology to create a real-time dashboard by using a push dataset:

Create a push dataset.

Create a Power BI report that connects to the push dataset. Use any type of visual, including custom visuals to visualize the dataset data.

Pin report visuals to a dashboard.

Optionally, use the dashboard Q&A experience to add other tiles.

Programmatically push rows into the push dataset table.

However, there are some important limitations to bear in mind:

The dataset can’t contain more than 75 tables, and tables can’t contain more than 75 columns.

A push dataset table can’t store more than 5 million rows, unless the basicFIFO retention policy is enabled. When enabled, a table will store approximately 200,000 rows of data and Power BI will replace older rows with new rows.

A single request can’t push more than 10,000 rows.

The ingestion rate is limited to one million rows per hour unless the table stores more than 250,000 rows of data. In this case, the ingestion rate is limited to 120 rows per hour.

Use hybrid datasets

A hybrid dataset is both a streaming and push dataset at the same time. It delivers the benefits of both types of datasets. Use a hybrid dataset to visualize real-time data in streaming tiles and regular tiles, which you pin from Power BI reports or Q&A. Also, hybrid datasets allow your real-time solution to monitor and analyze activities that happened more than 60 minutes ago.

When you create a streaming dataset in the Power BI service, you can make it a hybrid dataset by enabling the Historic data analysis option.

Use PubNub streaming datasets

A PubNub streaming dataset is a special type of streaming dataset. It requires you to have an established real-time platform with PubNub. The Power BI web client uses the PubNub SDK to read an existing PubNub data stream. Like with streaming datasets, there’s no underlying data model, so it’s not possible to use Power BI report visuals.

Instead, dashboard streaming tiles connect to a PubNub streaming dataset. These tiles are optimized to quickly display real-time data. Because Power BI connects directly to the PubNub data stream, there’s little latency between when the data is pushed and when the tiles update.

Compare streaming and push datasets

The following table compares the capabilities of streaming and push datasets.

Expand table
Capability	Streaming dataset	Push dataset
Latency	~1 second	3-5 seconds
Data retention	60 minutes	5 million rows per table, or 200,000 rows when basicFIFO retention is set
Maximum ingestion rates	5 requests/second, 15 KB per request	1 request/second, 16 MB per request (maximum 10,000 rows)
Data throughput limits	None	1 million rows/hour, but 120 rows/hours when the table exceeds 250,000 rows
Dataset structure	Single table	Rich data model that supports filtering and aggregation
Visual types	Streaming tiles only	Report visuals, including custom visuals
Animation updates	Smooth and fluid	A bit twitchy




Demo-Introduction - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/monitor-data-real-time-power-bi/2-describe-power-bi-real-time

Learn  Training  Browse  Implement advanced data visualization techniques by using Power BI  Monitor data in real-time with Power BI 
Add
Previous
Unit 2 of 8
Next
Describe Power BI real-time analytics
Completed
100 XP
6 minutes

Power BI can show real-time in different artifact types. Artifacts include Power BI reports, dashboards, and paginated reports. Whether viewed in the Power BI service, a Power BI mobile app, or an app with embedded Power BI content, Power BI content will refresh automatically to always show current data. Depending on the real-time feature, Power BI can reliably show current data with just seconds of latency.

Here’s an example of a real-time Power BI report. It relies upon a feature known as automatic page refresh, which is described in the next unit.

 Tip

In general, you should strive to deliver real-time Power BI visualizations with Power BI reports. They provide the greatest design flexibility, can be filtered, and are highly interactive. Considering creating real-time Power BI dashboards when you can’t achieve the requirements with Power BI reports, like when alerting is a requirement.

You should factor into your requirements what degree of latency is tolerable. It often depends on the velocity (or volatility) of the data, and the urgency to keep people informed about the current state of data.

Consider at Adventure Works that there’s a daily finance report. Report consumers expect to see complete financial data up until the previous day. They might describe that requirement as real-time, especially if former reporting solutions took days or weeks to deliver yesterday’s data. From a Power BI perspective, that’s not real-time. A regular Power BI dataset, set up to refresh daily (or even every 30 minutes) could deliver that result.

Now consider a different requirement at Adventure Works. This time, it’s a manufacturing dashboard that allows users to monitor the production processes as they’re happening. Users need to monitor the hundreds of IoT devices to learn about their throughput and anomalies, like excessive waits and delays. This dashboard can be considered real-time because it must show low-latency metrics and refresh the data constantly.

To be clear, Power BI real-time solutions are concerned with producing up-to-date results with between one second and 15 minutes of latency. Power BI can manage latencies greater than 15 minutes by using conventional data refresh techniques. Power BI isn’t well suited when latency delays can’t exceed one second.

To create real-time Power BI solutions, you might require advanced data modeling skills or programming skills.

Set automatic dashboard tile refresh

 Note

The focus of this module is on using Power BI real-time features. However, it’s helpful to know that in some circumstances you can achieve real-time results without these features.

Power BI datasets that represent a DirectQuery data model (or live connection to an external-hosted data model, like Azure Analysis Services or SQL Server Analysis Services)) provide an easy way to deliver real-time dashboards. In the dataset settings, you can enable the dataset Automatic dashboard tile refresh property. You can then set a refresh frequency between weekly and every 15 minutes.

For example, when you set the refresh frequency to 15 minutes, Power BI will refresh dashboard tile caches every 15 minutes. Dashboard consumers will see tiles (that connect to the dataset) update in real-time.

While this technique is simple to set up, take care not to place too much burden on the DirectQuery data source(s), especially when datasets enforce dynamic row-level security (RLS). Dynamic RLS applies filters based on the current user.

Consider that a dashboard with 10 tiles, shared with 100 users, connects to a DirectQuery dataset that enforces dynamic RLS, and it’s set to refresh tiles every 15 minutes. It will result in Power BI sending at least 1000 tile refresh queries to the data source every 15 minutes.

Next unit: Set up automatic page refresh

Continue




Set up automatic page refresh - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/monitor-data-real-time-power-bi/3-set-up-automatic-page-refresh
Set up automatic page refresh
12 minutes

Automatic page refresh (APR) is a feature that automatically refreshes a Power BI report. It’s a setting you can enable for a specific report page providing the report connects to a dataset that:

Includes DirectQuery storage tables.
Is a live connection to a tabular model in Azure Analysis Services (AAS) or SQL Server Analysis Services (SSAS).
Is a push (or hybrid) dataset.

 Note

Push and hybrid datasets are described in the next unit.

Conceptually, the APR feature is simple. According to a refresh interval you set, Power BI automatically refreshes the report page. It simulates the same refresh operation that the report consumer can invoke from the action bar.

Work with DirectQuery storage tables

A Power BI dataset can have DirectQuery storage tables when:

It represents a DirectQuery model.
It represents a composite model.
It contains a hybrid table.
It consumes a streaming dataflow (and uses DirectQuery storage mode).

 Note

For more information about DirectQuery and composite models, work through the Choose a model framework module.

Use hybrid tables

If you set up a dataset import table by using the Power BI incremental refresh feature, you can enable the Get the latest data in real-time with DirectQuery option.

By enabling this option, Power BI automatically creates a table partition that uses DirectQuery storage mode. In this case, the table becomes a hybrid table, meaning it has import partitions to store older data, and a single DirectQuery partition for current data.

When Power BI queries a hybrid table, the query may use the cache for older data, or passthrough to the data source to retrieve current data.

This option is only available with a Premium license.

For more information, see Configure incremental refresh and real-time data.

Use streaming dataflows

A streaming dataflow allows data modelers to connect to, ingest, mash up, model, and build reports based on streaming, near real-time data directly in the Power BI service.

 Note

A streaming dataflow is conceptually different from a regular dataflow.

You create streaming dataflows in the Power BI service. They ingest streaming input that you can source from Azure Event Hubs, Azure IoT Hubs, or Azure Blob storage. Using a drag-and-drop, no-code designer, you can filter, aggregate, join, group, and union input streams. You can also set up time-window functions, which can use tumbling, hopping, sliding, session, or snapshot windows of time.

Streaming dataflows are consumed by a model that you develop in Power BI Desktop. Use the dataflows connector to connect to the streaming dataflow, and be sure to set the model storage mode to DirectQuery.

For more information, see Streaming dataflows.

There are some important restrictions to bear in mind:

Streaming dataflows are only available with a Premium license.
A Power BI administrator must enable streaming dataflows.
A capacity administrator must enable the dataflows enhanced compute engine.
You can’t store streaming dataflows and regular dataflows in the same workspace.
Set up automatic page fresh

To set up APR, in the page settings, you enable the Page refresh setting.

 Note

This setting is only available when the report connects to a supported dataset. It won’t be available, for example, when the report connects to a dataset that represents in import model.

Once enabled, you can set the Refresh type property to one of two options:

Auto page refresh – Updates all page visuals based on a fixed interval, which can be from every one second to multiple days.
Change detection – Updates all page visuals providing that source data has changed since the last automatic refresh. It avoids unnecessary refreshes, which can help to reduce resource consumption for the Power BI service and the data source. This option is only supported for reports stored in a workspace that has its license mode set to Premium, Premium per user, or Embedded (known as Premium workspaces).

 Important

When using a fixed interval, consider the burden it might place on the data source. Factor in that multiple users may open the report page, and that each visual on the page results in at least one query to the data source.

Set up change detection

To set up change detection, you must create a special type of measure called a change detection measure. Your report can only have one change detection measure. Power BI uses it to query the data source. Each time, Power BI stores the query result so it can compare it with the next result (according to the refresh interval you set). When the results differ, Power BI refreshes the page.

Change detection measures are easy to set up in Power BI Desktop. The Change detection window allows defining a change detection measure that summarizes any column by using an aggregate function (count, count distinct, minimum, maximum, and sum).

At Adventure Works, they use APR to monitor real-time manufacturing metrics. IoT devices store events that include a timestamp. The change detection measure queries for the maximum timestamp event because the page should only refresh when new events are recorded.

In Power BI Desktop, you can use Performance Analyzer to monitor when Power BI queries the change detection measure, and when visuals refresh. For more information, see Use Performance Analyzer to examine report element performance.

Work with restrictions

Once you publish an APR report to the Power BI service, Power BI may enforce restrictions related to APR.

When you publish a report to a workspace that has its license mode set to Pro, it means the workspace resides in a shared capacity. A shared capacity is shared with other Microsoft customers. To avoid noisy neighbor situations (where a co-tenant monopolizes resources), an APR refresh has a minimum interval of 30 minutes, even if the refresh interval in your report is less than that value. Change detection measures aren’t supported in shared capacities.

When you publish a report to a workspace that has its license mode set to Premium per user, or Embedded (called a dedicated capacity), APR may not be enabled or is constrained. That’s because a capacity admin can enable or disable APR, and enable or disable the use of a change detection measure for a dedicated capacity. They can also set a minimum refresh interval and a minimum execution interval for change detection measures. When your report page settings are lower than the minimum intervals, the minimum intervals will prevail.

For more information about APR support for different datasets and capacity types, see Restrictions on refresh intervals.




Introduction - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/monitor-data-real-time-power-bi/1-introduction
Introduction
3 minutes

Real-time analytics allows organizations to make confident decisions based on up-to-date data. These organizations can expect to be agile and take meaningful actions based on the most current insights. Common scenarios for real-time analytics include Internet of Things (IoT) devices, factory sensors, e-commerce, inventory management, geo-tracking, financial trading, application telemetry, call centers, and others.

What’s meant by real-time is that the data is always current, and users aren’t required to interact with visualizations. Data visualizations should refresh automatically to always show current data.

 Note

In this module, the term real-time actually means near real-time. Near real-time means that there’s always a degree of delay (known as latency), due to data processing and network transmission time.

Learning objectives

By the end of this module, you’ll be able to:

Describe Power BI real-time analytics.
Set up automatic page refresh.
Create real-time dashboards.
Set up auto-refresh paginated reports.




Monitor data in real-time with Power BI - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/monitor-data-real-time-power-bi/

Monitor data in real-time with Power BI
Module
8 Units
Feedback
Intermediate
Data Analyst
Power BI

Describe real-time analytics in Power BI using automatic page refresh, real-time dashboards, and auto-refresh in paginated reports.

Learning objectives

By the end of this module, you’ll be able to:

Describe Power BI real-time analytics.
Set up automatic page refresh.
Create real-time dashboards.
Set up auto-refresh paginated reports.
Add
Prerequisites
Experience developing Power BI data models and creating reports and dashboards, and familiarity with DirectQuery storage mode.
Introduction
min
Describe Power BI real-time analytics
min
Set up automatic page refresh
min
Create real-time dashboards
min
Set-up auto-refresh paginated reports
min
Exercise: Monitor data in real-time with Power BI
min
Knowledge check
min
Summary
min


Summary - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/understand-advanced-data-visualization-concepts/8-summary
Summary
3 minutes

Enterprises often provide reports to large groups of users. Building an organization-wide custom theme is an excellent starting point to provide a cohesive user experience across reports. For complex data analysis, custom R and Python visuals allow you to define a clearer story. Enabling personalized visuals then empowers users to interact with the data in the best way that suits their needs. Performance Analyzer helps you develop efficient reports for all. And most importantly, built-in accessibility features allow you to create inclusive reports for everyone.

You'll surely have an enterprise-worthy report for all to enjoy using and improve overall user satisfaction after implementing some or all of these features.




Knowledge check - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/understand-advanced-data-visualization-concepts/7-knowledge-check
Knowledge check
3 minutes

Choose the best response for each of the questions below. Then select Check your answers.

Check your knowledge
1. 

What's the best way to share a custom theme?

 

Save and share custom theme JSON file.

Send the modified Power BI (.PBIX) file.

Set the default theme for the organization.

2. 

What elements can be changed with personalized visuals?

 

Custom theme color.

Visualization type.

Background images.

3. 

What's an inclusive benefit of designing a report with accessibility features?

 

Users don't have auto-play videos slowing the performance.

Users will admire the bright colors and decorative images used for cohesiveness.

Users can understand the report using a screen reader or keyboard.

Check your answers




Review report performance using Performance Analyzer - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/understand-advanced-data-visualization-concepts/6-review-report-performance-use-performance-analyzer
Review report performance using Performance Analyzer
3 minutes

A good report designer is always looking for ways to improve the end-user experience, and report speed is a common complaint. In an enterprise with many reports, every millisecond can improve the experience. By using Performance Analyzer, you can identify suboptimal report elements and adjust where possible.

What is Performance Analyzer?

Performance Analyzer is a built-in feature in Power BI Desktop that measures how long report elements take to update and refresh, allowing you to see if certain elements are significantly slower.

Monitor with Performance Analyzer

In Power BI Desktop, navigate to the View ribbon, then select Performance Analyzer. The Performance Analyzer pane will appear to the right of the canvas. When you're ready to interact with the report, select the Start Recording button, perform report actions like adjusting slicers, highlighting values, changing pages, etc. You can also Refresh visuals, and then Stop when you're done.

Some easy ways to improve performance include:

Limit visuals per page. If the entire page is slow to load, consider spreading visuals across multiple pages instead.
Remove unnecessary columns and rows. For slower queries, review the data and determine if anything can be removed.
Set data types. By default, Power BI will assume data types for imported columns, which should be verified, and all new columns must be manually set.
Apply most restrictive filters. More data will take more time to load, and isn't always the best user experience either. Consider limited results first, and allow drilling for more details.

 Tip

The best time to optimize your report is before sharing it with end-users as it allows you to improve performance before soliciting user feedback. However, improvement is a continual process and can and should be done regularly once the report is live as well. Stay informed with the Optimization guide for Power BI.




Create custom visuals with R or Python - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/understand-advanced-data-visualization-concepts/5-create-custom-visuals
Create custom visuals with R or Python
2 minutes

If your organization relies on open-source languages like R and Python, you can write scripts to transform data and create visualizations. Using R and Python visuals in Power BI extends the opportunity to collaborate with Data Scientists in your organization, and enables the use of visuals your data team may already be using.

 Note

This module won't go in-depth for creating R and Python visuals, however you can review the Add an R or Python visual Microsoft Learn unit for more detail.

R visuals

In order to develop R visuals, you'll need a supported version of R installed first. Then add the R visual to the canvas, just like using a table or bar chart visual, and add your R code within the visual.

Python visuals

To develop Python visuals, you need to have Python installed on your machine. Many Python packages are supported in Power BI, but not all. See Learn which Python packages are supported in Power BI for a complete list of supported packages and Python visual limitations.




Design and configure Power BI reports for accessibility - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/understand-advanced-data-visualization-concepts/4-design-configure-power-bi-reports-for-accessibility
Design and configure Power BI reports for accessibility
5 minutes

Accessibility is at the heart of Microsoft's values, and Power BI is committed to making Power BI experiences as accessible to as many people as possible. As an enterprise report creator, it's up to you to use the available tools to create inclusive reporting across the organization.

 Note

This unit highlights some accessible design features, and more details including a checklist, can be found in the Power BI Documentation.

Consider your audience

Easily improve accessibility for most consumers with the following modifications:

Alt text for visuals, shapes, and images
Set tab order for visuals, shapes, and images
Consistent font, colors, positioning
Colorblind-friendly color schemes
Using text or icons in addition to color
Avoid jargon and acronyms
Set sort order for visuals
Disable auto-start videos and audio
Provide captions and transcripts for videos and audio
Avoid excess decorative shapes and images
Alt text

Alt text is one of the most helpful and easily configurable accessibility features. For all visuals, shapes, and images, you can add alt text - a description for use with screen readers.

Screen readers will automatically read the title and type of visual, so add insight and context for low vision users, such as "Total sales by category, further broken down by product". If you want to include specific data points, instead of adding them to the static alt text, you can use DAX measures and conditional formatting to create dynamic alt text in enrich the user experience.

To set alt text, first select the visual or image, navigate to the Format visual/shape menu, and select Alt text. To add dynamic alt text, select the Fx button.

Set tab order

In Power BI Desktop, you can set the tab order for how a keyboard user will experience your report. First, navigate to the View tab in the ribbon, then select Selection in the Show Panes section.

You'll see Layer order and Tab order, with a list of report elements. Layer order allows you to stack elements, whereas tab order dictates which item will be accessed next when the keyboard user tabs to the next item. Use the up/down arrows to set the order. Hide items by clicking the eye icon. This action will move the item to the bottom, remove the numbered position, and put a line through the eye to indicate hidden state.

 Tip

Set tab order and turn off tab order (mark the item as hidden) on any decorative items.




Enable personalized visuals in a report - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/understand-advanced-data-visualization-concepts/3-enable-personalized-visuals-report
Enable personalized visuals in a report
3 minutes

In a larger organization, it can be difficult to design a single report that meets everyone's needs, so one solution is using personalized visuals. These allow individual consumers to make minor design changes to best understand the data themselves.

 Tip

This is a powerful feature for more advanced report consumers. Empower your users by sharing the Power BI Consumer Documentation for personalized visuals.

What can consumers change

Report consumers can make changes for the following items:

Visualization type
Swap measure or dimension
Add/remove legend
Compare multiple measures
Change aggregations

Consumers can create a personal bookmark to save their changes after personalizing a report, and even share changes with others. Best of all, they can always reset to default to restore the report view.

Enable personalized visuals

You can enable personalized visuals in Power BI Desktop or Power BI service.

In Power BI Desktop, go to File > Options and settings > Options > Current file > Report settings. Make sure Personalize visuals is turned on.
In Power BI service, go to Settings on the specific report, then toggle Personalize Visuals on, and Save.

By default, when you enable personalized visuals, it's enabled for all visuals in a report. You can modify each visual to allow personalization or not. You'll need to enable this setting for each report.

 Tip

Use a Power BI Template (.PBIT) file to enable personalized visuals and add on top of your custom theme for faster report building in the future.

For more information and limitations, see the Power BI Documentation.




Create and import a custom report theme - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/understand-advanced-data-visualization-concepts/2-create-import-custom-report-theme
Create and import a custom report theme
5 minutes

When you create a custom theme, you ensure a cohesive look and feel for reports across your organization. Deploying an enterprise-wide custom theme provide consistency for:

Organizational branding
Accessibility requirements
Access themes

Navigate to the View tab on the ribbon, then select the drop-down arrow on Themes to:

Choose a built-in theme
Browse for themes
View the Theme gallery
Customize current theme
Save current theme
Create your custom theme

Currently there are almost 20 built-in report themes, including high contrast and color-blind safe options. However, if you have a certain color palette and font family in mind, you can go to Customize current theme. From here, you can make changes in the following categories:

Theme name and color settings include theme colors, sentiment colors, divergent colors, and structural colors (Advanced).
Text settings include font family, size, and color, which sets the primary text class defaults for labels, titles, cards and KPIs, and tab headers.
Visual settings include background, border, header, and tooltips.
Page element settings include wallpaper and background.
Filter pane settings include background color, transparency, font and icon color, size, filter cards.

After you make your changes, select Apply to save changes to your theme. Now you can use your theme for your current report and export for future use. Custom themes are an excellent choice for organizations, allowing a cohesive look for reports across entire department or organization.

Using the Customize current theme option is a quick and easy way to create a custom theme. If you want to make finer adjustments to themes you can also create a custom theme through JSON, which we don’t cover in this module.

Export and import themes

After you’ve applied changes to your custom theme, you need to go back to the Themes drop-down menu and select Save current theme to export the theme. A JSON file will be created that can be shared with others and used for any future reports. Now that your custom theme has been exported, you and others will need to import it to apply to other reports. Importing is as easy as exporting is – navigate to the Themes drop-down menu again, select Browse for themes, and choose the JSON file you just created (or that was shared with you). You'll get a notification when the theme successfully imports.

Enterprise considerations

Any changes made to this theme will need to be saved again to either overwrite the existing theme or as a new theme. Themes are local to the file as well, so changes made on someone’s copy on their computer won’t affect your copy. The theme could be saved as a Power BI Template (.PBIT) file, and then shared that way as well. When sharing custom themes, consider a business process to validate that the theme and/or template are being used.

 Tip

Once you’ve published your report, consider creating themes for your dashboard in the Power BI service.




Introduction - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/understand-advanced-data-visualization-concepts/1-introduction
Introduction
1 minute

Often, report creators hear complaints that reports look outdated and take forever to load. You can solve many of these problems with features already available in Power BI. As an enterprise data analyst, implementing these advanced features will make your reports more cohesive, inclusive, and efficient.

In this module, you’ll learn how to:

Create and import a custom report theme.
Create custom visuals with R or Python.
Enable personalized visuals in a report.
Review report performance using Performance Analyzer.
Design and configure Power BI reports for accessibility.




Understand advanced data visualization concepts - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/understand-advanced-data-visualization-concepts/

Understand advanced data visualization concepts
Module
8 Units
Feedback
Intermediate
Data Analyst
Power BI

Create cohesive, inclusive, and efficient Power BI reports to effectively communicate results.

Learning objectives

In this module, you’ll learn how to:

Create and import a custom report theme.
Create custom visuals with R or Python.
Enable personalized visuals in a report.
Review report performance using Performance Analyzer.
Design and configure Power BI reports for accessibility.
Add
Prerequisites
Ability to visualize data and create reports in Power BI Desktop.
Understanding of reports in the Power BI Service.
Introduction
min
Create and import a custom report theme
min
Enable personalized visuals in a report
min
Design and configure Power BI reports for accessibility
min
Create custom visuals with R or Python
min
Review report performance using Performance Analyzer
min
Knowledge check
min
Summary
min


Exercise: Use tools to optimize Power BI performance - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/use-tools-optimize-power-bi-performance/5-exercise?launch-lab=true
Exercise: Use tools to optimize Power BI performance
45 minutes

 Note

A virtual machine (VM) containing the client tools you need is provided, along with the exercise instructions. Use the button above to open the VM. A limited number of concurrent sessions are available - if the hosted environment is unavailable, try again later.

Now it's your opportunity to practice optimizing Power BI performance using the tools covered in this module. When you finish the exercise, end the lab to close the VM. Don't forget to come back and complete the knowledge check to earn points for completing this module!




Exercise: Use tools to optimize Power BI performance - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/use-tools-optimize-power-bi-performance/5-exercise

Learn  Training  Browse  Use tools to optimize Power BI performance 
Add
Previous
Unit 5 of 7
Next
Exercise: Use tools to optimize Power BI performance
Completed
100 XP
45 minutes

This unit includes a lab to complete.

Use the free resources provided in the lab to complete the exercises in this unit. You will not be charged for the lab environment; however, you may need to bring your own subscription depending on the lab.

Microsoft provides this lab experience and related content for educational purposes. All presented information is owned by Microsoft and intended solely for learning about the covered products and services in this Microsoft Learn module.

Sign in to launch the lab

 Note

A virtual machine (VM) containing the client tools you need is provided, along with the exercise instructions. Use the button above to open the VM. A limited number of concurrent sessions are available - if the hosted environment is unavailable, try again later.

Now it's your opportunity to practice optimizing Power BI performance using the tools covered in this module. When you finish the exercise, end the lab to close the VM. Don't forget to come back and complete the knowledge check to earn points for completing this module!

Next unit: Knowledge check

Continue




Summary - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/use-tools-optimize-power-bi-performance/7-summary
Summary
3 minutes

You've been introduced to three tools that you can use to troubleshoot and improve data models.

To optimize a slow report, you can run the Performance analyzer to measure how each of the report elements performs when users interact with them. From there, you can dig into DAX query performance in DAX Studio, where you can view, sort, and filter performance data. You can also troubleshoot single measures or queries and/or evaluate the overall performance of your data model. To design data models proactively, you can use the Best Practice Analyzer rules in Tabular Editor to implement data modeling best practices as you go.

Learn more
Optimize a model for performance in Power BI
Optimization guide for Power BI




Knowledge check - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/use-tools-optimize-power-bi-performance/6-knowledge-check
Knowledge check
4 minutes

Choose the best response for each of the questions below. Then select Check your answers.

Check your knowledge
1. 

Stephanie is an enterprise data analyst, working as a resource to data analysts sitting in the finance department. One of the finance analysts asked Stephanie to help troubleshoot why report visuals take 5 seconds to render after a slicer selection is made. What tool should Stephanie use first to begin troubleshooting?

 

Visual Studio.

DAX Studio.

Performance analyzer.

2. 

James has been asked to troubleshoot a report that contains a matrix visual that renders visibly slower than any other visuals on the page. After James runs the Performance analyzer, he notices the need to dig further into the DAX query for the matrix using DAX Studio to understand what's causing the trouble. That same query is running in less than a second in DAX Studio. Has James solved the problem?

 

Yes. James doesn't need to make any more changes to the DAX query.

No. James needs to use Tabular Editor to continue to investigate.

No. James needs to clear the model cache to ensure that query results aren't cached in the model.

3. 

Mary-Jo is responsible for managing the maintenance and deployment of Power BI assets for the entire organization. What tool can Mary-Jo use to ensure all data models adhere to data modeling best practices?

 

DAX Studio.

Best Practice Analyzer in Tabular Editor.

Performance analyzer.

Check your answers




Optimize a data model by using Best Practice Analyzer - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/use-tools-optimize-power-bi-performance/4-optimize-data-model-use-best-practice-analyzer

Learn  Training  Browse  Use tools to optimize Power BI performance 
Add
Previous
Unit 4 of 7
Next
Optimize a data model by using Best Practice Analyzer
Completed
100 XP
8 minutes

How can you be sure to implement data modeling best practices as you design and build data models in Power BI? There's a tool for that! The Best Practice Analyzer (BPA) in Tabular Editor can be used during the development of tabular models in Power BI or Analysis Services models.

Describe Tabular Editor

Tabular Editor is an alternative tool for authoring tabular models for Analysis Services and Power BI. Tabular Editor 2 is an open-source project that can edit a BIM file without accessing any data in the model. Tabular editor enables users to easily manipulate tabular models and can be used in a few different ways. Tabular Editor can be used to:

quickly edit data models
automate repetitive tasks using scripting
incorporate DevOps with tabular models
run BPA rules

This unit focuses on using Tabular Editor to run the BPA to ensure you're implementing data modeling best practices as you build.

Describe the Best Practice Analyzer (BPA)

BPA is a set of rules run in Tabular Editor that notify you of potential modeling missteps or changes that you can make to improve your model design and performance. It includes recommendations for naming, user experience, and common optimizations that you can apply to improve performance. For more information, see Best practice rules to improve your model's performance.

BPA rules are a set of rules that you can add to your instance of Tabular Editor. When BPA rules are run on your tabular model, you'll get a list of rules that your model violates, and can fix them using Tabular Editor.

BPA includes a set of pre-defined rules, and you can also define your own rules to encourage certain conventions and practices when developing tabular models.

To use the pre-defined BPA rules, download the BPA rules .json file from the GitHub repository. The rules are divided into categories for easier viewing, for example:

Performance
DAX Expressions
Error Prevention
Formatting
Maintenance

Each rule has a description, and many of the rules also have a reference article/video. Reading the rule description and article will provide context as to why the rule is important and why you should follow it.

 Note

BPA rules can be run against tabular models in Power BI Desktop, SQL Server Analysis Services, Azure Analysis Services, or Power BI Premium.

Run BPA in Tabular Editor

Install Tabular Editor.

Tabular Editor has an open-source and a paid version. This unit refers to use of the open-source version only.

Download the Best Practice Rules from GitHub.

Save the file in C:\Users\username\AppData\Local\TabularEditor and name it BPARules.json.

In Power BI Desktop, select Tabular Editor on the External Tools tab of the ribbon.

Tabular Editor will automatically connect to the data model

On the Tools tab, select Best Practice Analyzer.

The Best Practice Analyzer window will open, displaying any violations of the Best Practice Rules.

 Note

The best practice analyzer scans your model for issues whenever a change is made to the model. This is on by default but can be disabled.

Review the objects in violation of the rules and fix them using Tabular Editor. For example, let's fix the columns violating the Don't summarize numeric columns rule, starting with the Weight column in the Product table.

Double select (or right select) the object to go to the object in Tabular Editor. Change the object properties as necessary. In this case, we're changing Summarize by from Sum to None.

To save your changes to back to the Model.bim file, select Save, or use the keyboard shortcut ctrl + s.

 Important

Tabular Editor uses the Tabular Object Model (TOM) to load and save metadata to and from Model.bim files. When fixing Best Practice rule violations, saving changes in Tabular Editor pushes modifications to the connected Power BI desktop data model.

Customize BPA for your organization

BPA was designed to enable you to create custom rules and best practices for your organization. If the Best Practice Rules don't suit your needs, you can create rules required for data modeling best practices as you deem appropriate.

You can also edit existing rules, disable and ignore rules, and set severity levels for each rule. From the Best Practice Analyzer window, select Manage Best Practice Rules. This will enable you to turn rules to use in your scan on and off, edit rules, and delete rules. Edits you make to Best Practice Rules in this window will then be saved to the .json file.

Incorporate the use of BPA into your Continuous Integration/Continuous Deployment (CI/CD) process

If your organization has an established CI/CD process using Git, BPA can be integrated into your Azure Pipelines. If objects violate Best Practice Rules in the build, you can establish a process to either fail the build or proceed with warning based on the severity of the violation.

 Note

The severity level that you set for each rule while managing your best practice rules only comes into play when deploying models using the command line option in Tabular Editor.

Next unit: Exercise: Use tools to optimize Power BI performance

Continue




Troubleshoot DAX performance by using DAX Studio - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/use-tools-optimize-power-bi-performance/3-troubleshoot-dax-performance-use-dax-studio
Troubleshoot DAX performance by using DAX Studio
12 minutes

In unit 2, we learned that Performance analyzer data can be further analyzed in DAX Studio. Import performance data into DAX Studio, where you can view, sort, and filter all performance data. You can also bring a single query into the editor where you can make adjustments to the query to improve performance.

Understand the VertiPaq engine

Before jumping into optimization, it's important to understand what's happening under the hood of Power BI. By using compression algorithms and a multi-threaded query processor, the Analysis Services VertiPaq engine delivers fast access to tabular model objects and data by Power BI.

Power BI reads the content of your data source and transforms it in the internal VertiPaq columnar data structure, where each column is encoded and compressed. Dictionaries and indexes are created for each column. Finally, data structures are created for relationships and computation and compression of calculated columns occurs.

DAX queries are being processed by two engines, the formula engine and the storage engine. We'll talk more about these engines below.

Describe DAX Studio

DAX Studio is an open-source tool for executing DAX queries against Power BI and Analysis Services models. DAX Studio is useful for:

Reviewing the contents of your data model.
Writing and optimizing complex DAX formulas and queries.

Download and install DAX Studio with the default settings, connect to your data model, and begin working on your queries. Once DAX Studio is installed, it can also be launched from the External Tools tab of the ribbon in Power BI desktop.

Optimize the data model

Now that you have an idea of how the VertiPaq engine works, let's discuss how you can use DAX Studio to optimize DAX queries that run in this environment.

Optimize DAX queries

Calculations using DAX, either measures or columns, are a part of a DAX query, which is processed by two engines in VertiPaq. When a query is processed, the formula engine processes the request, asks the storage engine for data, and performs necessary calculations. The storage engine retrieves and aggregates data requested by the formula engine.

In the diagram below, the DAX query is sent to the tabular model in steps 1 and 2. The request is then processed by the formula engine and sent to the storage engine, represented by step 3. In step 4, the storage engine either retrieves the data from the model and stores it in memory (for import mode), or passes the query on to the data source (for DirectQuery). For import mode, refreshing the data will retrieve the data from the source.

Troubleshooting in DAX Studio enables you to see detailed statistics on the server timings of your query. You're able to view the proportion of time the query takes in each engine, and can then adjust your queries accordingly to improve performance.

Let's talk through a scenario to understand how you can optimize a query using DAX Studio.

You have a report that contains a matrix visualizing 6 measures. Your CEO informs you that the visual is slow to render, and therefore the report is unusable. You start to dig in and confirmed slow render times using the Performance analyzer in Power BI desktop.

You then copy the query to look at it in DAX Studio to get more information on what might be causing your problem. In DAX Studio, you clear the cache(1), turn on the server timings (2), and then run the query (3).

From top left to bottom right, the statistics tell you how many milliseconds it took to run the query, and the duration the storage engine (SE) CPU took. In this case, the formula engine (FE) took 73.5% of the time, while the SE took the remaining 26.5% of the time. There were 34 individual SE queries and 21 cache hits.

From here, you can investigate what in your measures might be causing the issues. This requires deep DAX knowledge and is sometimes a case of trial and error.

You experiment with the measures and recognize that improving the sales measure by replacing complicated IF statements in DAX with variables and a time intelligence function. A safe way to experiment is to comment out measures and rework them. You can comment out measures by typing two forward slashes at the beginning of a line (//). For multi-row comments, use /* at the beginning of the comment and */ to close the comment.

After experimenting, you clear the cache and run the query again with the updates you made to the measure. You find that your updated measure performs much better, with nearly a 50% reduction in query execution time.

 Important

The storage engine caches the results in memory for reuse. Because of this cache, it is critical to clear the cache prior to running queries in DAX Studio.

View model metrics using VertiPaq Analyzer

Viewing the VertiPaq Analyzer Metrics in DAX Studio is a great way to get an overall view of what's going on in your data model. VertiPaq Analyzer reports the memory consumption of the data model and can be used to quickly identify where you're spending the most memory. In short - you can use VertiPaq Analyzer to make memory gobbling offenders obvious, rectify them in Power BI, and then rerun VertiPaq Analyzer to see the immediate benefits of your data model updates.

 Note

The VertiPaq engine only stores data in memory in import models. If you're using DirectQuery, the VertiPaq engine simply sends that query to the source. This means that viewing the VertiPaq Analyzer Metrics will only be helpful for import or composite models.

You can look at the size of the table, columns, etc., in bytes. The .pbix file further compresses these sizes - the displayed sizes in bytes are evaluated prior to compression.

To view model metrics, launch DAX Studio from the external tools tab of the Power BI ribbon and select View Metrics from the Advanced tab in DAX Studio.

VertiPaq analyzer displays a number of important metrics about your model. We're going to focus specifically on memory consumption and cardinality. For a complete list of what each of these columns mean, consult the DAX Studio documentation.

Viewing metrics in DAX Studio helps you immediately find and fix problems. In this case, you can see that the problem is a column with high cardinality. You can then fix that issue back in Power BI, refresh the metrics, and immediately see the effects of your changes on the model.

For example, notice that the model in the image below contains a table that consumes 99.6% of the database memory. By drilling into the table, you can see that two columns, End date and Start date are gobbling up the most memory.

Take a look at those two columns back in Power BI desktop and notice that they're Date/time columns. Date/time columns inherently have high cardinality due to all of the possible combinations of dates and times.

Using the VertiPaq Analyzer in DAX Studio can help you easily identify and eliminate columns with high cardinality (including auto Date/time and floating-point decimal data types), and identify and remove columns that aren't used for anything.

 Note

Refer to the Power BI optimization guide for more detailed information on optimizing the data model.




Use Performance analyzer - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/use-tools-optimize-power-bi-performance/2-use-performance-analyzer
Use Performance analyzer
10 minutes

The first tool we're looking at is the Performance analyzer, which is built in to Power BI Desktop. The Performance analyzer helps you understand how report elements like visuals and DAX queries are performing.

The Performance analyzer helps you optimize at two of the four architecture levels, the data model and report visuals. The Performance analyzer is a great place to start when you're optimizing reports.

Understand performance using Performance analyzer

The Performance analyzer displays and records logs that measure how each of your report elements performs when users interact with them. You can also see which aspects of their performance are most (or least) resource intensive. You can immediately see results, make changes, run the analyzer again, and see the impact of changes you've made.

The Performance Analyzer captures operations that occur in several major subsystems involved in executing a Power BI Report:

Report Canvas provides the user interface for Power BI reports including hosting visuals and filters, managing user interactions for consuming and authoring reports, and retrieving data for display. The Report Canvas is written using web technologies and runs in web browsers or web browser components. The Report Canvas retrieves data using a high-level, internal, Power BI query language known as Semantic Query.
Data Shape Engine (DSE) evaluates Semantic Queries by generating and running one, or more DAX queries against a data model hosted inside Power BI, Power BI Desktop, Azure Analysis Services, or SQL Server Analysis Services.
Data Model Engine (AS) stores the data model and provides services to reports, such as DAX query evaluation. The model may be hosted in Power BI, Power BI Desktop, Azure Analysis Services, or SQL Server Analysis Services. Depending on the data model host, a model may be tabular or multidimensional. Tabular models may contain in-memory tables, Direct Query tables, or a mix of such tables. DAX queries against tables in Direct Query mode will trigger queries to the Direct Query data source. For example, a DAX query against a Direct Query table backed by a SQL Server database will trigger one, or more, SQL queries.
Use Performance analyzer

To use the Performance analyzer, enable the Performance analyzer on the View tab of the ribbon, and select Start recording.

After you start recording, take actions in the report. For example, move from one report tab to the next, select a slicer item, or interact with any of your visuals. Any actions you take in the report are displayed and logged in real time in the Performance Analyzer pane, in the order that the visual is loaded by Power BI.

The Performance analyzer is looking at the time it takes for each visual to query the data model and render results. This is the time from when a user does something on the page to when the visual is rendered, in milliseconds.

In the example below, three actions were taken after recording started.

Navigated to a new report page
Changed a slicer value
Cross highlighted a table

Each visual's log information includes the time spent (duration) to complete the following categories of tasks:

DAX query - if a DAX query was required, this is the time between the visual sending the query, and for Analysis Services to return the results.
Evaluated parameters (preview) - time spent evaluating the field parameters within a visual. Learn more about field parameters (preview).
Visual display - time required for the visual to appear on the screen, including time required to retrieve any web images or geocoding.
Other - time required by the visual for preparing queries, waiting for other visuals to complete, or performing other background processing.

After you've interacted with elements of the report you want to measure with Performance Analyzer, you can select the Stop button. The performance information remains in the pane after you select Stop for you to analyze.

You can select Refresh visuals in the Performance Analyzer pane to refresh all visuals on the current page of the report, which will gather information about all visuals.

If one particular visual appears to be performing slow, you can also refresh individual visuals. When Performance Analyzer is recording, you can select Analyze this visual found in the top-right corner of each visual, to refresh that visual, and capture its performance information.

 Important

As users interact with visuals in Power BI reports, DAX queries are submitted to the dataset and cached into memory. Because of this, you may need to clear the DAX query cache to get accurate results in the Performance analyzer. You can clear the cache by either closing and re-opening your report, or using DAX Studio.

Evaluate performance data further

There are many different ways you can dig deeper into information recorded by the Performance analyzer.

You can use DAX Studio to investigate your queries in more detail by copying your query from the performance analyzer. After analyzing the query in DAX Studio, you can use your own knowledge and experience to identify where the performance issues are.

In the screenshot below, you can see that the DAX query in this table visual took 14 seconds. Users waited 14 seconds before seeing the results of the action that resulted in that query running, which in this case was a cross-highlight action. It's clear that this DAX needs to be optimized.

To analyze this query further, copy and paste the query into DAX Studio to repeat the execution of the query. In DAX Studio, you can activate more diagnostic tools using the Query Plan and Server Timings tracing options.

For DAX queries with long duration times, it's likely that a measure is written poorly or an issue has occurred with the data model. The issue might be caused by the relationships, columns, or metadata in your model, or it could be the status of the Auto date/time option.

You can also save Performance analyzer results by selecting the Export button, which creates a .json file containing results. Each event in the .json file contains timestamps, correlation information, and other metadata about the operation.

You can then load that data into DAX Studio to navigate through the performance metrics in more detail.

 Tip

For more information about the contents of the .json export file, see the Performance analyzer export file documentation.




Introduction - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/use-tools-optimize-power-bi-performance/1-introduction
Introduction
3 minutes

You've built Power BI reports and they're running slow. How do you identify the source of the problem? The report could be slow because of issues at the source, the structure of the data model, the visuals on the report page, or the environment.

Optimizing performance in the context of this module means making changes to the data model so that it runs more efficiently. The tools we're discussing in this module will help you troubleshoot and improve the data model, which ultimately improves user experience.

Ideally, you should assess performance as you move through the stages of the development lifecycle; development, testing, production, and optimization of your solutions.

In this module you'll learn how to use tools to optimize a data model, and which tools are useful in which scenarios.

Learning objectives

After completing this module, you'll be able to:

Optimize queries using Performance analyzer.
Troubleshoot DAX performance using DAX Studio.
Optimize a data model using Tabular Editor.




Use tools to optimize Power BI performance - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/use-tools-optimize-power-bi-performance/

Use tools to optimize Power BI performance
Module
7 Units
Feedback
Intermediate
Data Analyst
Power BI

Use tools to develop, manage, and optimize Power BI data model and DAX query performance.

Learning objectives

After completing this module, you'll be able to:

Optimize queries using performance analyzer.
Troubleshoot DAX performance using DAX Studio.
Optimize a data model using Tabular Editor.
Add
Prerequisites
Experience designing and building Power BI data models.
Introduction
min
Use Performance analyzer
min
Troubleshoot DAX performance by using DAX Studio
min
Optimize a data model by using Best Practice Analyzer
min
Exercise: Use tools to optimize Power BI performance
min
Knowledge check
min
Summary
min


Exercise: Enforce model security - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/enforce-power-bi-model-security/4a-exercise-enforce-model-security?launch-lab=true
Exercise: Enforce model security
45 minutes

 Note

A virtual machine (VM) containing the client tools you need is provided, along with the exercise instructions. Use the button above to open the VM. A limited number of concurrent sessions are available - if the hosted environment is unavailable, try again later.

When you finish the exercise, end the lab to close the VM. Don't forget to come back and complete the knowledge check to earn points for completing this module!




Exercise: Enforce model security - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/enforce-power-bi-model-security/4a-exercise-enforce-model-security

Learn  Training  Browse  Enforce Power BI model security 
Add
Previous
Unit 5 of 7
Next
Exercise: Enforce model security
Completed
100 XP
45 minutes

This unit requires a VM to complete.

VM Mode provides a free, web-based virtual machine environment to complete the steps in this unit.

Microsoft provides this lab experience and related content for educational purposes. All presented information is owned by Microsoft and intended solely for learning about the covered products and services in this Microsoft Learn module.

Sign in to launch VM mode

 Note

A virtual machine (VM) containing the client tools you need is provided, along with the exercise instructions. Use the button above to open the VM. A limited number of concurrent sessions are available - if the hosted environment is unavailable, try again later.

When you finish the exercise, end the lab to close the VM. Don't forget to come back and complete the knowledge check to earn points for completing this module!

Next unit: Knowledge check

Continue




Summary - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/enforce-power-bi-model-security/6-summary
Summary
3 minutes

When report consumers can see all model data there’s no need to take special steps. However, when they should see subsets of model data or be restricted from certain tables or columns, it’s critical that your model enforces appropriate security.

You typically restrict access to model data with RLS rules. However, when you develop a DirectQuery model for data sources that support SSO, Power BI can leverage the data source RLS. In this case, as a data modeler, you don’t need to create model roles.

There are many good development practices you should apply to ensure data permissions are enforced accurately and efficiently.




Knowledge check - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/enforce-power-bi-model-security/5-knowledge-check
Knowledge check
5 minutes

Choose the best response for each of the questions below. Then select Check your answers.

Check your knowledge
1. 

Joshua is a data modeler at Adventure Works who is developing a model for a large data warehouse. The model must enforce RLS, and the Power BI reports that connect to the model should deliver the fastest possible performance. What should Joshua do?

 

Apply rules to dimension tables.

Apply rules to hierarchies.

Apply rules to fact tables.

2. 

Rupali is a data modeler at Adventure Works who is developing an import model to analyze employee timesheet data. The employee table stores the employee social security number (SSN) in a column. While the model will be available for all company managers, it will also be available to employees in the Payroll department. However, reports must only reveal employee SSNs to payroll employees. What feature should Rupali use to restrict access to the SSN column?

 

RLS.

SSO.

OLS.

3. 

Kasper is a data modeler at Adventure Works who is developing a model that must enforce RLS. It must restrict access to only the sales regions assigned to the report consumer. The source database includes a table that stores employee usernames and their assigned region(s). What should Kasper do?

 

Create an OLS role and use a dynamic rule.

Create an RLS role and use a dynamic rule.

Create an RLS role and use a static rule.

Check your answers




Apply good modeling practices - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/enforce-power-bi-model-security/4-apply-good-modeling-practices
Apply good modeling practices
3 minutes

It’s critical that your model enforces data permissions correctly and efficiently. The following list provides you with good development practices to apply.

Strive to define fewer datasets (data models) with well-designed roles.

Strive to create fewer roles by using dynamic rules. A data-driven solution is easier to maintain because you don’t need to add new roles.

When possible, create rules that filter dimension tables instead of fact tables. It will help to deliver faster query performance.

Validate that the model design, including its relationships and relationship properties, are correctly set up.

Use the USERPRINCIPALNAME function instead of USERNAME function. It provides consistency when validating the roles in Power BI Desktop and the Power BI service.

Rigorously validate RLS and OLS by testing all roles.

Ensure that the Power BI Desktop data source connection uses the same credentials that will be applied when set up in the Power BI service.




Restrict access to Power BI model objects - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/enforce-power-bi-model-security/3-restrict-access-to-power-bi-model-objects
Restrict access to Power BI model objects
8 minutes

As a data modeler, you can consider restricting user access to Power BI model objects. Object-level security (OLS) can restrict access to specific tables and columns, and their metadata. Typically, you apply OLS to secure objects that store sensitive data, like employee personal data.

When Power BI enforces OLS, not only does it restrict access to tables and columns, but it can also secure metadata. When you secure metadata, it’s not possible to retrieve information about secured tables and columns by using Dynamic Management Views (DMVs).

 Important

Tabular models can hide tables and columns (and other objects) by using a perspective. A perspective defines viewable subsets of model objects to help provide a specific focus for report authors. Perspectives are intended to reduce the complexity of a model, helping report authors find resources of interest. However, perspectives aren’t a security feature because they don’t secure objects. A user can still query a table or column even when it’s not visible to them.

Consider an example at Adventure Works. This organization has a data warehouse dimension table named DimEmployee. The table includes columns that store employee name, phone, email address, and salary. While general report consumers can see employee name and contact details, they must not be able to see salary values. Only senior Human Resources staff are permitted to see salary values. So, the data modeler used OLS to grant access to the salary column only to specific Human Resources staff.

OLS is a feature inherited from Azure Analysis Services (AAS) and SQL Server Analysis Services (SSAS). The feature is available in Power BI Premium to provide backward compatibility for models migrated to Power BI. For this reason, it’s not possible to completely set up OLS in Power BI Desktop.

Set up OLS

To set up OLS, you start by creating roles. You can create roles in Power BI Desktop in the same way you do when setting up RLS. Next, you need to add OLS rules to the roles. This capability isn’t supported by Power BI Desktop, so you’ll need to take a different approach.

You add OLS rules to a Power BI Desktop model by using an XML for Analysis (XMLA) endpoint. XMLA endpoints are available with Power BI Premium, and they provide access to the Analysis Services engine in the Power BI service. The read/write endpoint supports dataset management, application lifecycle management, advanced data modeling, and more. You can use XMLA endpoint-enabled APIs for scripting, such as Tabular Model Scripting Language (TMSL) or the PowerShell SqlServer module. Or you can use a client tool, like SSMS. There are third-party tool options too, like Tabular Editor, which is an open-source tool for creating, maintaining, and managing models.

By default, all model tables and columns aren’t restricted. You can set them to None or Read. When set to None, users associated with the role can’t access the object. When set to Read, users associated with the role can access the object. When you’re restricting specific columns, ensure the table isn’t set to None.

Once you’ve added the OLS rules, you can publish the model to the Power BI service. Use the same process for RLS to map accounts and security groups to the roles.

Considerations

In a Power BI report, when a user doesn’t have permission to access a table or column, they'll receive an error message. The message will inform them that the object doesn’t exist.

Consider carefully whether OLS is the right solution for your project. When a user opens a Power BI report that queries a restricted object (for them), the error message could be confusing and will result in a negative experience. To them, it looks like the report is broken. A better approach might be to create a separate set of models or reports for the different report consumer requirements.

Restrictions

There are restrictions to be aware of when implementing OLS.

You can’t mix RLS and OLS in the same role. If you need to apply RLS and OLS in the same model, create separate roles dedicated to each type. Also, you can’t set table-level security if it breaks a relationship chain. For example, if there are relationships between tables A and B, and B and C, you can't secure table B. If table B is secured, a query on table A can't transit the relationships between table A and B, and B and C. In this case, you could set up a separate relationship between tables A and C.

However, model relationships that reference a secured column will work, providing that the column’s table isn’t secured.

Lastly, while it isn’t possible to secure measures, a measure that references secured objects is automatically restricted.

For more information, see Object-level security.




Restrict access to Power BI model data - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/enforce-power-bi-model-security/2-restrict-access-to-power-bi-model-data

Learn  Training  Browse  Enforce Power BI model security 
Add
Previous
Unit 2 of 7
Next
Restrict access to Power BI model data
Completed
100 XP
14 minutes

As a data modeler, you set up RLS by creating one or more roles. A role has a unique name in the model, and it usually includes one or more rules. Rules enforce filters on model tables by using Data Analysis Expressions (DAX) filter expressions.

 Note

By default, a data model has no roles. A data model without roles means that users (who have permission to query the data model) have access to all model data.

 Tip

It's possible to define a role that includes no rules. In this case, the role provides access to all rows of all model tables. This role set up would be suitable for an admin user who is allowed to view all data.

You can create, validate, and manage roles in Power BI Desktop. For Azure Analysis Services or SQL Server Analysis Services models, you can create, validate, and manage roles by using SQL Server Data Tools (SSDT).

You can also create and manage roles by using SQL Server Management Studio (SSMS), or by using a third-party tool, like Tabular Editor.

To gain a better understanding of how RLS restricts access to data, watch the following animated image.

Apply star schema design principles

We recommend you apply star schema design principles to produce a model comprising dimension and fact tables. It’s common to set up Power BI to enforce rules that filter dimension tables, allowing model relationships to efficiently propagate those filters to fact tables.

The following image is the model diagram of the Adventure Works sales analysis data model. It shows a star schema design comprising a single fact table named Sales. The other tables are dimension tables that support the analysis of sales measures by date, sales territory, customer, reseller, order, product, and salesperson. Notice the model relationships connecting all tables. These relationships propagate filters (directly or indirectly) to the Sales table.

This model design supports examples presented in this unit.

Define rules

Rule expressions are evaluated within row context. Row context means the expression is evaluated for each row using the column values of that row. When the expression returns TRUE, the user can “see” the row.

 Tip

To learn more about row context, work through the Add calculated tables and columns to Power BI Desktop models module. While this module describes adding model calculations, it includes a unit that introduces and describes row context.

You can define rules that are either static or dynamic.

Static rules

Static rules use DAX expressions that refer to constants.

Consider the following rule applied to the Region table that restricts data access to Midwest sales.

DAX
Copy

'Region'[Region] = "Midwest"



The following steps explain how Power BI enforces the rule. It:

Filters the Region table, resulting in one visible row (for Midwest).

Uses the model relationship to propagate the Region table filter to the State table, resulting in 14 visible rows (the Midwest region comprise 14 states).

Uses the model relationship to propagate the State table filter to the Sales table, resulting in thousands of visible rows (the sales facts for the states that belong to the Midwest region).

The simplest static rule that you can create restricts access to all table rows. Consider the following rule applied to the Sales Details table (not depicted in the model diagram).

DAX
Copy

FALSE()



This rule ensures users can't access any table data. It could be useful when salespeople are allowed to see aggregated sales results (from the Sales table) but not order-level details.

Defining static rules is simple and effective. Consider using them when you need to create only a few of them, as might be the case at Adventure Works where there are only six US regions. However, be aware of disadvantages: setting up static rules can involve significant effort to create and set up. It would also require you to update and republish the dataset when new regions are onboarded.

If there are many rules to set up and you anticipate adding new rules in the future, consider creating dynamic rules instead.

Dynamic rules

Dynamic rules use specific DAX functions that return environmental values (as opposed to constants). Environmental values are returned from three specific DAX functions:

USERNAME or USERPRINCIPALNAME – Returns the Power BI authenticated user as a text value.

CUSTOMDATA - Returns the CustomData property passed in the connection string. Non-Power BI reporting tools that connect to the dataset by using a connection string can set this property, like Microsoft Excel.

 Note

Be aware that the USERNAME function returns the user in the format of DOMAIN\username when used in Power BI Desktop. However, when used in the Power BI service, it returns the format of the user's User Principal Name (UPN), like username@adventureworks.com. Alternatively, you can use the USERPRINCIPALNAME function, which always returns the user in the user principal name format.

Consider a revised model design that now includes the (hidden) AppUser table. Each row of the AppUser table describes a username and region. A model relationship to the Region table propagates filters from the AppUser table.

The following rule applied to the AppUser table restricts data access to the region(s) of the authenticated user.

DAX
Copy

'AppUser'[UserName] = USERPRINCIPALNAME()



Defining dynamic rules is simple and effective when a model table stores username values. They allow you to enforce a data-driven RLS design. For example, when salespeople are added to, or removed from, the AppUser table (or are assigned to different regions), this design approach just works.

Validate roles

When you create roles, it’s important to test them to ensure they apply the correct filters. For data models created in Power BI Desktop, there’s the View as function that allows you to see the report when different roles are enforced, and different username values are passed.

Set up role mappings

Role mappings must be set up in advance of users accessing Power BI content. Role mapping involves assigning Microsoft Entra security objects to roles. Security objects can be user accounts or security groups.

 Tip

When possible, it’s a good practice to map roles to security groups. That way, there will be fewer mappings, and you can delegate the group membership management to the network administrators.

For Power BI Desktop developed models, role mapping is typically done in the Power BI service. For Azure Analysis Services or SQL Server Analysis Services models, role mapping is typically done in SSMS.

For more information about setting up RLS, see:

Row-level security (RLS) with Power BI

Row-level security (RLS) guidance in Power BI Desktop

Use single sign-on (SSO) for DirectQuery sources

When your data model has DirectQuery tables and their data source supports SSO, the data source can enforce data permissions. This way, the database enforces RLS, and Power BI datasets and reports honor the data source security.

Consider that Adventure Works has an Azure SQL Database for their sales operations that resides in the same tenant as Power BI. The database enforces RLS to control access to rows in various database tables. You can create a DirectQuery model that connects to this database without roles and publish it to the Power BI service. When you set the data source credentials in the Power BI service, you enable SSO. When report consumers open Power BI reports, Power BI passes their identity to the data source. The data source then enforces RLS based on the identity of the report consumer.

For information about Azure SQL Database RLS, see Row-level security.

 Note

Calculated tables and calculated columns that reference a DirectQuery table from a data source with SSO authentication aren’t supported in the Power BI service.

For more information about data sources that support SSO, see Single sign-on (SSO) for DirectQuery sources.

Next unit: Restrict access to Power BI model objects

Continue




Introduction - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/enforce-power-bi-model-security/1-introduction
Introduction
3 minutes

You can enforce model security by restricting access to subset of data, and by restricting access to specific model tables and columns. You might restrict access because some report consumers aren’t permitted to view specific data, like sales results of other sales regions. Achieving this requirement commonly involves setting up row-level security (RLS), which involves defining roles and rules in that filter data model. You can also set up object-level security (OLS), to restrict access to entire tables or columns.

Learning objectives

By the end of this module, you’ll be able to:

Restrict access to Power BI model data with RLS.
Restrict access to Power BI model objects with OLS.
Apply good development practices to enforce Power BI model security.




Enforce Power BI model security - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/enforce-power-bi-model-security/

Enforce Power BI model security
Module
7 Units
Feedback
Intermediate
Data Analyst
Power BI

Enforce model security in Power BI using row-level security and object-level security.

Learning objectives

By the end of this module, you’ll be able to:

Restrict access to Power BI model data with RLS.
Restrict access to Power BI model objects with OLS.
Apply good development practices to enforce Power BI model security.
Add
Prerequisites
Experience developing Power BI data models by using Power BI Desktop.
Introduction
min
Restrict access to Power BI model data
min
Restrict access to Power BI model objects
min
Apply good modeling practices
min
Exercise: Enforce model security
min
Knowledge check
min
Summary
min


Exercise: Create calculation groups - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/create-calculation-groups/5-lab?launch-lab=true
Exercise: Create calculation groups
45 minutes

 Note

A virtual machine (VM) containing the client tools you need is provided, along with the exercise instructions. Use the button above to open the VM. A limited number of concurrent sessions are available - if the hosted environment is unavailable, try again later.

When you finish the exercise, end the lab to close the VM. Don't forget to come back and complete the knowledge check to earn points for completing this module!




Exercise: Create calculation groups - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/create-calculation-groups/5-lab

Learn  Training  Browse  Design and build tabular models  Create calculation groups 
Add
Previous
Unit 5 of 7
Next
Exercise: Create calculation groups
Completed
100 XP
45 minutes

This unit includes a lab to complete.

Use the free resources provided in the lab to complete the exercises in this unit. You will not be charged for the lab environment; however, you may need to bring your own subscription depending on the lab.

Microsoft provides this lab experience and related content for educational purposes. All presented information is owned by Microsoft and intended solely for learning about the covered products and services in this Microsoft Learn module.

Sign in to launch the lab

 Note

A virtual machine (VM) containing the client tools you need is provided, along with the exercise instructions. Use the button above to open the VM. A limited number of concurrent sessions are available - if the hosted environment is unavailable, try again later.

When you finish the exercise, end the lab to close the VM. Don't forget to come back and complete the knowledge check to earn points for completing this module!

Next unit: Knowledge check

Continue




Summary - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/create-calculation-groups/7-summary
Summary
3 minutes

Calculation groups can help you analyze existing measures in multiple ways. You can easily analyze a measure for several time slices, such as month to date, quarter to date, year to date, and previous year.

Calculation groups can significantly reduce the number of redundant measures by grouping common measure expressions as calculation items.




Knowledge check - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/create-calculation-groups/6-knowledge-check
Knowledge check
3 minutes

Choose the best response for each of the questions below. Then select Check your answers.

Check your knowledge
1. 

What calculation item property should a report creator define to allow conditional application of format strings to measures?

 

Dynamic format strings.

Precedence.

Ordering.

2. 

What limitation of calculation groups should Power BI report authors be aware of?

 

There are no limitations of calculation groups.

Applying calculation groups to a model disables the use of implicit measures in a Power BI report.

Applying calculation groups disables the ability to refresh data models in the Power BI service.

3. 

Authors working with data models containing multiple calculation groups must be sure to specify what to control the order of application of calculation groups?

 

Ordering.

Precedence.

Calculation group name.

Check your answers




Create calculation groups in a model - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/create-calculation-groups/4-model
Create calculation groups in a model
3 minutes

Using calculation groups requires a baseline knowledge of DAX and filter context. This unit describes the application of calculation groups in a Power BI data model using Tabular Editor.

Install Tabular Editor

Application of calculation groups can't be accomplished in Power BI desktop. Download the open-source Tabular Editor 2.x tool to apply calculation groups.

With Tabular Editor installed, you can access it from the external tools tab of the ribbon in Power BI.

Launching Tabular Editor from the External Tools tab of the ribbon will open the Tabular Editor interface, with a connection to your Power BI data model.

In the screenshot below, Tabular Editor was launched from the Adventure Works DW 2020 Power BI report.

 Note

Learn more about external tools in Power BI Desktop.

Create calculation group and items

Use the Tabular Editor interface to create calculation groups and calculation items within those groups.

Apply a calculation group

Saving the calculation group in Tabular Editor will save changes to the connected data model. To apply those changes, you must refresh the calculation group query in Power BI desktop.

 Note

Calculation groups do not work with implicit measures. Measures to be evaluated by the calculation group must be explicitly defined.

Use your calculation group in a visual

You can now add your calculation group to a visual. A simple way to visualize your calculation group is to drop the calculation group into the columns field of a matrix visual. This will apply the calculation group to the measure you've placed in the values field.




Explore calculation groups features and usage - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/create-calculation-groups/3-explore-usage-features
Explore calculation groups features and usage
3 minutes

Before creating calculation groups, let's explore their properties. Calculation groups are made up of calculation items, which are simply DAX statements containing a substitute or placeholder for existing explicit measures in your model. For example, a calculation group, Time Intelligence, may contain the calculation items Year to Date, Quarter to Date, and Month to Date.

Calculation group properties

Anyone using calculation groups needs to be aware of the precedence property of a calculation group.

Precedence

Precedence is a property defined for a calculation group. When a data model contains more than one calculation group it's essential to define the precedence, or the order of evaluation.

Defining the precedence ensures that the different calculation groups are executed in the proper order. A higher number indicates greater precedence, meaning it will be applied before calculation groups with lower precedence.

All calculation items within a single calculation group share the same precedence.

Use Tabular Editor to set the precedence property for the calculation group.

Calculation item properties

Calculation items also contain properties that are important to report developers, including ordering and dynamic format strings.

Ordering

The ordinal value is the sort order of the calculation item. The order in which calculation items appear in a report can be changed by specifying the Ordinal property. Specifying calculation item order with the Ordinal property doesn't change precedence, the order in which calculation items are evaluated.

If the ordinal value isn't specified, the default behavior is that calculation items are ordered alphabetically by name.

Dynamic format strings

Calculation groups can also be used to define conditional format strings to a measure.

A simple example of using dynamic format strings may be having a different format for totals compared to other values.

Dynamic format strings are particularly useful for currency conversion. For example, report consumers may want to see sales by country/region, with the correct currency formatting applied for each. This is accomplished by adding a format string column to the currency dimension table and then creating a currency conversion calculation group and item.




Understand calculation groups - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/create-calculation-groups/2-understand
Understand calculation groups
3 minutes

Calculation groups are a simple way to reduce the number of measures in a model by grouping common measure expressions. Calculation groups work with existing explicit DAX measures by automating repetitive patterns.

Calculation groups can be created for tabular models in SQL Server 2019 and later, Analysis Services, Azure Analysis Services, and Power BI.

The matrix below contains three measures: Orders, Sales, and Profit. Using a calculation group enables you to create two calculation items, Previous Year and Year over Year, which are then applied to each of our three existing measures.

How to create calculation groups

The recommended tool for creating calculation groups in Power BI is the free, open-source Tabular Editor 2.x tool. Tabular Editor 2.x lets you manipulate and manage measures, calculated columns, display folders, perspectives, and translations in Analysis Services Tabular and Power BI XMLA Models (from Compatibility Level 1200 and onwards). Power BI Desktop doesn't have the user interface to create calculation groups.

Calculation groups are also supported in Visual Studio with Analysis Services Projects VSIX update 2.9.2 and later.

 Note

To understand more see Calculation groups in analysis services models.

Benefits of using a calculation group

The main benefit of using calculation groups is a reduction in the overall number of measures you need to create and maintain.

Calculation groups also enable the creation of creative report features, such as switching measures using a slicer, dynamic formatting, and even turning display labels on and off.

Limitations of using a calculation group

The main limitation of using calculation groups is that implicit measures are no longer supported in your report. If you're used to using implicit measures to quickly cross-check calculations, note that implementing calculation groups will require the creation of explicit measures.

 Tip

Implicit measures are automatically generated calculations, achieved by configuring a Power BI visual to summarize column values. Explicit measures are calculations added to a tabular model using a DAX formula.

Refer to calculation groups documentation to read more about limitations.




Introduction - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/create-calculation-groups/1-introduction
Introduction
3 minutes

Designing Power BI reports to meet the needs of users can be a challenge, requiring many different calculations and the ability to evaluate each of those calculations in multiple time periods.

For example, the sales manager at Contoso wants to understand sales trends for the year. They've asked you to create multiple measures (orders, sales, and profit) for several time slices, such as month to date, quarter to date, year to date, and previous year. After jumping in and starting to create measures for each metric for each time slice, you realize what a large amount of work this is going to be! Calculation groups are the best way to achieve this kind of a task without increasing the model’s complexity.

Learning objectives

In this module, you will:

Explore how calculation groups work.
Maintain calculation groups in a model.
Use calculation groups in a Power BI report.




Create calculation groups - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/create-calculation-groups/

Create calculation groups
Module
7 Units
Feedback
Intermediate
Data Analyst
Data Engineer
Power BI

In this module you’ll learn what calculation groups are, explore key features and usage scenarios, and learn to create calculation groups.

Learning objectives

After completing this module, you will be able to:

Explore how calculation groups work.
Maintain calculation groups in a model.
Use calculation groups in a Power BI report.
Add
Prerequisites
Experience creating reports using Power BI desktop.
Basic understanding of Tabular Editor 2.
Proficient with Data Analysis Expressions (DAX) in tabular models, at a basic level.
Introduction
min
Understand calculation groups
min
Explore calculation groups features and usage
min
Create calculation groups in a model
min
Exercise: Create calculation groups
min
Knowledge check
min
Summary
min


Exercise - Create Advanced DAX Calculations in Power BI Desktop - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/dax-power-bi-time-intelligence/3b-lab?launch-lab=true
Exercise - Create Advanced DAX Calculations in Power BI Desktop
45 minutes
Access your environment

Before you start this lab (unless you are continuing from a previous lab), select Launch lab above.

You are automatically logged in to your lab environment as data-ai\student.

You can now begin your work on this lab.

 Tip

To dock the lab environment so that it fills the window, select the PC icon at the top and then select Fit Window to Machine.

Exercise story

In this exercise, you’ll create measures with DAX expressions involving filter context manipulation.

In this exercise you learn how to:

Use the CALCULATE() function to manipulate filter context
Use Time Intelligence functions

This exercise should take approximately 45 minutes.

 Note

A limited number of concurrent sessions are available - if the hosted environment is unavailable, try again later.

Use your own device

Alternatively, configure your environment with the setup instructions.

Then follow these exercise instructions to complete the exercise.




dax-matrix-mountain-200-bike-stock-2020-june-ssm.png (314×242)
https://learn.microsoft.com/en-us/training/modules/dax-power-bi-time-intelligence/media/dax-matrix-mountain-200-bike-stock-2020-june-ssm.png#lightbox


dax-matrix-mountain-200-bike-stock-2-ss.png (1096×240)
https://learn.microsoft.com/en-us/training/modules/dax-power-bi-time-intelligence/media/dax-matrix-mountain-200-bike-stock-2-ss.png#lightbox


dax-matrix-mountain-200-bike-stock-1-ss.png (1098×244)
https://learn.microsoft.com/en-us/training/modules/dax-power-bi-time-intelligence/media/dax-matrix-mountain-200-bike-stock-1-ss.png#lightbox


dax-model-diagram-inventory-ss.png (532×462)
https://learn.microsoft.com/en-us/training/modules/dax-power-bi-time-intelligence/media/dax-model-diagram-inventory-ss.png#lightbox


dax-matrix-new-customers-ssm.png (568×313)
https://learn.microsoft.com/en-us/training/modules/dax-power-bi-time-intelligence/media/dax-matrix-new-customers-ssm.png#lightbox


dax-matrix-customers-ltd-ssm.png (557×312)
https://learn.microsoft.com/en-us/training/modules/dax-power-bi-time-intelligence/media/dax-matrix-customers-ltd-ssm.png#lightbox


dax-matrix-revenue-ytd-ss.png (373×387)
https://learn.microsoft.com/en-us/training/modules/dax-power-bi-time-intelligence/media/dax-matrix-revenue-ytd-ss.png#lightbox


dax-matrix-revenue-yoy-ssm.png (460×566)
https://learn.microsoft.com/en-us/training/modules/dax-power-bi-time-intelligence/media/dax-matrix-revenue-yoy-ssm.png#lightbox


dax-matrix-revenue-py-ssm.png (480×567)
https://learn.microsoft.com/en-us/training/modules/dax-power-bi-time-intelligence/media/dax-matrix-revenue-py-ssm.png#lightbox


Summary - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/dax-power-bi-time-intelligence/5-summary
Summary
5 minutes

In this module, you learned that time intelligence calculations are concerned with modifying the filter context for date filters. You were introduced to many DAX time intelligence functions, which support the creation of calculations, such as year-to-date (YTD) or year-over-year (YoY). You also learned that life-to-date (LTD) calculations can help you count new occurrences over your fact data, and that snapshot data can be filtered in a way to help guarantee that only a single snapshot value is returned.




dax-matrix-revenue-ytd-activity-ssm.png (367×313)
https://learn.microsoft.com/en-us/training/modules/dax-power-bi-time-intelligence/media/dax-matrix-revenue-ytd-activity-ssm.png#lightbox


Exercise - Create Advanced DAX Calculations in Power BI Desktop - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/dax-power-bi-time-intelligence/3b-lab

Learn  Training  Browse  Use DAX time intelligence functions in Power BI Desktop models 
Add
Previous
Unit 4 of 6
Next
Exercise - Create Advanced DAX Calculations in Power BI Desktop
Completed
100 XP
45 minutes

This unit includes a lab to complete.

Use the free resources provided in the lab to complete the exercises in this unit. You will not be charged for the lab environment; however, you may need to bring your own subscription depending on the lab.

Microsoft provides this lab experience and related content for educational purposes. All presented information is owned by Microsoft and intended solely for learning about the covered products and services in this Microsoft Learn module.

Sign in to launch the lab

Access your environment

Before you start this lab (unless you are continuing from a previous lab), select Launch lab above.

You are automatically logged in to your lab environment as data-ai\student.

You can now begin your work on this lab.

 Tip

To dock the lab environment so that it fills the window, select the PC icon at the top and then select Fit Window to Machine.

Exercise story

In this exercise, you’ll create measures with DAX expressions involving filter context manipulation.

In this exercise you learn how to:

Use the CALCULATE() function to manipulate filter context
Use Time Intelligence functions

This exercise should take approximately 45 minutes.

 Note

A limited number of concurrent sessions are available - if the hosted environment is unavailable, try again later.

Use your own device

Alternatively, configure your environment with the setup instructions.

Then follow these exercise instructions to complete the exercise.

Next unit: Check your knowledge

Continue




Check your knowledge - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/dax-power-bi-time-intelligence/4-check
Check your knowledge
3 minutes
Answer the following questions to see what you've learned.
1. 

In the context of semantic model calculations, which statement best describes time intelligence?

 

Snapshot balance reporting

Filter context modifications involving a date table

Complex calculations involving time

Calculations involving hours, minutes, or seconds

2. 

You're developing a semantic model in Power BI Desktop. You've just added a date table by using the CALENDARAUTO function. You've extended it with calculated columns, and you've related it to other model tables. What else should you do to ensure that DAX time intelligence calculations work correctly?

 

Add time intelligence measures to the date table.

Mark as a Date table.

Add a fiscal hierarchy.

Add a date column.

3. 

You have a table that stores account balance snapshots for each date, excluding weekends. You need to ensure that your measure formula only filters by a single date. Also, if no record is on the last date of a time period, it should use the latest account balance. Which DAX time intelligence function should you use?

 

FIRST

FIRSTNONBLANK

LAST

LASTNONBLANK

Check your answers




Additional time intelligence calculations - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/dax-power-bi-time-intelligence/3-calculations
Additional time intelligence calculations
5 minutes

Other DAX time intelligence functions exist that are concerned with returning a single date. You'll learn about these functions by applying them in two different scenarios.

The FIRSTDATE and the LASTDATE DAX functions return the first and last date in the current filter context for the specified column of dates.

Calculate new occurrences

Another use of time intelligence functions is to count new occurrences. The following example shows how you can calculate the number of new customers for a time period. A new customer is counted in the time period in which they made their first purchase.

Your first task is to add the following measure to the Sales table that counts the number of distinct customers life-to-date (LTD). Life-to-date means from the beginning of time until the last date in filter context. Format the measure as a whole number by using the thousands separator.

Customers LTD =
VAR CustomersLTD =
    CALCULATE(
        DISTINCTCOUNT(Sales[CustomerKey]),
        DATESBETWEEN(
            'Date'[Date],
            BLANK(),
            MAX('Date'[Date])
        ),
        'Sales Order'[Channel] = "Internet"
    )
RETURN
    CustomersLTD


Add the Customers LTD measure to the matrix visual. Notice that it produces a result of distinct customers LTD until the end of each month.

The DATESBETWEEN function returns a table that contains a column of dates that begins with a given start date and continues until a given end date. When the start date is BLANK, it will use the first date in the date column. (Conversely, when the end date is BLANK, it will use the last date in the date column.) In this case, the end date is determined by the MAX function, which returns the last date in filter context. Therefore, if the month of August 2017 is in filter context, then the MAX function will return August 31, 2017 and the DATESBETWEEN function will return all dates through to August 31, 2017.

Next, you will modify the measure by renaming it to New Customers and by adding a second variable to store the count of distinct customers before the time period in filter context. The RETURN clause now subtracts this value from LTD customers to produce a result, which is the number of new customers in the time period.

New Customers =
VAR CustomersLTD =
    CALCULATE(
        DISTINCTCOUNT(Sales[CustomerKey]),
        DATESBETWEEN(
            'Date'[Date],
            BLANK(),
            MAX('Date'[Date])
        ),
    'Sales Order'[Channel] = "Internet"
    )
VAR CustomersPrior =
    CALCULATE(
        DISTINCTCOUNT(Sales[CustomerKey]),
        DATESBETWEEN(
            'Date'[Date],
            BLANK(),
            MIN('Date'[Date]) - 1
        ),
        'Sales Order'[Channel] = "Internet"
    )
RETURN
    CustomersLTD - CustomersPrior


For the CustomersPrior variable, notice that the DATESBETWEEN function includes dates until the first date in filter context minus one. Because Microsoft Power BI internally stores dates as numbers, you can add or subtract numbers to shift a date.

Snapshot calculations

Occasionally, fact data is stored as snapshots in time. Common examples include inventory stock levels or account balances. A snapshot of values is loaded into the table on a periodic basis.

When summarizing snapshot values (like inventory stock levels), you can summarize values across any dimension except date. Adding stock level counts across product categories produces a meaningful summary, but adding stock level counts across dates does not. Adding yesterday's stock level to today's stock level isn't a useful operation to perform (unless you want to average that result).

When you are summarizing snapshot tables, measure formulas can rely on DAX time intelligence functions to enforce a single date filter.

In the following example, you will explore a scenario for the Adventure Works company. Switch to model view and select the Inventory model diagram.

Notice that the diagram shows three tables: Product, Date, and Inventory. The Inventory table stores snapshots of unit balances for each date and product. Importantly, the table contains no missing dates and no duplicate entries for any product on the same date. Also, the last snapshot record is stored for the date of June 15, 2020.

Now, switch to report view and select Page 2 of the report. Add the UnitsBalance column of the Inventory table to the matrix visual. Its default summarization is set to sum values.

This visual configuration is an example of how not to summarize a snapshot value. Adding daily snapshot balances together doesn't produce a meaningful result. Therefore, remove the UnitsBalance field from the matrix visual.

Now, you'll add a measure to the Inventory table that sums the UnitsBalance value for a single date. The date will be the last date of each time period. It's achieved by using the LASTDATE function. Format the measure as a whole number with the thousands separator.

Stock on Hand =
CALCULATE(
    SUM(Inventory[UnitsBalance]),
    LASTDATE('Date'[Date])
)


 Note

Notice that the measure formula uses the SUM function. An aggregate function must be used (measures don't allow direct references to columns), but given that only one row exists for each product for each date, the SUM function will only operate over a single row.

Add the Stock on Hand measure to the matrix visual. The value for each product is now based on the last recorded units balance for each month.

The measure returns BLANKs for June 2020 because no record exists for the last date in June. According to the data, it hasn't happened yet.

Filtering by the last date in filter context has inherent problems: A recorded date might not exist because it hasn't yet happened, or perhaps because stock balances aren't recorded on weekends.

Your next step is to adjust the measure formula to determine the last date that has a non-BLANK result and then filter by that date. You can achieve this task by using the LASTNONBLANK DAX function.

Use the following measure definition to modify the Stock on Hand measure.

Stock on Hand =
CALCULATE(
    SUM(Inventory[UnitsBalance]),
    LASTNONBLANK(
        'Date'[Date],
        CALCULATE(SUM(Inventory[UnitsBalance]))
    )
)


In the matrix visual, notice the values for June 2020 and the total (representing the entire year).

The LASTNONBLANK function is an iterator function. It returns the last date that produces a non-BLANK result. It achieves this result by iterating through all dates in filter context in descending chronological order. (Conversely, the FIRSTNONBLANK iterates in ascending chronological order.) For each date, it evaluates the passed in expression. When it encounters a non-BLANK result, the function returns the date. That date is then used to filter the CALCULATE function.

 Note

The LASTNONBLANK function evaluates its expression in row context. The CALCULATE function must be used to transition the row context to filter context to correctly evaluate the expression.

You should now hide the Inventory table UnitsBalance column. It will prevent report authors from inappropriately summarizing snapshot unit balances.




Introduction - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/dax-power-bi-time-intelligence/1-introduction
Introduction
1 minute

Time intelligence relates to calculations over time. Specifically, it relates to calculations over dates, months, quarters, or years, and possibly time. Rarely would you need to calculate over time in the sense of hours, minutes, or seconds.

In Data Analysis Expressions (DAX) calculations, time intelligence means modifying the filter context for date filters.

For example, at the Adventure Works company, their financial year begins on July 1 and ends on June 30 of the following year. They produce a table visual that displays monthly revenue and year-to-date (YTD) revenue.

The filter context for 2017 August contains each of the 31 dates of August, which are stored in the Date table. However, the calculated year-to-date revenue for 2017 August applies a different filter context. It's the first date of the year through to the last date in filter context. In this example, that's July 1, 2017 through to August 31, 2017.

Time intelligence calculations modify date filter contexts. They can help you answer these time-related questions:

What's the accumulation of revenue for the year, quarter, or month?
What revenue was produced for the same period last year?
What growth in revenue has been achieved over the same period last year?
How many new customers made their first order in each month?
What's the inventory stock on-hand value for the company's products?

This module describes how to create time intelligence measures to answer these questions.




Use DAX time intelligence functions - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/dax-power-bi-time-intelligence/2-functions
Use DAX time intelligence functions
5 minutes

DAX includes several time intelligence functions to simplify the task of modifying date filter context. You could write many of these intelligence formulas by using a CALCULATE function that modifies date filters, but that would create more work.

 Note

Many DAX time intelligence functions are concerned with standard date periods, specifically years, quarters, and months. If you have irregular time periods (for example, financial months that begin mid-way through the calendar month), or you need to work with weeks or time periods (hours, minutes, and so on), the DAX time intelligence functions won't be helpful. Instead, you'll need to use the CALCULATE function and pass in hand-crafted date or time filters.

Date table requirement

To work with time intelligence DAX functions, you need to meet the prerequisite model requirement of having at least one date table in your model. A date table is a table that meets the following requirements:

It must have a column of data type Date (or date/time), known as the date column.
The date column must contain unique values.
The date column must not contain BLANKs.
The date column must not have any missing dates.
The date column must span full years. A year isn't necessarily a calendar year (January-December).
The date table must be indicated as a date table.

For more information, see Create date tables in Power BI Desktop.

Summarizations over time

One group of DAX time intelligence functions is concerned with summarizations over time:

DATESYTD - Returns a single-column table that contains dates for the year-to-date (YTD) in the current filter context. This group also includes the DATESMTD and DATESQTD DAX functions for month-to-date (MTD) and quarter-to-date (QTD). You can pass these functions as filters into the CALCULATE DAX function.
TOTALYTD - Evaluates an expression for YTD in the current filter context. The equivalent QTD and MTD DAX functions of TOTALQTD and TOTALMTD are also included.
DATESBETWEEN - Returns a table that contains a column of dates that begins with a given start date and continues until a given end date.
DATESINPERIOD - Returns a table that contains a column of dates that begins with a given start date and continues for the specified number of intervals.

 Note

While the TOTALYTD function is simple to use, you are limited to passing in one filter expression. If you need to apply multiple filter expressions, use the CALCULATE function and then pass the DATESYTD function in as one of the filter expressions.

In the following example, you will create your first time intelligence calculation that will use the TOTALYTD function. The syntax is as follows:

TOTALYTD(<expression>, <dates>, [, <filter>][, <year_end_date>])


The function requires an expression and, as is common to all time intelligence functions, a reference to the date column of a marked date table. Optionally, a single filter expression or the year-end date can be passed in (required only when the year doesn't finish on December 31).

Download and open the Adventure Works DW 2020 M07.pbix file. Then, add the following measure definition to the Sales table that calculates YTD revenue. Format the measure as currency with two decimal places.

Revenue YTD =
TOTALYTD([Revenue], 'Date'[Date], "6-30")


The year-end date value of "6-30" represents June 30.

On Page 1 of the report, add the Revenue YTD measure to the matrix visual. Notice that it produces a summarization of the revenue amounts from the beginning of the year through to the filtered month.

Comparisons over time

Another group of DAX time intelligence functions is concerned with shifting time periods:

DATEADD - Returns a table that contains a column of dates, shifted either forward or backward in time by the specified number of intervals from the dates in the current filter context.
PARALLELPERIOD - Returns a table that contains a column of dates that represents a period that is parallel to the dates in the specified dates column, in the current filter context, with the dates shifted a number of intervals either forward in time or back in time.
SAMEPERIODLASTYEAR - Returns a table that contains a column of dates that are shifted one year back in time from the dates in the specified dates column, in the current filter context.
Many helper DAX functions for navigating backward or forward for specific time periods, all of which returns a table of dates. These helper functions include NEXTDAY, NEXTMONTH, NEXTQUARTER, NEXTYEAR, and PREVIOUSDAY, PREVIOUSMONTH, PREVIOUSQUARTER, and PREVIOUSYEAR.

Now, you will add a measure to the Sales table that calculates revenue for the prior year by using the SAMEPERIODLASTYEAR function. Format the measure as currency with two decimal places.

Revenue PY =
VAR RevenuePriorYear = CALCULATE([Revenue], SAMEPERIODLASTYEAR('Date'[Date]))
RETURN
    RevenuePriorYear


Add the Revenue PY measure to the matrix visual. Notice that it produces results that are similar to the previous year's revenue amounts.

Next, you will modify the measure by renaming it to Revenue YoY % and then updating the RETURN clause to calculate the change ratio. Be sure to change the format to a percentage with two decimals places.

Revenue YoY % =
VAR RevenuePriorYear = CALCULATE([Revenue], SAMEPERIODLASTYEAR('Date'[Date]))
RETURN
    DIVIDE(
        [Revenue] - RevenuePriorYear,
        RevenuePriorYear
    )


Notice that the Revenue YoY % measure produces a ratio of change factor over the previous year's monthly revenue. For example, July 2018 represents a 106.53 percent increase over the previous year's monthly revenue, and November 2018 represents a 24.22 percent decrease over the previous year's monthly revenue.

 Note

The Revenue YoY % measure demonstrates a good use of DAX variables. The measure improves the readability of the formula and allows you to unit test part of the measure logic (by returning the RevenuePriorYear variable value). Additionally, the measure is an optimal formula because it doesn't need to retrieve the prior year's revenue value twice. Having stored it once in a variable, the RETURN clause uses to the variable value twice.




Use DAX time intelligence functions in Power BI Desktop models - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/dax-power-bi-time-intelligence/

Use DAX time intelligence functions in Power BI Desktop models
Module
6 Units
Feedback
Intermediate
Data Analyst
App Maker
Power BI
Microsoft Power Platform

By the end of this module, you'll learn the meaning of time intelligence and how to add time intelligence DAX calculations to your model.

Learning objectives

By the end of this module, you'll be able to:

Define time intelligence.
Use common DAX time intelligence functions.
Create useful intelligence calculations.
Add
Prerequisites

You should have experience creating Microsoft Power BI Desktop models and designing Power BI report layouts. You should also understand how to create Data Analysis Expressions (DAX) measures and how to work with iterator functions and filter context.

Introduction
min
Use DAX time intelligence functions
min
Additional time intelligence calculations
min
Exercise - Create Advanced DAX Calculations in Power BI Desktop
min
Check your knowledge
min
Summary
min


Exercise: Work with model relationships - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/create-power-bi-model-relationships/5a-exercise-work-model-relationships?launch-lab=true
Exercise: Work with model relationships
45 minutes

 Note

A virtual machine (VM) containing the client tools you need is provided, along with the exercise instructions. Use the button above to open the VM. A limited number of concurrent sessions are available - if the hosted environment is unavailable, try again later.

When you finish the exercise, end the lab to close the VM. Don't forget to come back and complete the knowledge check to earn points for completing this module!




Summary - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/create-power-bi-model-relationships/7-summary
Summary
3 minutes

By far, the most common model relationship is the one-to-many relationship. It fits with well with star schema design principals. Dimension tables are the “one” side, and your fact tables are the “many” side.

Once you gain mastery of model relationships and how to set them up, you’re on your way to develop more complex and efficient models. You can develop models with many-to-many relationships, and work with role-playing dimensions by activating inactive relationships.

While you add relationships at model design time, model calculations can navigate relationships, modify relationship behavior, and even create virtual relationships.




Exercise: Work with model relationships - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/create-power-bi-model-relationships/5a-exercise-work-model-relationships

Learn  Training  Browse  Create Power BI model relationships 
Add
Previous
Unit 6 of 8
Next
Exercise: Work with model relationships
Completed
100 XP
45 minutes

This unit includes a lab to complete.

Use the free resources provided in the lab to complete the exercises in this unit. You will not be charged for the lab environment; however, you may need to bring your own subscription depending on the lab.

Microsoft provides this lab experience and related content for educational purposes. All presented information is owned by Microsoft and intended solely for learning about the covered products and services in this Microsoft Learn module.

Sign in to launch the lab

 Note

A virtual machine (VM) containing the client tools you need is provided, along with the exercise instructions. Use the button above to open the VM. A limited number of concurrent sessions are available - if the hosted environment is unavailable, try again later.

When you finish the exercise, end the lab to close the VM. Don't forget to come back and complete the knowledge check to earn points for completing this module!

Next unit: Knowledge check

Continue




Knowledge check - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/create-power-bi-model-relationships/6-knowledge-check
Knowledge check
5 minutes

Choose the best response for each of the questions below. Then select Check your answers.

Check your knowledge
1. 

Belle is a data modeler at Adventure Works who is developing a model for a data warehouse. The model includes a table that stores products and another table that stores sales of those products. Belle adds a relationship to relate the two tables.

Which cardinality type should Belle set to achieve an optimal model?

 

One-to-one.

One-to-many.

Many-to-many.

2. 

Akira is a data modeler at Adventure Works who is developing a model for a data warehouse. The model includes a table that stores products and another table that stores sales monthly targets of product categories. Each product can belong to multiple categories and each category can have multiple products. Akira adds a relationship to relate the two tables.

Which cardinality type should Akira set to achieve an optimal model?

 

One-to-one.

One-to-many.

Many-to-many.

3. 

Margaret is a data modeler at Adventure Works who is adding a measure to sales model. When evaluated, the measure must filter by filters already applied to an unrelated table.

Which DAX function should Margaret use to create a virtual relationship?

 

RELATEDTABLE.

TREATAS.

USERELATIONSHIP

Check your answers




Understand relationship evaluation - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/create-power-bi-model-relationships/5-understand-relationship-evaluation

Learn  Training  Browse  Create Power BI model relationships 
Add
Previous
Unit 5 of 8
Next
Understand relationship evaluation
Completed
100 XP
12 minutes

Model relationships, from an evaluation perspective, are classified as either regular or limited. It's not a configurable relationship property. It’s inferred from the cardinality type and the data source of the two related tables. It's important to understand the evaluation type because there may be performance implications or consequences should data integrity be compromised. These implications and integrity consequences are described in this unit.

First, some modeling theory is required to fully understand relationship evaluations.

An import or DirectQuery model sources all of its data from either the VertiPaq cache or the source database. In both instances, Power BI is able to determine that a "one" side of a relationship exists.

A composite model, however, can comprise tables using different storage modes (import, DirectQuery, or dual), or multiple DirectQuery sources. Each source, including the VertiPaq cache of imported data, is considered to be a source group. Model relationships can then be classified as intra source group or inter/cross source group. An intra source group relationship is one that relates two tables within a source group, while an inter/cross source group relationship relates tables across two source groups. Relationships in import or DirectQuery models are always intra source group.

Here's an example of a composite model.

In this example, the composite model consists of two source groups: a VertiPaq source group and a DirectQuery source group. The VertiPaq source group contains three tables, and the DirectQuery source group contains two tables. One cross source group relationship exists to relate a table in the VertiPaq source group to a table in the DirectQuery source group.

Regular relationships

A model relationship is regular when the query engine can determine the "one" side of relationship. It has confirmation that the "one" side column contains unique values. All one-to-many intra source group relationships are regular relationships.

In the following example, there are two regular relationships, both marked as R. Relationships include the one-to-many relationship contained within the VertiPaq source group, and the one-to-many relationship contained within the DirectQuery source group.

For import models, where all data is stored in the VertiPaq cache, Power BI creates a data structure for each regular relationship at data refresh time. The data structures consist of indexed mappings of all column-to-column values, and their purpose is to accelerate joining tables at query time.

At query time, regular relationships permit table expansion to happen. Table expansion results in the creation of a virtual table by including the native columns of the base table and then expanding into related tables. For import tables, table expansion is done in the query engine; for DirectQuery tables it’s done in the native query that’s sent to the source database (as long as the Assume referential integrity property isn't enabled). The query engine then acts upon the expanded table, applying filters and grouping by the values in the expanded table columns.

 Note

Inactive relationships are expanded also, even when the relationship isn't used by a calculation. Bi-directional relationships have no impact on table expansion.

For one-to-many relationships, table expansion takes place from the "many" to the "one" sides by using LEFT OUTER JOIN semantics. When a matching value from the "many" to the "one" side doesn't exist, a blank virtual row is added to the "one" side table. This behavior applies only to regular relationships, not to limited relationships.

Table expansion also occurs for one-to-one intra source group relationships, but by using FULL OUTER JOIN semantics. This join type ensures that blank virtual rows are added on either side, when necessary.

Blank virtual rows are effectively unknown members. Unknown members represent referential integrity violations where the "many" side value has no corresponding "one" side value. Ideally these blanks shouldn’t exist. They can be eliminated by cleansing or repairing the source data.

Here’s how table expansion works with an animated example.

In this example, the model consists of three tables: Category, Product, and Sales. The Category table relates to the Product table with a one-to-many relationship, and the Product table relates to the Sales table with a one-to-many relationship. The Category table contains two rows, the Product table contains three rows, and the Sales tables contains five rows.

There are matching values on both sides of all relationships meaning that there aren’t any referential integrity violations. A query-time expanded table is revealed. The table consists of the columns from all three tables. It's effectively a denormalized perspective of the data contained in the three tables. A new row is added to the Sales table, and it has a production identifier value (9) that has no matching value in the Product table. It's a referential integrity violation. In the expanded table, the new row has (Blank) values for the Category and Product table columns.

Limited relationships

A model relationship is limited when there's no guaranteed "one" side. A limited relationship can happen for two reasons:

The relationship uses a many-to-many cardinality type (even if one or both columns contain unique values).

The relationship is cross source group (which can only ever be the case for composite models).

In the following example, there are two limited relationships, both marked as L. The two relationships include the many-to-many relationship contained within the VertiPaq source group, and the one-to-many cross source group relationship.

For import models, data structures are never created for limited relationships. In that case, Power BI resolves table joins at query time.

Table expansion never occurs for limited relationships. Table joins are achieved by using INNER JOIN semantics, and for this reason, blank virtual rows aren’t added to compensate for referential integrity violations.

There are additional restrictions related to limited relationships:

The RELATED DAX function can't be used to retrieve the "one" side column values.

Enforcing RLS has topology restrictions.

 Note

In Power BI Desktop model view, it's not always possible to determine whether a model relationship is regular or limited. A many-to-many relationship will always be limited, as will be a one-to-many relationship when it's a cross source group relationship. To determine whether it's a cross source group relationship, you'll need to inspect the table storage modes and data sources to arrive at the correct determination.

Precedence rules

Bi-directional relationships can introduce multiple, and therefore ambiguous, filter propagation paths between model tables. The following list presents precedence rules that Power BI uses for ambiguity detection and path resolution:

Many-to-one and one-to-one relationships, including limited relationships

Many-to-many relationships

Bi-directional relationships, in the reverse direction (that is, from the "many" side)

Performance preference

The following list orders filter propagation performance, from fastest to slowest performance:

One-to-many intra source group relationships

Many-to-many model relationships achieved with an intermediary table and that involve at least one bi-directional relationship

Many-to-many cardinality relationships

Cross source group relationships

Next unit: Exercise: Work with model relationships

Continue




Use DAX relationship functions - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/create-power-bi-model-relationships/4-use-dax-relationship-functions
Use DAX relationship functions
3 minutes

There are several DAX functions that are relevant to model relationships.

RELATED

The RELATED function retrieves the value from "one" side of a relationship. It’s useful when involving calculations from different tables that are evaluated in row context.

 Tip

To learn more about row context, work through the Add calculated tables and columns to Power BI Desktop models module. While this module describes adding model calculations, it includes a unit that introduces and describes row context.

RELATEDTABLE

The RELATEDTABLE function retrieves a table of rows from the "many" side of a relationship.

USERELATIONSHIP

The USERELATIONSHIP function forces the use of a specific inactive model relationship. It’s useful when your model includes a role-playing dimension table, and you choose to create inactive relationships from this table.

For more information, see Active vs inactive relationship guidance.

CROSSFILTER

The CROSSFILTER function either modifies the relationship cross filter direction (to one or both), or it disables filter propagation (none). It’s useful when you need to change or ignore model relationships during the evaluation of a specific calculation.

COMBINEVALUES

The COMBINEVALUES function joins two or more text strings into one text string. The purpose of this function is to support multi-column relationships in DirectQuery models when tables belong to the same source group.

TREATAS

The TREATAS function applies the result of a table expression as filters to columns from an unrelated table. It’s helpful in advanced scenarios when you want to create a virtual relationship during the evaluation of a specific calculation.

Parent and child functions

The Parent and child functions are a family of related functions that you can use to generate calculated columns to naturalize a parent-child hierarchy. You can then use these columns to create a fixed-level hierarchy.

For more information, see Understanding functions for parent-child hierarchies in DAX.




Set up relationships - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/create-power-bi-model-relationships/3-set-up-relationships
Set up relationships
14 minutes

A model relationship relates one column in a table to one column in a different table. (There's one specialized case where this requirement isn't true, and it applies only to multi-column relationships in DirectQuery models. This topic is described in the next unit.)

 Note

It's not possible to relate a column to a different column in the same table. This concept is sometimes confused with the ability to define a relational database foreign key constraint that’s table self-referencing. You can use this relational database concept to store parent-child relationships (for example, each employee record is related to a "reports to" employee). However, you can’t use model relationships to generate a model hierarchy based on this type of relationship. To create a parent-child hierarchy, see Parent and Child functions.

Set relationship cardinality

Each model relationship is defined by a cardinality type. There are four cardinality type options, representing the data characteristics of the "from" and "to" related columns. The "one" side means the column contains unique values; the "many" side means the column can contain duplicate values.

 Note

If a data refresh operation attempts to load duplicate values into a "one" side column, the entire data refresh will fail.

The four options, together with their shorthand notations, are described in the following bulleted list:

One-to-many (1:*)

Many-to-one (*:1)

One-to-one (1:1)

Many-to-many (:)

When you create a relationship in Power BI Desktop, the designer automatically detects and sets the cardinality type. Power BI Desktop queries the model to know which columns contain unique values. For import models, it uses internal storage statistics; for DirectQuery models, it sends profiling queries to the data source.

Sometimes, however, Power BI Desktop can get it wrong. It can get it wrong when tables are yet to be loaded with data, or because columns that you expect to contain duplicate values currently contain unique values. In either case, you can update the cardinality type as long as any "one" side columns contain unique values (or the table is yet to be loaded with rows of data).

One-to-many (and many-to-one) cardinality

The one-to-many and many-to-one cardinality options are the same, and they're also the most common cardinality types.

When configuring a one-to-many or many-to-one relationship, you'll choose the one that matches the order in which you related the columns. Consider how you would configure the relationship from the Product table to the Sales table by using the ProductID column found in each table. The cardinality type would be one-to-many, as the ProductID column in the Product table contains unique values. If you related the tables in the reverse direction, Sales to Product, then the cardinality would be many-to-one.

One-to-one cardinality

A one-to-one relationship means both columns contain unique values. This cardinality type isn't common, and it likely represents a suboptimal model design because of the storage of redundant data. It’s often a better idea to use Power Query to consolidate the two tables into one.

For more information on using this cardinality type, see One-to-one relationship guidance.

Many-to-many cardinality

A many-to-many relationship means both columns can contain duplicate values. This cardinality type is infrequently used. It's typically useful when designing complex model requirements. You can use it to relate many-to-many facts or to relate higher grain facts. For example, when sales target facts are stored at product category level and the product dimension table is stored at product level.

For guidance on using this cardinality type, see Many-to-many relationship guidance.

 Tip

In Power BI Desktop model view, you can interpret a relationship's cardinality type by looking at the indicators (1 or *) on either side of the relationship line. To determine which columns are related, you'll need to select, or hover the cursor over, the relationship line to highlight the related columns.

Set cross filter direction

Each model relationship is defined with a cross filter direction. Your setting determines the direction(s) that filters will propagate. The possible cross filter options are dependent on the cardinality type.

Expand table
Cardinality type	Cross filter options
One-to-many (or many-to-one)	Single
Both
One-to-one	Both
Many-to-many	Single (Table1 to Table2)
Single (Table2 to Table1)
Both

Single cross filter direction means "single direction", and Both means "both directions". A relationship that filters in both directions is commonly described as bi-directional.

For one-to-many relationships, the cross filter direction is always from the "one" side, and optionally from the "many" side (bi-directional). For one-to-one relationships, the cross filter direction is always from both tables. Lastly, for many-to-many relationships, cross filter direction can be from either one of the tables, or from both tables. Notice that when the cardinality type includes a "one" side, that filters will always propagate from that side.

When the cross filter direction is set to Both, an additional property becomes available. It can apply bi-directional filtering when Power BI enforces row-level security (RLS) rules. For more information about RLS, see the Row-level security (RLS) with Power BI Desktop article.

You can modify the relationship cross filter direction, including the disabling of filter propagation, by using a model calculation. It's achieved by using the CROSSFILTER DAX function, which is described in Unit 3.

Bear in mind that bi-directional relationships can impact negatively on performance. Further, attempting to configure a bi-directional relationship could result in ambiguous filter propagation paths. In this case, Power BI Desktop may fail to commit the relationship change and will alert you with an error message. Sometimes, however, Power BI Desktop may allow you to define ambiguous relationship paths between tables. Precedence rules that affect ambiguity detection and path resolution are described in Unit 4.

We recommend using bi-directional filtering only as needed. For more information, see Bi-directional relationship guidance.

 Tip

In Power BI Desktop model view, you can interpret a relationship's cross filter direction by noticing the arrowhead(s) along the relationship line. A single arrowhead represents a single-direction filter in the direction of the arrowhead; a double arrowhead represents a bi-directional relationship.

Set active vs inactive relationships

There can only be one active filter propagation path between two model tables. However, it's possible to introduce additional relationship paths, though you must set these relationships as inactive. Inactive relationships can only be made active during the evaluation of a model calculation. It’s achieved by using the USERELATIONSHIP DAX function, which is described in Unit 3.

Generally, we recommend defining active relationships whenever possible. They widen the scope and potential of how report authors can use your model. Using only active relationships means that role-playing dimension tables should be duplicated in your model.

In specific circumstances, however, you can define one or more inactive relationships for a role-playing dimension table. You can consider this design when:

There's no requirement for report visuals to simultaneously filter by different roles.

You use the USERELATIONSHIP DAX function to activate a specific relationship for relevant model calculations.

For more information, see Active vs inactive relationship guidance.

 Tip

In Power BI Desktop model view, you can interpret a relationship's active vs inactive status. An active relationship is represented by a solid line; an inactive relationship is represented as a dashed line.

Set assume referential integrity

The Assume referential integrity property is available only for one-to-many and one-to-one relationships between two DirectQuery storage mode tables that belong to the same source group. You can only enable this property when the “many” side column doesn’t contain NULLs.

When enabled, native queries sent to the data source will join the two tables together by using an INNER JOIN rather than an OUTER JOIN. Generally, enabling this property improves query performance, though it does depend on the specifics of the data source.

Always enable this property when a single-column database foreign key constraint exists between the two tables. Even when a foreign key constraint doesn't exist, consider enabling the property as long as you're certain data integrity exists.

 Important

If data integrity should become compromised, the inner join will eliminate unmatched rows between the tables. For example, consider a model Sales table with a ProductID column value that didn’t exist in the related Product table. Filter propagation from the Product table to the Sales table will eliminate sales rows for unknown products. This would result in an understatement of the sales results.

For more information, see Assume referential integrity settings in Power BI Desktop.




Understand model relationships - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/create-power-bi-model-relationships/2-understand-model-relationships

Learn  Training  Browse  Create Power BI model relationships 
Add
Previous
Unit 2 of 8
Next
Understand model relationships
Completed
100 XP
6 minutes

A model relationship propagates filters applied on the column of one model table to a different model table. Filters will propagate so long as there's a relationship path to follow, which can involve propagation to multiple tables.

Relationship paths are deterministic, meaning that filters are always propagated in the same way and without random variation. Relationships can, however, be disabled, or have filter context modified by model calculations that use particular DAX functions. Use DAX relationship functions are described in Unit 2.

 Important

Model relationships don’t enforce data integrity. Unit 4, which describes relationship evaluation, explains how model relationships behave when there are data integrity issues with your data.

Here's how relationships propagate filters with an animated example.

In this example, the model consists of four tables: Category, Product, Year, and Sales. The Category table relates to the Product table, and the Product table relates to the Sales table. The Year table also relates to the Sales table. All relationships are one-to-many (the details of which are described in the next unit).

A query, possibly generated by a Power BI card visual, requests the total sales quantity for sales orders made for a single category, Cat-A, and for a single year, CY2018. It's why you can see filters applied on the Category and Year tables. The filter on the Category table propagates to the Product table to isolate two products that are assigned to the category Cat-A. Then the Product table filters propagate to the Sales table to isolate just two sales rows for these products. These two sales rows represent the sales of products assigned to category Cat-A. Their combined quantity is 14 units. At the same time, the Year table filter propagates to further filter the Sales table, resulting in just the one sales row that is for products assigned to category Cat-A and that was ordered in year CY2018. The quantity value returned by the query is 11 units. Note that when multiple filters are applied to a table (like the Sales table in this example), it's always an AND operation, requiring that all conditions must be true.

Apply star schema design principles

We recommend you apply star schema design principles to produce a model comprising dimension and fact tables. It’s common to set up Power BI to enforce rules that filter dimension tables, allowing model relationships to efficiently propagate those filters to fact tables.

The following image is the model diagram of the Adventure Works sales analysis data model. It shows a star schema design comprising a single fact table named Sales. The other four tables are dimension tables that support the analysis of sales measures by date, state, region, and product. Notice the model relationships connecting all tables. These relationships propagate filters (directly or indirectly) to the Sales table.

Use disconnected tables

It's unusual that a model table isn't related to another model table. Such a table in a valid model design is described as a disconnected table. A disconnected table isn't intended to propagate filters to other model tables. Instead, it accepts "user input" (perhaps with a slicer visual), allowing model calculations to use the input value in a meaningful way. For example, consider a disconnected table that’s loaded with a range of currency exchange rate values. As long as a filter is applied to filter by a single rate value, a measure expression can use that value to convert sales values.

The Power BI Desktop what-if parameter is a feature that creates a disconnected table. For more information, see Create and use a What if parameter to visualize variables in Power BI Desktop.

Next unit: Set up relationships

Continue




Introduction - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/create-power-bi-model-relationships/1-introduction
Introduction
2 minutes

Together with model tables, model relationships form the basis of a tabular model. Relationships are responsible for the propagation of filters to other model tables. When correctly set up, they silently work to provide a natural and intuitive querying experience.

There’s a lot to learn about relationships, and it can take new developers time to understand how to set them up correctly, especially for complex data relationships. There are also Data Analysis Expressions (DAX) functions that work with model relationships. These functions allow your model calculation to navigate relationships, modify relationship behavior, and even create virtual relationships.

Learning objectives

By the end of this module, you’ll be able to:

Understand how model relationship work.
Set up relationships.
Use DAX relationship functions.
Understand relationship evaluation.




Create Power BI model relationships - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/create-power-bi-model-relationships/

Create Power BI model relationships
Module
8 Units
Feedback
Intermediate
Data Analyst
Power BI

Power BI model relationships form the basis of a tabular model. Define Power BI model relationships, set up relationships, recognize DAX relationship functions, and describe relationship evaluation.

Learning objectives

By the end of this module, you’ll be able to:

Understand how model relationship work.
Set up relationships.
Use DAX relationship functions.
Understand relationship evaluation.
Add
Prerequisites
Experience developing Power BI data models by using Power BI Desktop.
Introduction
min
Understand model relationships
min
Set up relationships
min
Use DAX relationship functions
min
Understand relationship evaluation
min
Exercise: Work with model relationships
min
Knowledge check
min
Summary
min


Summary - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/create-manage-scalable-power-bi-dataflows/7-summary
Summary
3 minutes

Power BI dataflows are an enterprise-focused data prep solution, enabling an ecosystem of data that's ready for consumption, reuse, and integration. Power BI dataflows work well for analysts who want to cleanse and transform data one time in Power Query online for reuse in other reports and by other analysts. They're also a great solution to reduce the load placed on source systems to extract data.

Learn more
Azure Data Factory wrangling overview
Understanding the differences between dataflow types
Developing dataflows solutions




Knowledge check - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/create-manage-scalable-power-bi-dataflows/6-knowledge-check
Knowledge check
5 minutes

Choose the best response for each of the questions below. Then select Check your answers.

Check your knowledge
1. 

Ivor is working on an analytics solution that requires ETL logic to be applied to a table that sits outside of the data warehouse. Multiple analysts need to use this supplemental cleaned data in their reports, and it's important that all reports reflect the exact version of the cleansed data. Analysts don't have access to the data source. What should Ivor do to provide analysts with this data at the lowest access level necessary?

 

Create a Power BI dataset in a shared workspace and give all analysts contributor access.

Create a Power BI dataflow in a shared workspace and give all analysts viewer access.

Create a Power BI dataflow in a shared workspace and give all analysts admin access.

2. 

Sherry is using a dataflow in Power BI Premium to refresh data from a web API. The volume of data is large and Sherry is using incremental refresh. Sherry notices that the dataflow refresh is suddenly taking much longer than it used to. Where can she check dataflow performance and investigate what might be going on?

 

The refresh history in the dataflow settings.

The performance analyzer in Power BI desktop.

The performance profiler in Visual Studio.

3. 

Juliane is creating a dataset to be used in multiple reports. To avoid duplicating logic, she wants to create a dataflow that references an existing dataflow. What requirement does Juliane need to confirm prior to creating her dataflow that contains the linked entity?

 

Juliane must have access to both of the underlying data sources.

The workspace where the dataflow resides must be in Premium capacity.

Both dataflows must reside in the same workspace.

Check your answers




Exercise: Create a dataflow - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/create-manage-scalable-power-bi-dataflows/5-exercise-create-dataflow
Exercise: Create a dataflow
45 minutes

Now it's your opportunity to create a dataflow yourself.

In this exercise, you'll use a provided script to provision an Azure Synapse Analytics workspace in your Azure subscription. You'll then create a dataflow to deliver date dimension data sourced from the Azure Synapse Adventure Works data warehouse. The dataflow will provide a consistent definition of date-related data for use by the organization's business analysts.

 Note

To complete this lab, you will need both an Azure subscription in which you have administrative access and a Power BI account. If you need a free trial Power BI account, sign up and follow the steps to create an account before continuing with this lab.

Launch the exercise and follow the instructions.




Create reusable assets - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/create-manage-scalable-power-bi-dataflows/3-create-reusable-assets
Create reusable assets
5 minutes

Dataflows are created and refreshed in the Power BI service, and can be consumed in either Power BI desktop or the Power BI service.

Create a dataflow

There are multiple methods of creating dataflows in the Power BI service. We're going to cover creating a dataflow using the option to define new tables.

You can also create dataflows using linked tables, a computed table, or using import/export. Refer to the Power BI documentation for step-by-step instructions of each creation option.

From within a shared workspace, select New Dataflow.

Here you'll have the option to define new tables, link tables from other dataflows, import model, or attach a Common Data Model folder. Select Add new tables.

Selecting Add new tables will direct you to Power Query online, where you'll choose a data source.

Depending on the data source you select, you'll need to enter connection settings and specify connection credentials. Specifying connection credentials will look similar to data connections using Power Query in Power BI desktop.

Enter the appropriate connection settings and credentials and select Sign In.

After signing in, you'll get a preview of the assets in the source data system. Here you can select which tables to use. Dataflows contain tables, but don't contain relationships.

Once you select the data you'd like to use, you can use the dataflow editor to shape and transform that data. The dataflow editor looks and behaves similar to Power Query in Power BI desktop.

 Important

Dataflows can only be created in shared workspaces.

Refresh a dataflow

When you create a dataflow, you're prompted to refresh the data for the dataflow. Refreshing a dataflow is required before it can be consumed in a dataset inside Power BI Desktop, or referenced as a linked or computed table.

To configure a dataflow refresh from the shared workspace, navigate to Settings via the More options menu.

Here you can take ownership of a dataflow, edit the data source credentials, schedule a refresh, configure enhanced compute engine settings, and endorse content.

Connect to a dataflow

Dataflows can be consumed in three ways. Report builders can:

Create a linked table from the dataflow.
Create a dataset from the dataflow.
Create a connection from external tools that can read from the common data model format.

You can connect to a dataflow in Power BI desktop using the Power BI dataflows connector in the Get Data window.

 Note

Learn more about configuring and consuming dataflows.

Refining dataflow settings

For dataflows in workspaces using Power BI Premium capacity, you can use the Admin portal to change, or refine, how dataflows are created and how they use resources in your Power BI Premium subscription. See refining dataflow settings to learn more.




Implement best practices - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/create-manage-scalable-power-bi-dataflows/4-implement-best-practices
Implement best practices
3 minutes

Dataflows can solve problems, but they can also create problems when implemented suboptimally.

Best practices

Keep the following best practices in mind when planning your dataflow implementation:

Bring in only data you need.
Leverage query folding.
Endorse your dataflows as either promoted or certified to encourage use.
Use incremental refresh to control partition processing.
Use Power Automate for trigger-based dataflow and dataset refresh.
Review and optimize dataflow refresh using refresh history and the CSV log.
Use dataflows in Power BI Premium to take advantage of:
Enhanced compute engine.
DirectQuery.
Computed entities.
Linked entities.
Incremental refresh.
Split things into multiple dataflows and reuse dataflows cross multiple workspaces.

 Tip

For more detailed information, see Dataflow best practices.




Introduction - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/create-manage-scalable-power-bi-dataflows/1-introduction
Introduction
2 minutes

Dataflows in Power BI enable standardized data preparation at a level in between the data source and Power BI reports. They're a great option in both self-service and enterprise analytics, enabling analysts to build reports on a single version of transformed data.

This module introduces Power BI dataflows, their use cases, and best practices in dataflow implementation. You'll also practice building a dataflow for use in a Power BI report.

Learning objectives

In this module, you will:

Describe Power BI dataflows and use cases.
Describe best practices for implementing Power BI dataflows.
Create and consume Power BI dataflows.




Define use cases for dataflows - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/create-manage-scalable-power-bi-dataflows/2-define-use-cases-for-dataflows
Define use cases for dataflows
9 minutes

Power BI dataflows enable you to build reusable data tables in a workspace using Power Query Online, and share them for use in other reports and with other users for reuse in other workspaces. Dataflows are objects in a workspace alongside datasets, dashboards, reports, and workbooks. When a Power BI dataflow is refreshed, behind the scenes it loads its data into files located in a data lake, Azure Data Lake Storage Gen 2 (ADLS Gen 2).

Power BI dataflows should be used in Premium capacity for enterprise solutions, to take advantage of features like advanced compute, incremental refresh, and linked and computed entities.

 Note

Dataflows are supported for Power BI Pro, Premium Per User (PPU), and Power BI Premium users. Learn more about Premium-only features of dataflows.

Data used with Power BI is stored in internal storage provided by Power BI by default. With the integration of dataflows and Azure Data Lake Storage Gen 2 (ADLS Gen2), you can store your dataflows in your organization's Azure Data Lake Storage Gen2 account. This essentially allows you to "bring your own storage" to Power BI dataflows, and establish a connection at the tenant or workspace level.

Why use dataflows?

Dataflows were designed to promote reusable ETL logic that prevents the need to create additional connections to your data source.

Dataflows are a great choice for you if:

There's no data warehouse in your organization.
You want to extend a core dataset or data in the data warehouse with consistent data.
Self service users need frequent access to an up-to-date subset of data from the data warehouse without having access to the data warehouse itself.
You have slower data sources.
Dataflows extract data once and reuse it multiple times, which can reduce the overall data refresh time for slower data sources.
Computed entities may be faster than referencing queries with the enhanced compute engine.
You have chargeable data sources.
Dataflows can reduce costs associated with data refresh if you're getting data from chargeable data sources.
Dataflows increase control and reduce the number of calls to the source system.
Datasets refresh against dataflows without affecting source systems.
You have different versions of datasets floating around your organization. Dataflows increase consistency between datasets.
Increased structural consistency by reducing the chance that users will prepare data differently
Increased temporal consistency by having a single set of data extracted from source systems at a single point in time
Shared tables that have no source, such as a standard date dimension, can be standardized across your organization.
You want to reduce or hide the complexity of data sources.
You can expose common data entities for larger groups of analysts that have already been transformed and simplified.
You can also partition data horizontally, using multiple data flows. For example, upstream dataflows contain all data and are available only to a small group of users. Downstream dataflows then contain curated subsets of data, and can be made available to members of appropriate security groups.
Benefits and limitations

While there are notable benefits to using dataflows in your dataset design, there are also a few limitations that users should keep in mind.

Benefits:

Reduced load on database queries.
Reduced number of users accessing source data.
Provides single version of properly structured data for analysts to build reports from.

Limitations:

Not a replacement for a data warehouse.
Row-level security isn't supported.
If not using dataflows in Premium capacity, performance can be an issue.

 Important

See Dataflow considerations and limitations for a complete list of considerations and limitations.

Dataflows in Power BI Premium

Power BI Premium was designed for enterprise deployments. Dataflow features available in premium offer substantial performance benefits and include the use of:

Enhanced compute engine
DirectQuery
Computed entities
Linked entities
Incremental refresh
Optimize dataflows using the enhanced compute engine

The enhanced compute engine in Power BI dataflows enables you to optimize the use of dataflows by:

Speeding up refresh operations when computed entities or linked entities are involved (for example, performing joins, distinct, filters, and group by).
Enabling DirectQuery connectivity over dataflows using the compute engine.
Achieve improved performance in the transformation steps of dataflows when entities are cached within the compute engine.

 Tip

Learn more about Power BI Premium features of dataflows.

Distinction between dataflows

Perhaps you've also heard of Azure Data Factory dataflows and you're wondering what the best type of dataflow is to use in your scenario.

Power BI dataflows and Azure Data Factory (ADF) wrangling dataflows are often considered to do the same thing: extract data from source systems, transform data, and load the transformed data into a destination. They're both powered by Power Query online, but there are differences in these two types of dataflows. You can implement a solution that works with a combination of the two.

When to use ADF wrangling dataflows or Power BI dataflows

Data transformation should always be done as close to the source as possible. If your analytics solution includes Azure Data Factory and you have the skills to implement transformations upstream of Power BI, you should.

Expand table
Features	Power BI dataflows	Data Factory wrangling dataflows
Destinations	Dataverse or Azure Data Lake Storage	Many destinations
Power Query transformation	All Power Query functions are supported	A limited set of functions is supported
Sources	Many sources are supported	Only a few sources
Scalability	Depends on the Premium capacity and the use of the enhanced compute engine	Highly scalable

 Tip

Learn more about how Microsoft Power Platform dataflows and Azure Data Factory wrangling dataflows relate to each other.




Create and manage scalable Power BI dataflows - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/create-manage-scalable-power-bi-dataflows/

Learn  Training  Browse  Prepare data for tabular models in Power BI 
800 XP
Create and manage scalable Power BI dataflows
1 hr 12 min
Module
7 Units
Feedback
Intermediate
Data Analyst
Power BI

Create Power BI transformation logic for reuse across your organization with Power BI dataflows. Learn how to combine Power BI dataflows with Power BI Premium for scalable ETL, and practice creating and consuming dataflows.

Learning objectives

By the end of this module, you’ll be able to:

Describe Power BI dataflows and use cases.
Describe best practices for implementing Power BI dataflows.
Create and consume Power BI dataflows.
Start
Add
Prerequisites
You'll need knowledge of Power BI data model design including star schema design basics.
Consider completing the Model data in Power BI learning path.
This module is part of these learning paths
Prepare data for tabular models in Power BI
Introduction
2 min
Define use cases for dataflows
9 min
Create reusable assets
5 min
Implement best practices
3 min
Exercise: Create a dataflow
45 min
Knowledge check
5 min
Summary
3 min


Summary - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/understand-scalability-power-bi/6-summary
Summary
2 minutes

You've learned that scalability and working with large data is achievable in Power BI. The correct model framework, a proper data model, and using Power BI Premium features like large dataset storage enables performant enterprise reporting in Power BI.

Learn more
Power BI enterprise documentation
Power BI implementation planning: Advanced data model management




Knowledge check - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/understand-scalability-power-bi/5-knowledge-check
Knowledge check
3 minutes

Choose the best response for each of the questions below. Then select Check your answers.

Check your knowledge
1. 

What is the most efficient approach for data transformation when designing data models for scalability?

 

Data transformation should always be done using Power Query in Power BI.

Data transformation should be done as close to the source as possible, before reaching Power BI.

Data transformation should be done using DAX, after data is ingested and loaded to the model.

2. 

Which of the following best practices for Power BI data modeling is relevant only to DirectQuery models?

 

Set relationships to enforce integrity using the assume referential integrity property on relationships.

Use a star schema as opposed to wide tables.

Disable auto date/time in Power BI Desktop.

3. 

What are the data model size limits on a dataset with large dataset storage format determined by?

 

Power BI Premium 10 GB size limit.

Power BI Premium capacity size or the maximum size set by the administrator.

There are no size limits if large dataset storage format is enabled.

Check your answers




Exercise: Create a star schema model - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/understand-scalability-power-bi/4a-exercise-create-star-schema-model
Exercise: Create a star schema model
45 minutes

Now it's your opportunity to try creating a star schema in Power BI yourself, connecting to an Azure Synapse Analytics dedicated SQL pool. In this exercise, you'll use a provided script to provision an Azure Synapse Analytics workspace in your Azure subscription, develop a dataset, and create a star schema model in Power BI desktop.

 Note

To complete this lab, you will need both an Azure subscription in which you have administrative access and a Power BI account. If you need a free trial Power BI account, sign up and follow the steps to create an account before continuing with this lab. Launch the exercise and follow the instructions.




Implement Power BI data modeling best practices - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/understand-scalability-power-bi/3-implement-data-modeling-best-practices
Implement Power BI data modeling best practices
7 minutes

Implementing data modeling best practices is key to performant, scalable data models.

Choose the correct Power BI model framework

Choosing the correct Power BI model framework is at the heart of building any scalable solution.

The first place to start with your Power BI data model is import mode. Import mode offers you the most options, design flexibility, and delivers fast performance.

Use DirectQuery when your data source stores large volumes of data and/or your report needs to deliver near real-time data.

Finally, use a composite model when you need to:

Boost the query performance of a DirectQuery model.
Deliver near real-time query results from an import model.
Extend a Power BI dataset (or Azure Analysis Services model) with other data.

Composite models combine data from more than one DirectQuery source or combine DirectQuery with import data.

 Important

Review the Choose a Power BI model framework module for more information on using import, DirectQuery, or composite models.

Implement data modeling best practices

There are some basic principles to abide by when building any data model. These principles become even more important as data begins to grow.

Most importantly, you want to do as much data preparation work as possible before data reaches Power BI, as far upstream as possible. For example, if you have the opportunity to transform data in the data warehouse, that's where it should be done. Transformation at the source produces consistency for any other solutions built on that data and ensures that your Power BI model doesn't need to do any extra processing. This may require working with your data engineer or other members of the data team and is critically important.

Best practices for import mode:
Always start with import mode if you can.
Only bring in data you need.
Remove unnecessary rows and columns.
Only process what is absolutely necessary (tables/partitions) given the business requirements.
Avoid wide tables.
Use a star schema in Power BI.
If your source is a beautifully modeled data warehouse, you're a step ahead.
Big data is often in wide flat tables. Take advantage of dimensional models for their performance benefits.
Power BI supports multiple fact tables with different dimensionality and different granularities – you don’t have to put everything into one large table.
Pre-aggregate data before loading it to the model where possible.
Reduce the usage of calculated columns.
Data transformations requiring additional columns should be done as close to the source as possible.
Avoid high cardinality columns.
Consider breaking a datetime column into two columns, one for date and one for time.
Use appropriate data types.
Use integers instead of strings for ID columns.
Use surrogate keys for ID columns if necessary.
Limit the use of bi-directional filters on relationships.
Disable auto date/time.
Connect to a date table at the source or create your own date table.
Disable attribute hierarchies for non-attribute columns.
If querying a relational database, query database views rather than tables.
A view provides an abstraction layer to manage columns, and relates back to the first consideration, pushing transformations as close to the source as possible.
Views shouldn't contain logic. They should only contain a SELECT statement from a table.
Consider partitioning and incremental refresh to avoid loading data you don’t need to.
Check to ensure query folding is achieved.
If query folding isn't possible, you have another opportunity to work with the data engineer to move transformation upstream.

 Tip

Learn more about techniques to help reduce the data loaded into import models.

Best practices specific to DirectQuery mode:
Set relationships to enforce integrity using the Assume referential integrity property on relationships.
The Assume Referential Integrity setting on relationships enables queries to use INNER JOIN statements rather than OUTER JOIN.
Limit the use of bi-directional filters on relationships.
Use only when necessary.
Limit the complexity of DAX calculations.
Because query folding occurs by default in DirectQuery, more complex DAX measures means added complexity at the source, leading to slow queries.
The need for complex DAX also leads back to the key principle of applying transformations as far upstream as possible. You may need to work with the data engineer to apply transformations at the source.
Avoid the use of calculated columns.
Transformations requiring additional columns should be done as far upstream as possible, particularly when using DirectQuery.
Avoid relationships on calculated columns
Avoid relationships on Unique Identifier columns
Use dual storage mode for dimensions related to fact tables that are in DirectQuery.

 Note

Refer to the DirectQuery model guidance for a complete list of considerations in developing DirectQuery models.

There's also a tool you can use as you're developing tabular models that will alert you of modeling missteps or changes that would improve model design and performance. The Best Practice Analyzer within Tabular Editor was designed to help you design models that adhere to modeling best practices.

In the next unit, you'll learn how to configure the large dataset storage format using Power BI Premium.




Configure large datasets - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/understand-scalability-power-bi/4-configure-large-datasets
Configure large datasets
4 minutes

Power BI datasets store data in a highly compressed, in-memory cache for optimized query performance. Enterprise deployment of an analytics solution using Power BI will likely require Power BI Premium. With the large dataset storage format enabled, dataset sizes are limited only by the capacity size, or a maximum size set by the administrator. This differs from datasets in Power BI Premium, which are limited to 10 GB after compression if large dataset storage format isn't enabled.

Large datasets can be enabled for all Premium P SKUs, Embedded A SKUs, and with Premium Per User (PPU). The large dataset size limit in Premium is comparable to Azure Analysis Services, in terms of data model size limitations.

The large dataset feature brings the Power BI dataset cache sizes to parity with Azure Analysis Services model sizes. The large dataset feature enables consolidation of tabular models from SQL Server Analysis Services and Azure Analysis Services on one common platform based on Power BI Premium.

 Note

To use large dataset storage format, the dataset must be stored in a workspace that allocated to Premium capacity.

Enabling the large dataset format enables fast user interactivity and allows data to grow beyond the 10-GB limit. Additionally, the large dataset format can also improve xmla write operation performance, even for datasets that may not be large.

 Important

Datasets enabled for large models can't be downloaded as a Power BI Desktop (.pbix) file from the Power BI service. Read more about .pbix download limitations.

Enable large dataset storage format

To take advantage of the large dataset storage format option, it must be enabled in the Power BI service. Here you can enable large dataset storage format for a single dataset, or for all datasets created in a workspace.

Enable large dataset storage format for a single dataset

In the dataset settings in the Power BI service, toggle the slider to on and select Apply.

Enable large dataset storage format for all datasets created in a workspace

You can set the default storage format for all datasets created in a workspace in the workspace settings. In the settings, select Premium, and select Large dataset storage format as the Default storage format.

Large dataset storage format for a workspace can also be enabled using PowerShell.

 Note

See Configure large datasets to learn more about large models in Power BI Premium including information on checking dataset size, dataset eviction, considerations, and limitations.




Describe the significance of scalable models - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/understand-scalability-power-bi/2-describe-significance-of-scalable-models
Describe the significance of scalable models
4 minutes

One of the keys to manageable, performant solutions with large data is good model design. We'll discuss model scalability, why it's important, and what tools exist in Power BI to help you and your team accomplish your goals.

What is enterprise or large-scale data?

Before we talk through scalability, let's define what we're talking about. You'll see throughout the module that we refer to enterprise-scale or large-scale data rather than big data. In this module, enterprise-scale or large-scale data refers to tables with a large number of records or rows. Power BI, used with tools like Azure Synapse Analytics, can analyze massive datasets, in the range of trillions of rows or petabytes of data.

If you're familiar with working with enterprise data, it may be helpful to understand that Power BI is the next generation of Analysis Services. It's the same technology under the hood of Analysis Services and Power BI datasets, the VertiPaq engine.

 Tip

Take a look at the Model, query, and explore data in Azure Synapse learning path for more information on data analytics in Azure.

What is scalability and why is it important?

Scalability in this context refers to building data models that can handle growth in the volume of data. A data model that ingests thousands of rows of data may grow to millions of rows over time, and the model must be designed to accommodate such growth. It's important to consider that your data will grow and/or change, which increases complexity.

Scalability must be at the forefront in enterprise solutions to ensure:

Flexibility - models need to be able to accommodate change
Data growth - models must be able to handle an increase in data volume with acceptable report performance
Reduced complexity - models built with scalability in mind will be less complex and easier to manage
How do I design for scalability?

The best approach to building scalable Power BI data models will always be building with data modeling best practices in mind.

Beyond the data model, Power BI Premium was designed specifically for enterprise deployments. Premium capacity offers greater storage capacity and allows for larger individual datasets depending on the SKU. Implementing the premium only large dataset storage feature enables data to grow beyond the Power BI desktop (.pbix) file size limitations.

 Tip

Are you planning a Power BI enterprise deployment? Read the Power BI enterprise deployment whitepaper for a full list of enterprise deployment considerations.

Another important consideration in designing for scalability using Power BI Premium is choosing the right capacity. You'll need to work with your Power BI administrator to determine which Power BI Premium licensing SKU is available to you. If you're having performance issues in Premium capacity, work first to optimize your model, and then work with your Power BI administrator to monitor Power BI Premium capacities.

At the most basic level, it's important to understand that Premium capacities require sufficient memory for processing. You'll need to double the amount of RAM to process your data model refresh. For example, if you have a 40-GB dataset, you'll need at least 80-GB of memory available. A 40-GB dataset would be best supported by a P3/A6 capacity, which contains 100-GB of memory.

 Tip

Review Power BI license types and capabilities. If you're not sure which license type your organization has, check with the Power BI administrator.




Introduction - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/understand-scalability-power-bi/1-introduction
Introduction
3 minutes

Perhaps you've heard that Power BI can seamlessly handle trillions of rows of data, but you're not able to accomplish that in your own Power BI tenant. Working with high volume and large-scale data can be done in Power BI with the right groundwork in place.

This module introduces considerations for building enterprise scale, IT-driven solutions. You'll review data modeling best practices and Power BI Premium features for working with large data.

Learning objectives

In this module, you will:

Describe the importance of building scalable data models
Implement Power BI data modeling best practices
Use the Power BI large dataset storage format




Understand scalability in Power BI - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/understand-scalability-power-bi/

Understand scalability in Power BI
Module
7 Units
Feedback
Intermediate
Data Analyst
Azure Synapse Analytics
Power BI

Scalable data models enable enterprise-scale analytics in Power BI. Implement data modeling best practices, use large dataset storage format, and practice building a star schema to design analytics solutions that can scale.

Learning objectives

By the end of this module, you’ll be able to:

Describe the importance of building scalable data models
Implement Power BI data modeling best practices
Use the Power BI large dataset storage format
Add
Prerequisites

Consider completing the Model data in Power BI learning path. You will need knowledge of:

Power BI data model design including star schema design basics
Introduction
min
Describe the significance of scalable models
min
Implement Power BI data modeling best practices
min
Configure large datasets
min
Exercise: Create a star schema model
min
Knowledge check
min
Summary
min


Summary - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/choose-power-bi-model-framework/8-summary
Summary
3 minutes

You can develop your Power BI model based on three different frameworks: import, DirectQuery, and composite. Each framework has its own benefits and limitations, and features to help you optimize your model.

Ultimately, you should strive to develop a model that efficiently delivers fast performance with low latency, even for high volume data sources.




Knowledge check - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/choose-power-bi-model-framework/7-knowledge-check
Knowledge check
5 minutes

Choose the best response for each of the questions below. Then select Check your answers.

Check your knowledge
1. 

Geoffrey is a data modeler at Adventure Works who developed a DirectQuery model that connects to the data warehouse. To improve the query performance of higher-grain sales queries, Geoffrey added an import aggregation table. What else should Geoffrey do to improve query performance of the higher-grain queries?

 

Set related dimension tables as aggregation tables.

Set related dimension tables to dual storage mode.

Set related dimension tables to import storage mode.

2. 

Breana is a data modeler at Adventure Works who developed a manufacturing model, which is an import model. Breana needs to ensure that manufacturing reports deliver real-time results. Which type of table should Breana create?

 

Aggregation table.

Hybrid table.

Partitioned table.

3. 

Mousef is a business analyst at Adventure Works who wants to create a new model by extending the sales dataset, which is delivered by IT. Mousef wants to add a new table of census population data sourced from a web page. Which model framework should Mousef use?

 

Composite.

DirectQuery.

Live connection.

Check your answers




Choose a model framework - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/choose-power-bi-model-framework/6-choose-model-framework
Choose a model framework
3 minutes

Here’s some general guidance about choosing the appropriate model framework for your project. It especially applies to enterprise solutions, where data volumes are large, query throughput is high, and fast responsiveness is important.

Most importantly, choose the import model framework whenever possible. This framework offers you the most options, design flexibility, and delivers fast performance. Be sure to apply data reduction techniques to ensure that Power BI loads the least amount of data possible.

Choose the DirectQuery model framework when your data source stores large volumes of data and/or your report needs to deliver near real-time data.

Choose the composite model framework to:

Boost the query performance of a DirectQuery model.
Deliver near real-time query results from an import model.
Extend a Power BI dataset (or AAS model) with additional data.

You can boost the query performance of a DirectQuery model by using aggregation tables, which can use import or DirectQuery storage mode. When using import aggregation tables, be sure to set related dimension tables to use dual storage mode. That way, Power BI can satisfy higher-grain queries entirely from cache.

You can deliver near real-time query results from in import model by creating a hybrid table. In this case, Power BI adds a DirectQuery partition for the current period.

Lastly, you can create specialized models by chaining to a core model by using DirectQuery. This type of development is typically done by a business analyst who extends core models, which IT delivers and supports.

 Important

Plan carefully. In Power BI Desktop, it’s always possible to convert a DirectQuery table to an import table. But it’s not possible to convert an import table to a DirectQuery table.




Determine when to develop a composite model - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/choose-power-bi-model-framework/5-determine-when-to-develop-composite-model
Determine when to develop a composite model
7 minutes

A composite model comprises more than one source group. Typically, there’s always the import source group and a DirectQuery source group.

 Note

Generally, the benefits and limitations associated with import and DirectQuery storage modes apply to composite models.

Composite model benefits

There are several benefits to developing a composite model.

Composite models provide you with design flexibility. You can choose to integrate data using different storage modes, striking the right balance between imported data and pass-through data. Commonly, enterprise models benefit from using DirectQuery tables on large data sources and by boosting query performance with imported tables. Power BI features that support this scenario are described later in this unit.

Composite models can also boost the performance of a DirectQuery model by providing Power BI with opportunity to satisfy some analytic queries from imported data. Querying cached data almost always performs better than pass-through queries.

Lastly, when your model includes DirectQuery tables to a remote model, like a Power BI dataset, you can extend your model with new calculated columns and tables. It results in a specialized model based on a core model. For more information, see Power BI usage scenarios: Customizable managed self-service BI.

Composite model limitations

There are several limitations related to composite models.

Import (or dual, as described later) storage mode tables still require periodic refresh. Imported data can become out of sync with DirectQuery sourced data, so it’s important to refresh it periodically.

When an analytic query must combine imported and DirectQuery data, Power BI must consolidate source group query results, which can impact performance. To help avoid this situation for higher-grain queries, you can add import aggregation tables to your model (or enable automatic aggregations) and set related dimension tables to use dual storage mode. This scenario is described later in this unit.

When chaining models (DirectQuery to Power BI datasets), modifications made to upstream models can break downstream models. Be sure to assess the impact of modifications by performing dataset impact analysis first.

Relationships between tables from different source groups are known as limited relationships. A model relationship is limited when the Power BI can’t determine a “one” side of a relationship. Limited relationships may result in different evaluations of model queries and calculations. For more information, see Relationship evaluation.

Boost DirectQuery model performance with import data

When there’s a justification to develop a DirectQuery model, you can mitigate some limitations by using specific Power BI features that involve import tables.

Import aggregation tables

You can add import storage mode user-defined aggregation tables or enable automatic aggregations. This way, Power BI directs higher-grain fact queries to a cached aggregation. To boost query performance further, ensure that related dimension tables are set to use dual storage mode.

Automatic aggregations are a Premium feature. For more information, see Automatic aggregations.

Dual storage mode

A dual storage mode table is set to use both import and DirectQuery storage modes. At query time, Power BI determines the most efficient mode to use. Whenever possible, Power BI attempts to satisfy analytic queries by using cached data.

Dual storage mode tables work well with import aggregation tables. They allow Power BI to satisfy higher-grain queries entirely from cached data.

Slicer visuals and filter card lists, which are often based on dimension table columns, render more quickly because they’re queried from cached data.

Deliver real-time data from an import model

When you set up an import table with incremental refresh, you can enable the Get the latest data in real-time with DirectQuery option.

By enabling this option, Power BI automatically creates a table partition that uses DirectQuery storage mode. In this case, the table becomes a hybrid table, meaning it has import partitions to store older data, and a single DirectQuery partition for current data.

When Power BI queries a hybrid table, the query uses the cache for older data, and passes through to the data source to retrieve current data.

This option is only available with a Premium license.

For more information, see Configure incremental refresh and real-time data.




Determine when to develop a DirectQuery model - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/choose-power-bi-model-framework/4-determine-when-to-develop-directquery-model

Learn  Training  Browse  Choose a Power BI model framework 
Add
Previous
Unit 4 of 8
Next
Determine when to develop a DirectQuery model
Completed
100 XP
9 minutes

A DirectQuery model comprises tables that have their storage mode property set to DirectQuery, and they belong to the same source group.

A source group is a set of model tables that relate to a data source. There are two types:

Import – Represents all import storage mode tables including calculated tables. There can only be one import source group in a model.
DirectQuery – Represents all DirectQuery storage mode tables that relate to a specific data source.

 Note

An import model and a DirectQuery model only comprise a single source group. When there’s more than one source group, the model framework is known as a composite model. Composite models are described in Unit 5.

DirectQuery model benefits

There are several benefits to developing a DirectQuery model.

Model large or fast-changing data sources

A DirectQuery model is a great framework choice when your source data exhibits volume and/or velocity characteristics. Because DirectQuery tables don’t require refresh, they’re well suited to large data stores, like a data warehouse. It’s impractical and inefficient, if not impossible, to import an entire data warehouse into a model. When the source data changes rapidly and users need to see current data, a DirectQuery model can deliver near real-time query results.

When a report queries a DirectQuery model, Power BI passes those queries to the underlying data source.

Enforce source RLS

DirectQuery is also useful when the source database enforces row-level security (RLS). Instead of replicating RLS rules in your Power BI model, the source data base can enforce its rules. This approach works only for some relational databases, and it involves setting up single sign-on for the dataset data source. For more information, see Azure SQL Database with DirectQuery.

Data sovereignty restrictions

If your organization has security policies that restrict data leaving their premises, then it isn’t possible to import data. A DirectQuery model that connects to an on-premises data source may be appropriate. (You can also consider installing Power BI Report Server for on-premises reporting.)

Create specialized datasets

Typically, DirectQuery mode supports relational database sources. That’s because Power BI must translate analytic queries to native queries understood by the data source.

However, there’s one powerful exception. You can connect to a Power BI dataset (or Azure Analysis Services model) and convert it to a DirectQuery local model. A local model is a relative term that describes a model’s relationship to another model. In this case, the original dataset is a remote model, and the new dataset is the local model. These models are chained, which is term used to describe related models. You can chain up to three models in this way.

This capability to chain models supports the potential to personalize and/or extend a remote model. The simplest thing you can do is rename objects, like tables or columns, or add measures to the local model. You can also extend the model with calculated columns or calculated tables, or add new import or DirectQuery tables. However, these extensions result in the creation of new source groups, which means the model becomes a composite model. That scenario is described in Unit 5.

For more information, see Using DirectQuery for Power BI datasets and Azure Analysis Services.

DirectQuery model limitations

There are many limitations related to DirectQuery models that you must bear in mind. Here are the main limitations:

Not all data sources are supported. Typically, only major relational database systems are supported. Power BI datasets and Azure Analysis Services models are supported too.

All Power Query (M) transformations are not possible, because these queries must translate to native queries that are understood by source systems. So, for example, it’s not possible to use pivot or unpivot transformations.

Analytic query performance can be slow, especially if source systems aren’t optimized (with indexes or materialized views), or there are insufficient resources for the analytic workload.

Analytic queries can impact on source system performance. It could result in a slower experience for all workloads, including OLTP operations.

Boost DirectQuery model performance

When there’s a justification to develop a DirectQuery model, you can mitigate some limitations in two ways.

Data source optimizations

You can optimize the source database to ensure the expected analytic query workload performs well. Specifically, you can create indexes and materialized views, and ensure the database has sufficient resources for all workloads.

 Tip

We recommend that you always work in collaboration with the database owner. It’s important that they understand the additional workload a DirectQuery model can place on their database.

DirectQuery user-defined aggregation tables

You can add user-defined aggregation tables to a DirectQuery model. User-defined aggregation tables are special model tables that are hidden (from users, calculations, and RLS). They work best when they satisfy higher-grain analytic queries over large fact tables. When you set the aggregation table to use DirectQuery storage mode, it can query a materialized view in the data source. You can also set an aggregation table to use import storage mode or enable automatic aggregations, and these options are described in Unit 4.

For more information, see DirectQuery model guidance in Power BI Desktop.

Next unit: Determine when to develop a composite model

Continue




Determine when to develop an import model - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/choose-power-bi-model-framework/3-determine-when-to-develop-import-model
Determine when to develop an import model
7 minutes

An import model comprises tables that have their storage mode property set to Import. It includes calculated tables, which you define with a DAX formula, too.

Import model benefits

Import models are the most frequently developed model framework because there are many benefits. Import models:

Support all Power BI data source types, including databases, files, feeds, web pages, dataflows, and more.
Can integrate source data. For example, one table sources its data from a relational database while a related table sources its data from a web page.
Support all DAX and Power Query (M) functionality.
Support calculated tables.
Deliver the best query performance. That’s because the data cached in the model is optimized for analytic queries (filter, group, and summarize) and the model is stored entirely in memory.

In short, import models offer you the most options and design flexibility, and they deliver fast performance. For this reason, Power BI Desktop defaults to use import storage mode when you “Get data.”

Import model limitations

Despite the many compelling benefits, there are limitations of import models that you must bear in mind. Limitations are related to model size and data refresh.

Model size

Power BI imposes dataset size restrictions, which limit the size of a model. When you publish the model to a shared capacity, there’s a 1-GB limit per dataset. When this size limit is exceeded, the dataset will fail to refresh. When you publish the model to a dedicated capacity (also known as Premium capacities), it can grow beyond 10-GB, providing you enable the Large dataset storage format setting for the capacity.

You should always strive to reduce the amount of data stored in tables. This strategy helps to reduce the duration of model refreshes and speed up model queries. There are numerous data reduction techniques that you can apply, including:

Remove unnecessary columns
Remove unnecessary rows
Group by and summarize to raise the grain of fact tables
Optimize column data types with a preference for numeric data
Preference for custom columns in Power Query instead of calculated columns in the model
Disable Power Query query load
Disable auto date/time
Use DirectQuery table storage, as described in later units of this module.

For more information, see Data reduction techniques for Import modeling.

 Note

The 1-GB per dataset limit refers to the compressed size of the Power BI model, not the volume of data being collected from the source system.

Data refresh

Imported data must be periodically refreshed. Dataset data is only as current as the last successful data refresh. To keep data current, you set up scheduled data refresh, or report consumers can perform an on-demand refresh.

Power BI imposes limits on how often scheduled refresh operations can occur. It’s up to eight times per day in a shared capacity, and up to 48 times per day in a dedicated capacity.

You should determine whether this degree of latency is tolerable. It often depends on the velocity (or volatility) of the data, and the urgency to keep users informed about the current state of data. When scheduled refresh limits aren’t acceptable, consider using DirectQuery storage tables, or creating a hybrid table. Or take a different approach, and create a real-time dataset instead.

 Tip

Hybrid tables are described in unit 4. For information about real-time datasets, work through the Monitor data in real-time with Power BI module.

You must also consider refresh workload and duration. By default, to refresh a table, Power BI removes all data and reloads it again. These operations can place an unacceptable burden on source systems, especially for large fact tables. To reduce this burden, you can set up the incremental refresh feature. Incremental refresh automates the creation and management of time-period partitions, and intelligently update only those partitions that require refresh.

When your data source supports incremental refresh, it can result in faster and more reliable refreshes, and reduced resource consumption of Power BI and source systems.

Advanced data modelers can customize their own partitioning strategy. Automation scripts can create, manage, and refresh table partitions. For more information, see Power BI usage scenarios: Advanced data model management. This usage scenario describes using the XMLA endpoint available with Power BI Premium.




Describe Power BI model fundamentals - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/choose-power-bi-model-framework/2-describe-power-bi-model-fundamentals
Describe Power BI model fundamentals
9 minutes

This unit introduces Power BI model terms. It’s important that you understand these terms in order to choose the appropriate model framework for your project. This unit describes the following terms:

Data model
Power BI dataset
Analytic query
Tabular model
Star schema design
Table storage mode
Model framework
Data model

A Power BI data model is a query-able data resource that’s optimized for analytics. Reports can query data models by using one of two analytic languages: Data Analysis Expressions (DAX) or Multidimensional Expressions (MDX). Power BI uses DAX, while paginated reports can use either DAX or MDX. The Analyze in Excel features uses MDX.

 Tip

A data model is also described as semantic model, especially in enterprise scenarios. Commonly, in the context of a data discussion and in this module, a data model is simply referred to as a model.

Power BI dataset

You develop a Power BI model in Power BI Desktop, and once published to a workspace in the Power BI service, it’s then known as a dataset. A dataset is a Power BI artifact that’s a source of data for visualizations in Power BI reports and dashboards.

 Note

Not all datasets originate from models developed in Power BI Desktop. Some datasets represent connections to external-hosted models in AAS or SSAS. Others can represent real-time data structures, including push datasets, streaming datasets, or hybrid datasets. This module is concerned only with models developed in Power BI Desktop.

Analytic query

Power BI reports and dashboards must query a dataset. When Power BI visualizes dataset data, it prepares and sends an analytic query. An analytic query produces a query result from a model that’s easy for a person to understand, especially when visualized.

An analytic query has three phases that are executed in this order:

Filter
Group
Summarize

Filtering (sometimes known as slicing) narrows down on a subset of the model data. Filter values aren’t visible in the query result. Most analytic queries apply filters because it’s common to filter by a time period, and usually other attributes. Filtering happens in different ways. In a Power BI report, you can set filters at report, page, or visual level. Report layouts often include slicer visuals to filter visuals on the report page. When the model enforces row-level security (RLS), it applies filters to model tables to restrict access to specific data. Measures, which summarize model data, can also apply filters.

Grouping (sometimes known as dicing) divides query result into groups. Each group is also a filter, but unlike the filtering phase, filter values are visible in the query result. For example, grouping by customer filters each group by customer.

Summarization produces a single value result. Typically, a report visual summarizes a numeric field by using an aggregate function. Aggregate functions include sum, count, minimum, maximum, and others. You can achieve simple summarization by aggregating a column, or you can achieve complex summarization by creating a measure using a DAX formula.

Consider an example: A Power BI report page includes a slicer to filter by a single year. There’s also a column chart visual that shows quarterly sales for the filtered year.

In this example, the slicer filters the visual by calendar year 2021. The column chart groups by quarters (of the filtered year). Each column is a group that represents a visible filter. The column heights represent the summarized sales values for each quarter of the filtered year.

Tabular model

A Power BI model is a tabular model. A tabular model comprises one or more tables of columns. It can also include relationships, hierarchies, and calculations.

Star schema design

To produce an optimized and easy-to-use tabular model, we recommend you produce a star schema design. Star schema design is a mature modeling approach widely adopted by relational data warehouses. It requires you to classify model tables as either dimension or fact.

Dimension tables describe business entities; the things you model. Entities can include products, people, places, and concepts including time itself. Fact tables store observations or events, and can be, for example, sales orders, stock balances, exchange rates, or temperature readings. A fact table contains dimension key columns that relate to dimension tables, and numeric measure columns. A fact table forms the center of a star, and the related dimension tables form the points of the star.

In an analytic query, dimensions table columns filter or group. Fact table columns are summarized.

For more information, see Understand star schema and the importance for Power BI.

Table storage mode

Each Power BI model table (except calculated tables) has a storage mode property. The storage mode property can be either Import, DirectQuery, or Dual, and it determines whether table data is stored in the model.

Import – Queries retrieve data that’s stored, or cached, in the model.
DirectQuery – Queries pass through to the data source.
Dual – Queries retrieve stored data or pass through to the data source. Power BI determines the most efficient plan, striving to use cached data whenever possible.
Model framework

Table storage mode settings determine the model framework, which can be either import, DirectQuery, or composite. The following units in this module describe each of these frameworks and provides guidance on their use.

An import model comprises tables that have their storage mode property set to Import.
A DirectQuery model comprises tables that have their storage mode property set to DirectQuery, and they belong to the same source group. Source group is described later in this module.
A composite model comprises more than one source group.




Introduction - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/choose-power-bi-model-framework/1-introduction
Introduction
3 minutes

For over two decades, Microsoft continues to make deep investments in enterprise business intelligence (BI). Azure Analysis Services (AAS) and SQL Server Analysis Services (SSAS) are based on mature BI data modeling technology used by countless enterprises. The same technology is also at the heart of Power BI data models.

Power BI offers you a choice when designing your model. You can use Power BI Desktop to develop your model, and you can develop it by using different frameworks. These frameworks help to deliver fast performance, near real-time results, or both.

This module introduces the frameworks, their benefits and limitations, and features to help optimize your models. Lastly, it provides you with guidance to help you choose the right framework and features for your project.

Learning objectives

By the end of this module, you’ll be able to:

Describe Power BI model fundamentals.
Determine when to develop an import model.
Determine when to develop a DirectQuery model.
Determine when to develop a composite model.
Choose an appropriate Power BI model framework.




Choose a Power BI model framework - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/choose-power-bi-model-framework/

Choose a Power BI model framework
Module
8 Units
Feedback
Intermediate
Data Analyst
Power BI

Describe model frameworks, their benefits and limitations, and features to help optimize your Power BI data models.

Learning objectives

By the end of this module, you’ll be able to:

Describe Power BI model fundamentals.
Determine when to develop an import model.
Determine when to develop a DirectQuery model.
Determine when to develop a composite model.
Choose an appropriate Power BI model framework.
Add
Prerequisites
Experience developing Power BI data models, reports, and dashboards.
Introduction
min
Describe Power BI model fundamentals
min
Determine when to develop an import model
min
Determine when to develop a DirectQuery model
min
Determine when to develop a composite model
min
Choose a model framework
min
Knowledge check
min
Summary
min


Summary - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/design-multidimensional-schema-to-optimize-analytical-workloads/8-summary
Summary
1 minute

Relational data warehousing skills are essential in multiple data professional roles, including data engineers, data analysts, and data scientists.

In this module, you learned how to:

Design a schema for a relational data warehouse.
Create fact, dimension, and staging tables.
Use SQL to load data into data warehouse tables.
Use SQL to query relational data warehouse tables.
Learn more

To learn more about using Azure Synapse Analytics for relational data warehousing, refer to Synapse POC playbook: Data warehousing with dedicated SQL pool in Azure Synapse Analytics.




Knowledge check - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/design-multidimensional-schema-to-optimize-analytical-workloads/7-knowledge-check
Knowledge check
5 minutes
1. 

In which of the following table types should an insurance company store details of customer attributes by which claims will be aggregated?

 

Staging table

Dimension table

Fact table

2. 

You create a dimension table for product data, assigning a unique numeric key for each row in a column named ProductKey. The ProductKey is only defined in the data warehouse. What kind of key is ProductKey?

 

A surrogate key

An alternate key

A business key

3. 

What distribution option would be best for a sales fact table that will contain billions of records?

 

HASH

ROUND_ROBIN

REPLICATE

4. 

You need to write a query to return the total of the UnitsProduced numeric measure in the FactProduction table aggregated by the ProductName attribute in the FactProduct table. Both tables include a ProductKey surrogate key field. What should you do?

 

Use two SELECT queries with a UNION ALL clause to combine the rows in the FactProduction table with those in the FactProduct table.

Use a SELECT query against the FactProduction table with a WHERE clause to filter out rows with a ProductKey that doesn't exist in the FactProduct table.

Use a SELECT query with a SUM function to total the UnitsProduced metric, using a JOIN on the ProductKey surrogate key to match the FactProduction records to the FactProduct records and a GROUP BY clause to aggregate by ProductName.

5. 

You use the RANK function in a query to rank customers in order of the number of purchases they have made. Five customers have made the same number of purchases and are all ranked equally as 1. What rank will the customer with the next highest number of purchases be assigned?

 

two

six

one

6. 

You need to compare approximate production volumes by product while optimizing query response time. Which function should you use?

 

COUNT

NTILE

APPROX_COUNT_DISTINCT

Check your answers




Exercise - Explore a data warehouse - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/design-multidimensional-schema-to-optimize-analytical-workloads/6-exercise-explore-data-warehouse
Exercise - Explore a data warehouse
45 minutes

Now it's your opportunity to explore a relational data warehouse. In this exercise, you'll use a provided script to provision an Azure Synapse Analytics workspace in your Azure subscription; and then explore a data warehouse that has been created for you.

 Note

To complete this lab, you will need an Azure subscription in which you have administrative access.

Launch the exercise and follow the instructions.




Query a data warehouse - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/design-multidimensional-schema-to-optimize-analytical-workloads/5-query-data
Query a data warehouse
10 minutes

When the dimension and fact tables in a data warehouse have been loaded with data, you can use SQL to query the tables and analyze the data they contain. The Transact-SQL syntax used to query tables in a Synapse dedicated SQL pool is similar to SQL used in SQL Server or Azure SQL Database.

Aggregating measures by dimension attributes

Most data analytics with a data warehouse involves aggregating numeric measures in fact tables by attributes in dimension tables. Because of the way a star or snowflake schema is implemented, queries to perform this kind of aggregation rely on JOIN clauses to connect fact tables to dimension tables, and a combination of aggregate functions and GROUP BY clauses to define the aggregation hierarchies.

For example, the following SQL queries the FactSales and DimDate tables in a hypothetical data warehouse to aggregate sales amounts by year and quarter:

SELECT  dates.CalendarYear,
        dates.CalendarQuarter,
        SUM(sales.SalesAmount) AS TotalSales
FROM dbo.FactSales AS sales
JOIN dbo.DimDate AS dates ON sales.OrderDateKey = dates.DateKey
GROUP BY dates.CalendarYear, dates.CalendarQuarter
ORDER BY dates.CalendarYear, dates.CalendarQuarter;


The results from this query would look similar to the following table:

Expand table
CalendarYear	CalendarQuarter	TotalSales
2020	1	25980.16
2020	2	27453.87
2020	3	28527.15
2020	4	31083.45
2021	1	34562.96
2021	2	36162.27
...	...	...

You can join as many dimension tables as needed to calculate the aggregations you need. For example, the following code extends the previous example to break down the quarterly sales totals by city based on the customer's address details in the DimCustomer table:

SELECT  dates.CalendarYear,
        dates.CalendarQuarter,
        custs.City,
        SUM(sales.SalesAmount) AS TotalSales
FROM dbo.FactSales AS sales
JOIN dbo.DimDate AS dates ON sales.OrderDateKey = dates.DateKey
JOIN dbo.DimCustomer AS custs ON sales.CustomerKey = custs.CustomerKey
GROUP BY dates.CalendarYear, dates.CalendarQuarter, custs.City
ORDER BY dates.CalendarYear, dates.CalendarQuarter, custs.City;


This time, the results include a quarterly sales total for each city:

Expand table
CalendarYear	CalendarQuarter	City	TotalSales
2020	1	Amsterdam	5982.53
2020	1	Berlin	2826.98
2020	1	Chicago	5372.72
...	...	...	..
2020	2	Amsterdam	7163.93
2020	2	Berlin	8191.12
2020	2	Chicago	2428.72
...	...	...	..
2020	3	Amsterdam	7261.92
2020	3	Berlin	4202.65
2020	3	Chicago	2287.87
...	...	...	..
2020	4	Amsterdam	8262.73
2020	4	Berlin	5373.61
2020	4	Chicago	7726.23
...	...	...	..
2021	1	Amsterdam	7261.28
2021	1	Berlin	3648.28
2021	1	Chicago	1027.27
...	...	...	..
Joins in a snowflake schema

When using a snowflake schema, dimensions may be partially normalized; requiring multiple joins to relate fact tables to snowflake dimensions. For example, suppose your data warehouse includes a DimProduct dimension table from which the product categories have been normalized into a separate DimCategory table. A query to aggregate items sold by product category might look similar to the following example:

SELECT  cat.ProductCategory,
        SUM(sales.OrderQuantity) AS ItemsSold
FROM dbo.FactSales AS sales
JOIN dbo.DimProduct AS prod ON sales.ProductKey = prod.ProductKey
JOIN dbo.DimCategory AS cat ON prod.CategoryKey = cat.CategoryKey
GROUP BY cat.ProductCategory
ORDER BY cat.ProductCategory;


The results from this query include the number of items sold for each product category:

Expand table
ProductCategory	ItemsSold
Accessories	28271
Bits and pieces	5368
...	...

 Note

JOIN clauses for FactSales and DimProduct and for DimProduct and DimCategory are both required, even though no fields from DimProduct are returned by the query.

Using ranking functions

Another common kind of analytical query is to partition the results based on a dimension attribute and rank the results within each partition. For example, you might want to rank stores each year by their sales revenue. To accomplish this goal, you can use Transact-SQL ranking functions such as ROW_NUMBER, RANK, DENSE_RANK, and NTILE. These functions enable you to partition the data over categories, each returning a specific value that indicates the relative position of each row within the partition:

ROW_NUMBER returns the ordinal position of the row within the partition. For example, the first row is numbered 1, the second 2, and so on.
RANK returns the ranked position of each row in the ordered results. For example, in a partition of stores ordered by sales volume, the store with the highest sales volume is ranked 1. If multiple stores have the same sales volumes, they'll be ranked the same, and the rank assigned to subsequent stores reflects the number of stores that have higher sales volumes - including ties.
DENSE_RANK ranks rows in a partition the same way as RANK, but when multiple rows have the same rank, subsequent rows are ranking positions ignore ties.
NTILE returns the specified percentile in which the row falls. For example, in a partition of stores ordered by sales volume, NTILE(4) returns the quartile in which a store's sales volume places it.

For example, consider the following query:

SELECT  ProductCategory,
        ProductName,
        ListPrice,
        ROW_NUMBER() OVER
            (PARTITION BY ProductCategory ORDER BY ListPrice DESC) AS RowNumber,
        RANK() OVER
            (PARTITION BY ProductCategory ORDER BY ListPrice DESC) AS Rank,
        DENSE_RANK() OVER
            (PARTITION BY ProductCategory ORDER BY ListPrice DESC) AS DenseRank,
        NTILE(4) OVER
            (PARTITION BY ProductCategory ORDER BY ListPrice DESC) AS Quartile
FROM dbo.DimProduct
ORDER BY ProductCategory;


The query partitions products into groupings based on their categories, and within each category partition, the relative position of each product is determined based on its list price. The results from this query might look similar to the following table:

Expand table
ProductCategory	ProductName	ListPrice	RowNumber	Rank	DenseRank	Quartile
Accessories	Widget	8.99	1	1	1	1
Accessories	Knicknak	8.49	2	2	2	1
Accessories	Sprocket	5.99	3	3	3	2
Accessories	Doodah	5.99	4	3	3	2
Accessories	Spangle	2.99	5	5	4	3
Accessories	Badabing	0.25	6	6	5	4
Bits and pieces	Flimflam	7.49	1	1	1	1
Bits and pieces	Snickity wotsit	6.99	2	2	2	1
Bits and pieces	Flange	4.25	3	3	3	2
...	...	...	...	...	...	...

 Note

The sample results demonstrate the difference between RANK and DENSE_RANK. Note that in the Accessories category, the Sprocket and Doodah products have the same list price; and are both ranked as the 3rd highest priced product. The next highest priced product has a RANK of 5 (there are four products more expensive than it) and a DENSE_RANK of 4 (there are three higher prices).

To learn more about ranking functions, see Ranking Functions (Transact-SQL) in the Azure Synapse Analytics documentation.

Retrieving an approximate count

While the purpose of a data warehouse is primarily to support analytical data models and reports for the enterprise; data analysts and data scientists often need to perform some initial data exploration, just to determine the basic scale and distribution of the data.

For example, the following query uses the COUNT function to retrieve the number of sales for each year in a hypothetical data warehouse:

SELECT dates.CalendarYear AS CalendarYear,
    COUNT(DISTINCT sales.OrderNumber) AS Orders
FROM FactSales AS sales
JOIN DimDate AS dates ON sales.OrderDateKey = dates.DateKey
GROUP BY dates.CalendarYear
ORDER BY CalendarYear;


The results of this query might look similar to the following table:

Expand table
CalendarYear	Orders
2019	239870
2020	284741
2021	309272
...	...

The volume of data in a data warehouse can mean that even simple queries to count the number of records that meet specified criteria can take a considerable time to run. In many cases, a precise count isn't required - an approximate estimate will suffice. In such cases, you can use the APPROX_COUNT_DISTINCT function as shown in the following example:

SELECT dates.CalendarYear AS CalendarYear,
    APPROX_COUNT_DISTINCT(sales.OrderNumber) AS ApproxOrders
FROM FactSales AS sales
JOIN DimDate AS dates ON sales.OrderDateKey = dates.DateKey
GROUP BY dates.CalendarYear
ORDER BY CalendarYear;


The APPROX_COUNT_DISTINCT function uses a HyperLogLog algorithm to retrieve an approximate count. The result is guaranteed to have a maximum error rate of 2% with 97% probability, so the results of this query with the same hypothetical data as before might look similar to the following table:

Expand table
CalendarYear	ApproxOrders
2019	235552
2020	290436
2021	304633
...	...

The counts are less accurate, but still sufficient for an approximate comparison of yearly sales. With a large volume of data, the query using the APPROX_COUNT_DISTINCT function completes more quickly, and the reduced accuracy may be an acceptable trade-off during basic data exploration.

 Note

See the APPROX_COUNT_DISTINCT function documentation for more details.




Load data warehouse tables - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/design-multidimensional-schema-to-optimize-analytical-workloads/4-load-data
Load data warehouse tables
10 minutes

At a basic level, loading a data warehouse is typically achieved by adding new data from files in a data lake into tables in the data warehouse. The COPY statement is an effective way to accomplish this task, as shown in the following example:

COPY INTO dbo.StageProducts
    (ProductID, ProductName, ProductCategory, Color, Size, ListPrice, Discontinued)
FROM 'https://mydatalake.blob.core.windows.net/data/stagedfiles/products/*.parquet'
WITH
(
    FILE_TYPE = 'PARQUET',
    MAXERRORS = 0,
    IDENTITY_INSERT = 'OFF'
);

Considerations for designing a data warehouse load process

One of the most common patterns for loading a data warehouse is to transfer data from source systems to files in a data lake, ingest the file data into staging tables, and then use SQL statements to load the data from the staging tables into the dimension and fact tables. Usually data loading is performed as a periodic batch process in which inserts and updates to the data warehouse are coordinated to occur at a regular interval (for example, daily, weekly, or monthly).

In most cases, you should implement a data warehouse load process that performs tasks in the following order:

Ingest the new data to be loaded into a data lake, applying pre-load cleansing or transformations as required.
Load the data from files into staging tables in the relational data warehouse.
Load the dimension tables from the dimension data in the staging tables, updating existing rows or inserting new rows and generating surrogate key values as necessary.
Load the fact tables from the fact data in the staging tables, looking up the appropriate surrogate keys for related dimensions.
Perform post-load optimization by updating indexes and table distribution statistics.

After using the COPY statement to load data into staging tables, you can use a combination of INSERT, UPDATE, MERGE, and CREATE TABLE AS SELECT (CTAS) statements to load the staged data into dimension and fact tables.

 Note

Implementing an effective data warehouse loading solution requires careful consideration of how to manage surrogate keys, slowly changing dimensions, and other complexities inherent in a relational data warehouse schema. To learn more about techniques for loading a data warehouse, consider completing the Load data into a relational data warehouse module.




Create data warehouse tables - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/design-multidimensional-schema-to-optimize-analytical-workloads/3-create-tables
Create data warehouse tables
10 minutes

Now that you understand the basic architectural principles for a relational data warehouse schema, let's explore how to create a data warehouse.

Creating a dedicated SQL pool

To create a relational data warehouse in Azure Synapse Analytics, you must create a dedicated SQL Pool. The simplest way to do this in an existing Azure Synapse Analytics workspace is to use the Manage page in Azure Synapse Studio, as shown here:

When provisioning a dedicated SQL pool, you can specify the following configuration settings:

A unique name for the dedicated SQL pool.
A performance level for the SQL pool, which can range from DW100c to DW30000c and which determines the cost per hour for the pool when it's running.
Whether to start with an empty pool or restore an existing database from a backup.
The collation of the SQL pool, which determines sort order and string comparison rules for the database. (You can't change the collation after creation).

After creating a dedicated SQL pool, you can control its running state in the Manage page of Synapse Studio; pausing it when not required to prevent unnecessary costs.

When the pool is running, you can explore it on the Data page, and create SQL scripts to run in it.

Considerations for creating tables

To create tables in the dedicated SQL pool, you use the CREATE TABLE (or sometimes the CREATE EXTERNAL TABLE) Transact-SQL statement. The specific options used in the statement depend on the type of table you're creating, which can include:

Fact tables
Dimension tables
Staging tables

 Note

The data warehouse is composed of fact and dimension tables as discussed previously. Staging tables are often used as part of the data warehousing loading process to ingest data from source systems.

When designing a star schema model for small or medium sized datasets you can use your preferred database, such as Azure SQL. For larger data sets you may benefit from implementing your data warehouse in Azure Synapse Analytics instead of SQL Server. It's important to understand some key differences when creating tables in Synapse Analytics.

Data integrity constraints

Dedicated SQL pools in Synapse Analytics don't support foreign key and unique constraints as found in other relational database systems like SQL Server. This means that jobs used to load data must maintain uniqueness and referential integrity for keys, without relying on the table definitions in the database to do so.

 Tip

For more information about constraints in Azure Synapse Analytics dedicated SQL pools, see Primary key, foreign key, and unique key using dedicated SQL pool in Azure Synapse Analytics.

Indexes

While Synapse Analytics dedicated SQL pools support clustered indexes as found in SQL Server, the default index type is clustered columnstore. This index type offers a significant performance advantage when querying large quantities of data in a typical data warehouse schema and should be used where possible. However, some tables may include data types that can't be included in a clustered columnstore index (for example, VARBINARY(MAX)), in which case a clustered index can be used instead.

 Tip

For more information about indexing in Azure Synapse Analytics dedicated SQL pools, see Indexes on dedicated SQL pool tables in Azure Synapse Analytics.

Distribution

Azure Synapse Analytics dedicated SQL pools use a massively parallel processing (MPP) architecture, as opposed to the symmetric multiprocessing (SMP) architecture used in most OLTP database systems. In an MPP system, the data in a table is distributed for processing across a pool of nodes. Synapse Analytics supports the following kinds of distribution:

Hash: A deterministic hash value is calculated for the specified column and used to assign the row to a compute node.
Round-robin: Rows are distributed evenly across all compute nodes.
Replicated: A copy of the table is stored on each compute node.

The table type often determines which option to choose for distributing the table.

Expand table
Table type	Recommended distribution option
Dimension	Use replicated distribution for smaller tables to avoid data shuffling when joining to distributed fact tables. If tables are too large to store on each compute node, use hash distribution.
Fact	Use hash distribution with clustered columnstore index to distribute fact tables across compute nodes.
Staging	Use round-robin distribution for staging tables to evenly distribute data across compute nodes.

 Tip

For more information about distribution strategies for tables in Azure Synapse Analytics, see Guidance for designing distributed tables using dedicated SQL pool in Azure Synapse Analytics.

Creating dimension tables

When you create a dimension table, ensure that the table definition includes surrogate and alternate keys as well as columns for the attributes of the dimension that you want to use to group aggregations. It's often easiest to use an IDENTITY column to auto-generate an incrementing surrogate key (otherwise you need to generate unique keys every time you load data). The following example shows a CREATE TABLE statement for a hypothetical DimCustomer dimension table.

CREATE TABLE dbo.DimCustomer
(
    CustomerKey INT IDENTITY NOT NULL,
    CustomerAlternateKey NVARCHAR(15) NULL,
    CustomerName NVARCHAR(80) NOT NULL,
    EmailAddress NVARCHAR(50) NULL,
    Phone NVARCHAR(25) NULL,
    StreetAddress NVARCHAR(100),
    City NVARCHAR(20),
    PostalCode NVARCHAR(10),
    CountryRegion NVARCHAR(20)
)
WITH
(
    DISTRIBUTION = REPLICATE,
    CLUSTERED COLUMNSTORE INDEX
);


 Note

If desired, you can create a specific schema as a namespace for your tables. In this example, the default dbo schema is used.

If you intend to use a snowflake schema in which dimension tables are related to one another, you should include the key for the parent dimension in the definition of the child dimension table. For example, the following SQL code could be used to move the geographical address details from the DimCustomer table to a separate DimGeography dimension table:

CREATE TABLE dbo.DimGeography
(
    GeographyKey INT IDENTITY NOT NULL,
    GeographyAlternateKey NVARCHAR(10) NULL,
    StreetAddress NVARCHAR(100),
    City NVARCHAR(20),
    PostalCode NVARCHAR(10),
    CountryRegion NVARCHAR(20)
)
WITH
(
    DISTRIBUTION = REPLICATE,
    CLUSTERED COLUMNSTORE INDEX
);

CREATE TABLE dbo.DimCustomer
(
    CustomerKey INT IDENTITY NOT NULL,
    CustomerAlternateKey NVARCHAR(15) NULL,
    GeographyKey INT NULL,
    CustomerName NVARCHAR(80) NOT NULL,
    EmailAddress NVARCHAR(50) NULL,
    Phone NVARCHAR(25) NULL
)
WITH
(
    DISTRIBUTION = REPLICATE,
    CLUSTERED COLUMNSTORE INDEX
);

Time dimension tables

Most data warehouses include a time dimension table that enables you to aggregate data by multiple hierarchical levels of time interval. For example, the following example creates a DimDate table with attributes that relate to specific dates.

CREATE TABLE dbo.DimDate
( 
    DateKey INT NOT NULL,
    DateAltKey DATETIME NOT NULL,
    DayOfMonth INT NOT NULL,
    DayOfWeek INT NOT NULL,
    DayName NVARCHAR(15) NOT NULL,
    MonthOfYear INT NOT NULL,
    MonthName NVARCHAR(15) NOT NULL,
    CalendarQuarter INT  NOT NULL,
    CalendarYear INT NOT NULL,
    FiscalQuarter INT NOT NULL,
    FiscalYear INT NOT NULL
)
WITH
(
    DISTRIBUTION = REPLICATE,
    CLUSTERED COLUMNSTORE INDEX
);


 Tip

A common pattern when creating a dimension table for dates is to use the numeric date in DDMMYYYY or YYYYMMDD format as an integer surrogate key, and the date as a DATE or DATETIME datatype as the alternate key.

Creating fact tables

Fact tables include the keys for each dimension to which they're related, and the attributes and numeric measures for specific events or observations that you want to analyze.

The following code example creates a hypothetical fact table named FactSales that is related to multiple dimensions through key columns (date, customer, product, and store)

CREATE TABLE dbo.FactSales
(
    OrderDateKey INT NOT NULL,
    CustomerKey INT NOT NULL,
    ProductKey INT NOT NULL,
    StoreKey INT NOT NULL,
    OrderNumber NVARCHAR(10) NOT NULL,
    OrderLineItem INT NOT NULL,
    OrderQuantity SMALLINT NOT NULL,
    UnitPrice DECIMAL NOT NULL,
    Discount DECIMAL NOT NULL,
    Tax DECIMAL NOT NULL,
    SalesAmount DECIMAL NOT NULL
)
WITH
(
    DISTRIBUTION = HASH(OrderNumber),
    CLUSTERED COLUMNSTORE INDEX
);

Creating staging tables

Staging tables are used as temporary storage for data as it's being loaded into the data warehouse. A typical pattern is to structure the table to make it as efficient as possible to ingest the data from its external source (often files in a data lake) into the relational database, and then use SQL statements to load the data from the staging tables into the dimension and fact tables.

The following code example creates a staging table for product data that will ultimately be loaded into a dimension table:

CREATE TABLE dbo.StageProduct
(
    ProductID NVARCHAR(10) NOT NULL,
    ProductName NVARCHAR(200) NOT NULL,
    ProductCategory NVARCHAR(200) NOT NULL,
    Color NVARCHAR(10),
    Size NVARCHAR(10),
    ListPrice DECIMAL NOT NULL,
    Discontinued BIT NOT NULL
)
WITH
(
    DISTRIBUTION = ROUND_ROBIN,
    CLUSTERED COLUMNSTORE INDEX
);

Using external tables

In some cases, if the data to be loaded is in files with an appropriate structure, it can be more effective to create external tables that reference the file location. This way, the data can be read directly from the source files instead of being loaded into the relational store. The following example, shows how to create an external table that references files in the data lake associated with the Synapse workspace:


-- External data source links to data lake location
CREATE EXTERNAL DATA SOURCE StagedFiles
WITH (
    LOCATION = 'https://mydatalake.blob.core.windows.net/data/stagedfiles/'
);
GO

-- External format specifies file format
CREATE EXTERNAL FILE FORMAT ParquetFormat
WITH (
    FORMAT_TYPE = PARQUET,
    DATA_COMPRESSION = 'org.apache.hadoop.io.compress.SnappyCodec'
);
GO

-- External table references files in external data source
CREATE EXTERNAL TABLE dbo.ExternalStageProduct
(
    ProductID NVARCHAR(10) NOT NULL,
    ProductName NVARCHAR(200) NOT NULL,
    ProductCategory NVARCHAR(200) NOT NULL,
    Color NVARCHAR(10),
    Size NVARCHAR(10),
    ListPrice DECIMAL NOT NULL,
    Discontinued BIT NOT NULL
)
WITH
(
    DATA_SOURCE = StagedFiles,
    LOCATION = 'products/*.parquet',
    FILE_FORMAT = ParquetFormat
);
GO


 Note

For more information about using external tables, see Use external tables with Synapse SQL in the Azure Synapse Analytics documentation.




Design a data warehouse schema - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/design-multidimensional-schema-to-optimize-analytical-workloads/2-design-star-schema
Design a data warehouse schema
7 minutes

Like all relational databases, a data warehouse contains tables in which the data you want to analyze is stored. Most commonly, these tables are organized in a schema that is optimized for multidimensional modeling, in which numerical measures associated with events known as facts can be aggregated by the attributes of associated entities across multiple dimensions. For example, measures associated with a sales order (such as the amount paid or the quantity of items ordered) can be aggregated by attributes of the date on which the sale occurred, the customer, the store, and so on.

Tables in a data warehouse

A common pattern for relational data warehouses is to define a schema that includes two kinds of table: dimension tables and fact tables.

Dimension tables

Dimension tables describe business entities, such as products, people, places, and dates. Dimension tables contain columns for attributes of an entity. For example, a customer entity might have a first name, a last name, an email address, and a postal address (which might consist of a street address, a city, a postal code, and a country or region). In addition to attribute columns, a dimension table contains a unique key column that uniquely identifies each row in the table. In fact, it's common for a dimension table to include two key columns:

a surrogate key that is specific to the data warehouse and uniquely identifies each row in the dimension table in the data warehouse - usually an incrementing integer number.
An alternate key, often a natural or business key that is used to identify a specific instance of an entity in the transactional source system from which the entity record originated - such as a product code or a customer ID.

 Note

Why have two keys? There are a few good reasons:

The data warehouse may be populated with data from multiple source systems, which can lead to the risk of duplicate or incompatible business keys.
Simple numeric keys generally perform better in queries that join lots of tables - a common pattern in data warehouses.
Attributes of entities may change over time - for example, a customer might change their address. Since the data warehouse is used to support historic reporting, you may want to retain a record for each instance of an entity at multiple points in time; so that, for example, sales orders for a specific customer are counted for the city where they lived at the time the order was placed. In this case, multiple customer records would have the same business key associated with the customer, but different surrogate keys for each discrete address where the customer lived at various times.

An example of a dimension table for customer might contain the following data:

Expand table
CustomerKey	CustomerAltKey	Name	Email	Street	City	PostalCode	CountryRegion
123	I-543	Navin Jones	navin1@contoso.com	1 Main St.	Seattle	90000	United States
124	R-589	Mary Smith	mary2@contoso.com	234 190th Ave	Buffalo	50001	United States
125	I-321	Antoine Dubois	antoine1@contoso.com	2 Rue Jolie	Paris	20098	France
126	I-543	Navin Jones	navin1@contoso.com	24 125th Ave.	New York	50000	United States
...	...	...	...	...	...	...	...

 Note

Observe that the table contains two records for Navin Jones. Both records use the same alternate key to identify this person (I-543), but each record has a different surrogate key. From this, you can surmise that the customer moved from Seattle to New York. Sales made to the customer while living in Seattle are associated with the key 123, while purchases made after moving to New York are recorded against record 126.

In addition to dimension tables that represent business entities, it's common for a data warehouse to include a dimension table that represents time. This table enables data analysts to aggregate data over temporal intervals. Depending on the type of data you need to analyze, the lowest granularity (referred to as the grain) of a time dimension could represent times (to the hour, second, millisecond, nanosecond, or even lower), or dates.

An example of a time dimension table with a grain at the date level might contain the following data:

Expand table
DateKey	DateAltKey	DayOfWeek	DayOfMonth	Weekday	Month	MonthName	Quarter	Year
19990101	01-01-1999	6	1	Friday	1	January	1	1999
...	...	...	...	...	...	...	...	...
20220101	01-01-2022	7	1	Saturday	1	January	1	2022
20220102	02-01-2022	1	2	Sunday	1	January	1	2022
...	...	...	...	...	...	...	...	...
20301231	31-12-2030	3	31	Tuesday	12	December	4	2030

The timespan covered by the records in the table must include the earliest and latest points in time for any associated events recorded in a related fact table. Usually there's a record for every interval at the appropriate grain in between.

Fact tables

Fact tables store details of observations or events; for example, sales orders, stock balances, exchange rates, or recorded temperatures. A fact table contains columns for numeric values that can be aggregated by dimensions. In addition to the numeric columns, a fact table contains key columns that reference unique keys in related dimension tables.

For example, a fact table containing details of sales orders might contain the following data:

Expand table
OrderDateKey	CustomerKey	StoreKey	ProductKey	OrderNo	LineItemNo	Quantity	UnitPrice	Tax	ItemTotal	
20220101	123	5	701	1001	1	2	2.50	0.50	5.50	
20220101	123	5	765	1001	2	1	2.00	0.20	2.20	
20220102	125	2	723	1002	1	1	4.99	0.49	5.48	
20220103	126	1	823	1003	1	1	7.99	0.80	8.79	
...	...	...	...	...	...	...	...		...	...

A fact table's dimension key columns determine its grain. For example, the sales orders fact table includes keys for dates, customers, stores, and products. An order might include multiple products, so the grain represents line items for individual products sold in stores to customers on specific days.

Data warehouse schema designs

In most transactional databases that are used in business applications, the data is normalized to reduce duplication. In a data warehouse however, the dimension data is generally de-normalized to reduce the number of joins required to query the data.

Often, a data warehouse is organized as a star schema, in which a fact table is directly related to the dimension tables, as shown in this example:

]

The attributes of an entity can be used to aggregate measures in fact tables over multiple hierarchical levels - for example, to find total sales revenue by country or region, city, postal code, or individual customer. The attributes for each level can be stored in the same dimension table. However, when an entity has a large number of hierarchical attribute levels, or when some attributes can be shared by multiple dimensions (for example, both customers and stores have a geographical address), it can make sense to apply some normalization to the dimension tables and create a snowflake schema, as shown in the following example:

In this case, the DimProduct table has been normalized to create separate dimension tables for product categories and suppliers, and a DimGeography table has been added to represent geographical attributes for both customers and stores. Each row in the DimProduct table contains key values for the corresponding rows in the DimCategory and DimSupplier tables; and each row in the DimCustomer and DimStore tables contains a key value for the corresponding row in the DimGeography table.




Introduction - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/design-multidimensional-schema-to-optimize-analytical-workloads/1-introduction
Introduction
1 minute

Relational data warehouses are at the center of most enterprise business intelligence (BI) solutions. While the specific details may vary across data warehouse implementations, a common pattern based on a denormalized, multidimensional schema has emerged as the standard design for a relational data warehouse.

Azure Synapse Analytics includes a highly scalable relational database engine that is optimized for data warehousing workloads. By using dedicated SQL pools in Azure Synapse Analytics, you can create databases that are capable of hosting and querying huge volumes of data in relational tables.

In this module, you'll learn how to:

Design a schema for a relational data warehouse.
Create fact, dimension, and staging tables.
Use SQL to load data into data warehouse tables.
Use SQL to query relational data warehouse tables.




Analyze data in a relational data warehouse - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/design-multidimensional-schema-to-optimize-analytical-workloads/

Analyze data in a relational data warehouse
Module
8 Units
Feedback
Intermediate
Data Analyst
Data Engineer
Azure Synapse Analytics

Relational data warehouses are a core element of most enterprise Business Intelligence (BI) solutions, and are used as the basis for data models, reports, and analysis.

Learning objectives

In this module, you'll learn how to:

Design a schema for a relational data warehouse.
Create fact, dimension, and staging tables.
Use SQL to load data into data warehouse tables.
Use SQL to query relational data warehouse tables.
Add
Prerequisites

Before taking this module, you should have:

An understanding of data fundamentals.
Experience of querying data with Transact-SQL.
Introduction
min
Design a data warehouse schema
min
Create data warehouse tables
min
Load data warehouse tables
min
Query a data warehouse
min
Exercise - Explore a data warehouse
min
Knowledge check
min
Summary
min


Summary - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/understand-big-data-engineering-with-apache-spark-azure-synapse-analytics/8-summary
Summary
1 minute

Apache Spark is a key technology used in big data analytics, and the Spark pool support in Azure Synapse Analytics enables you to combine big data processing in Spark with large-scale data warehousing in SQL.

In this module, you learned how to:

Identify core features and capabilities of Apache Spark.
Configure a Spark pool in Azure Synapse Analytics.
Run code to load, analyze, and visualize data in a Spark notebook.




Knowledge check - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/understand-big-data-engineering-with-apache-spark-azure-synapse-analytics/7-knowledge-check
Knowledge check
3 minutes
Check your knowledge
1. 

Which definition best describes Apache Spark?

 

A highly scalable relational database management system.

A virtual server with a Python runtime.

A distributed platform for parallel data processing using multiple languages.

2. 

You need to use Spark to analyze data in a parquet file. What should you do?

 

Load the parquet file into a dataframe.

Import the data into a table in a serverless SQL pool.

Convert the data to CSV format.

3. 

You want to write code in a notebook cell that uses a SQL query to retrieve data from a view in the Spark catalog. Which magic should you use?

 

%%spark

%%pyspark

%%sql

Check your answers




Exercise - Analyze data with Spark - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/understand-big-data-engineering-with-apache-spark-azure-synapse-analytics/6-exercise-spark
Exercise - Analyze data with Spark
45 minutes

Now it's your opportunity to use a Spark pool in Azure Synapse Analytics. In this exercise, you'll use a provided script to provision an Azure Synapse Analytics workspace in your Azure subscription; and then use a Spark pool to analyze and visualize data from files in a data lake.

 Note

To complete this lab, you will need an Azure subscription in which you have administrative access.

Launch the exercise and follow the instructions.




Visualize data with Spark - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/understand-big-data-engineering-with-apache-spark-azure-synapse-analytics/5-visualize-data
Visualize data with Spark
5 minutes

One of the most intuitive ways to analyze the results of data queries is to visualize them as charts. Notebooks in Azure Synapse Analytics provide some basic charting capabilities in the user interface, and when that functionality doesn't provide what you need, you can use one of the many Python graphics libraries to create and display data visualizations in the notebook.

Using built-in notebook charts

When you display a dataframe or run a SQL query in a Spark notebook in Azure Synapse Analytics, the results are displayed under the code cell. By default, results are rendered as a table, but you can also change the results view to a chart and use the chart properties to customize how the chart visualizes the data, as shown here:

The built-in charting functionality in notebooks is useful when you're working with results of a query that don't include any existing groupings or aggregations, and you want to quickly summarize the data visually. When you want to have more control over how the data is formatted, or to display values that you have already aggregated in a query, you should consider using a graphics package to create your own visualizations.

Using graphics packages in code

There are many graphics packages that you can use to create data visualizations in code. In particular, Python supports a large selection of packages; most of them built on the base Matplotlib library. The output from a graphics library can be rendered in a notebook, making it easy to combine code to ingest and manipulate data with inline data visualizations and markdown cells to provide commentary.

For example, you could use the following PySpark code to aggregate data from the hypothetical products data explored previously in this module, and use Matplotlib to create a chart from the aggregated data.

from matplotlib import pyplot as plt

# Get the data as a Pandas dataframe
data = spark.sql("SELECT Category, COUNT(ProductID) AS ProductCount \
                  FROM products \
                  GROUP BY Category \
                  ORDER BY Category").toPandas()

# Clear the plot area
plt.clf()

# Create a Figure
fig = plt.figure(figsize=(12,8))

# Create a bar plot of product counts by category
plt.bar(x=data['Category'], height=data['ProductCount'], color='orange')

# Customize the chart
plt.title('Product Counts by Category')
plt.xlabel('Category')
plt.ylabel('Products')
plt.grid(color='#95a5a6', linestyle='--', linewidth=2, axis='y', alpha=0.7)
plt.xticks(rotation=70)

# Show the plot area
plt.show()


The Matplotlib library requires data to be in a Pandas dataframe rather than a Spark dataframe, so the toPandas method is used to convert it. The code then creates a figure with a specified size and plots a bar chart with some custom property configuration before showing the resulting plot.

The chart produced by the code would look similar to the following image:

You can use the Matplotlib library to create many kinds of chart; or if preferred, you can use other libraries such as Seaborn to create highly customized charts.




Analyze data with Spark - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/understand-big-data-engineering-with-apache-spark-azure-synapse-analytics/4-write-spark-code
Analyze data with Spark
5 minutes

One of the benefits of using Spark is that you can write and run code in various programming languages, enabling you to use the programming skills you already have and to use the most appropriate language for a given task. The default language in a new Azure Synapse Analytics Spark notebook is PySpark - a Spark-optimized version of Python, which is commonly used by data scientists and analysts due to its strong support for data manipulation and visualization. Additionally, you can use languages such as Scala (a Java-derived language that can be used interactively) and SQL (a variant of the commonly used SQL language included in the Spark SQL library to work with relational data structures). Software engineers can also create compiled solutions that run on Spark using frameworks such as Java and Microsoft .NET.

Exploring data with dataframes

Natively, Spark uses a data structure called a resilient distributed dataset (RDD); but while you can write code that works directly with RDDs, the most commonly used data structure for working with structured data in Spark is the dataframe, which is provided as part of the Spark SQL library. Dataframes in Spark are similar to those in the ubiquitous Pandas Python library, but optimized to work in Spark's distributed processing environment.

 Note

In addition to the Dataframe API, Spark SQL provides a strongly-typed Dataset API that is supported in Java and Scala. We'll focus on the Dataframe API in this module.

Loading data into a dataframe

Let's explore a hypothetical example to see how you can use a dataframe to work with data. Suppose you have the following data in a comma-delimited text file named products.csv in the primary storage account for an Azure Synapse Analytics workspace:

ProductID,ProductName,Category,ListPrice
771,"Mountain-100 Silver, 38",Mountain Bikes,3399.9900
772,"Mountain-100 Silver, 42",Mountain Bikes,3399.9900
773,"Mountain-100 Silver, 44",Mountain Bikes,3399.9900
...


In a Spark notebook, you could use the following PySpark code to load the data into a dataframe and display the first 10 rows:

%%pyspark
df = spark.read.load('abfss://container@store.dfs.core.windows.net/products.csv',
    format='csv',
    header=True
)
display(df.limit(10))


The %%pyspark line at the beginning is called a magic, and tells Spark that the language used in this cell is PySpark. You can select the language you want to use as a default in the toolbar of the Notebook interface, and then use a magic to override that choice for a specific cell. For example, here's the equivalent Scala code for the products data example:

%%spark
val df = spark.read.format("csv").option("header", "true").load("abfss://container@store.dfs.core.windows.net/products.csv")
display(df.limit(10))


The magic %%spark is used to specify Scala.

Both of these code samples would produce output like this:

Expand table
ProductID	ProductName	Category	ListPrice
771	Mountain-100 Silver, 38	Mountain Bikes	3399.9900
772	Mountain-100 Silver, 42	Mountain Bikes	3399.9900
773	Mountain-100 Silver, 44	Mountain Bikes	3399.9900
...	...	...	...
Specifying a dataframe schema

In the previous example, the first row of the CSV file contained the column names, and Spark was able to infer the data type of each column from the data it contains. You can also specify an explicit schema for the data, which is useful when the column names aren't included in the data file, like this CSV example:

771,"Mountain-100 Silver, 38",Mountain Bikes,3399.9900
772,"Mountain-100 Silver, 42",Mountain Bikes,3399.9900
773,"Mountain-100 Silver, 44",Mountain Bikes,3399.9900
...


The following PySpark example shows how to specify a schema for the dataframe to be loaded from a file named product-data.csv in this format:

from pyspark.sql.types import *
from pyspark.sql.functions import *

productSchema = StructType([
    StructField("ProductID", IntegerType()),
    StructField("ProductName", StringType()),
    StructField("Category", StringType()),
    StructField("ListPrice", FloatType())
    ])

df = spark.read.load('abfss://container@store.dfs.core.windows.net/product-data.csv',
    format='csv',
    schema=productSchema,
    header=False)
display(df.limit(10))


The results would once again be similar to:

Expand table
ProductID	ProductName	Category	ListPrice
771	Mountain-100 Silver, 38	Mountain Bikes	3399.9900
772	Mountain-100 Silver, 42	Mountain Bikes	3399.9900
773	Mountain-100 Silver, 44	Mountain Bikes	3399.9900
...	...	...	...
Filtering and grouping dataframes

You can use the methods of the Dataframe class to filter, sort, group, and otherwise manipulate the data it contains. For example, the following code example uses the select method to retrieve the ProductName and ListPrice columns from the df dataframe containing product data in the previous example:

pricelist_df = df.select("ProductID", "ListPrice")


The results from this code example would look something like this:

Expand table
ProductID	ListPrice
771	3399.9900
772	3399.9900
773	3399.9900
...	...

In common with most data manipulation methods, select returns a new dataframe object.

 Tip

Selecting a subset of columns from a dataframe is a common operation, which can also be achieved by using the following shorter syntax:

pricelist_df = df["ProductID", "ListPrice"]

You can "chain" methods together to perform a series of manipulations that results in a transformed dataframe. For example, this example code chains the select and where methods to create a new dataframe containing the ProductName and ListPrice columns for products with a category of Mountain Bikes or Road Bikes:

bikes_df = df.select("ProductName", "ListPrice").where((df["Category"]=="Mountain Bikes") | (df["Category"]=="Road Bikes"))
display(bikes_df)


The results from this code example would look something like this:

Expand table
ProductName	ListPrice
Mountain-100 Silver, 38	3399.9900
Road-750 Black, 52	539.9900
...	...

To group and aggregate data, you can use the groupBy method and aggregate functions. For example, the following PySpark code counts the number of products for each category:

counts_df = df.select("ProductID", "Category").groupBy("Category").count()
display(counts_df)


The results from this code example would look something like this:

Expand table
Category	count
Headsets	3
Wheels	14
Mountain Bikes	32
...	...
Using SQL expressions in Spark

The Dataframe API is part of a Spark library named Spark SQL, which enables data analysts to use SQL expressions to query and manipulate data.

Creating database objects in the Spark catalog

The Spark catalog is a metastore for relational data objects such as views and tables. The Spark runtime can use the catalog to seamlessly integrate code written in any Spark-supported language with SQL expressions that may be more natural to some data analysts or developers.

One of the simplest ways to make data in a dataframe available for querying in the Spark catalog is to create a temporary view, as shown in the following code example:

df.createOrReplaceTempView("products")


A view is temporary, meaning that it's automatically deleted at the end of the current session. You can also create tables that are persisted in the catalog to define a database that can be queried using Spark SQL.

 Note

We won't explore Spark catalog tables in depth in this module, but it's worth taking the time to highlight a few key points:

You can create an empty table by using the spark.catalog.createTable method. Tables are metadata structures that store their underlying data in the storage location associated with the catalog. Deleting a table also deletes its underlying data.
You can save a dataframe as a table by using its saveAsTable method.
You can create an external table by using the spark.catalog.createExternalTable method. External tables define metadata in the catalog but get their underlying data from an external storage location; typically a folder in a data lake. Deleting an external table does not delete the underlying data.
Using the Spark SQL API to query data

You can use the Spark SQL API in code written in any language to query data in the catalog. For example, the following PySpark code uses a SQL query to return data from the products view as a dataframe.

bikes_df = spark.sql("SELECT ProductID, ProductName, ListPrice \
                      FROM products \
                      WHERE Category IN ('Mountain Bikes', 'Road Bikes')")
display(bikes_df)


The results from the code example would look similar to the following table:

Expand table
ProductID	ProductName	ListPrice
38	Mountain-100 Silver, 38	3399.9900
52	Road-750 Black, 52	539.9900
...	...	...
Using SQL code

The previous example demonstrated how to use the Spark SQL API to embed SQL expressions in Spark code. In a notebook, you can also use the %%sql magic to run SQL code that queries objects in the catalog, like this:

%%sql

SELECT Category, COUNT(ProductID) AS ProductCount
FROM products
GROUP BY Category
ORDER BY Category


The SQL code example returns a resultset that is automatically displayed in the notebook as a table, like the one below:

Expand table
Category	ProductCount
Bib-Shorts	3
Bike Racks	1
Bike Stands	1
...	...




Use Spark in Azure Synapse Analytics - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/understand-big-data-engineering-with-apache-spark-azure-synapse-analytics/3-use-spark
Use Spark in Azure Synapse Analytics
3 minutes

You can run many different kinds of application on Spark, including code in Python or Scala scripts, Java code compiled as a Java Archive (JAR), and others. Spark is commonly used in two kinds of workload:

Batch or stream processing jobs to ingest, clean, and transform data - often running as part of an automated pipeline.
Interactive analytics sessions to explore, analyze, and visualize data.
Running Spark code in notebooks

Azure Synapse Studio includes an integrated notebook interface for working with Spark. Notebooks provide an intuitive way to combine code with Markdown notes, commonly used by data scientists and data analysts. The look and feel of the integrated notebook experience within Azure Synapse Studio is similar to that of Jupyter notebooks - a popular open source notebook platform.

 Note

While usually used interactively, notebooks can be included in automated pipelines and run as an unattended script.

Notebooks consist of one or more cells, each containing either code or markdown. Code cells in notebooks have some features that can help you be more productive, including:

Syntax highlighting and error support.
Code auto-completion​.
Interactive data visualizations.
The ability to export results.

 Tip

To learn more about working with notebooks in Azure Synapse Analytics, see the Create, develop, and maintain Synapse notebooks in Azure Synapse Analytics article in the Azure Synapse Analytics documentation.

Accessing data from a Synapse Spark pool

You can use Spark in Azure Synapse Analytics to work with data from various sources, including:

A data lake based on the primary storage account for the Azure Synapse Analytics workspace.
A data lake based on storage defined as a linked service in the workspace.
A dedicated or serverless SQL pool in the workspace.
An Azure SQL or SQL Server database (using the Spark connector for SQL Server)
An Azure Cosmos DB analytical database defined as a linked service and configured using Azure Synapse Link for Cosmos DB.
An Azure Data Explorer Kusto database defined as a linked service in the workspace.
An external Hive metastore defined as a linked service in the workspace.

One of the most common uses of Spark is to work with data in a data lake, where you can read and write files in multiple commonly used formats, including delimited text, Parquet, Avro, and others.




Get to know Apache Spark - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/understand-big-data-engineering-with-apache-spark-azure-synapse-analytics/2-get-to-know-spark
Get to know Apache Spark
3 minutes

Apache Spark is distributed data processing framework that enables large-scale data analytics by coordinating work across multiple processing nodes in a cluster.

How Spark works

Apache Spark applications run as independent sets of processes on a cluster, coordinated by the SparkContext object in your main program (called the driver program). The SparkContext connects to the cluster manager, which allocates resources across applications using an implementation of Apache Hadoop YARN. Once connected, Spark acquires executors on nodes in the cluster to run your application code.

The SparkContext runs the main function and parallel operations on the cluster nodes, and then collects the results of the operations. The nodes read and write data from and to the file system and cache transformed data in-memory as Resilient Distributed Datasets (RDDs).

The SparkContext is responsible for converting an application to a directed acyclic graph (DAG). The graph consists of individual tasks that get executed within an executor process on the nodes. Each application gets its own executor processes, which stay up for the duration of the whole application and run tasks in multiple threads.

Spark pools in Azure Synapse Analytics

In Azure Synapse Analytics, a cluster is implemented as a Spark pool, which provides a runtime for Spark operations. You can create one or more Spark pools in an Azure Synapse Analytics workspace by using the Azure portal, or in Azure Synapse Studio. When defining a Spark pool, you can specify configuration options for the pool, including:

A name for the spark pool.
The size of virtual machine (VM) used for the nodes in the pool, including the option to use hardware accelerated GPU-enabled nodes.
The number of nodes in the pool, and whether the pool size is fixed or individual nodes can be brought online dynamically to auto-scale the cluster; in which case, you can specify the minimum and maximum number of active nodes.
The version of the Spark Runtime to be used in the pool; which dictates the versions of individual components such as Python, Java, and others that get installed.

 Tip

For more information about Spark pool configuration options, see Apache Spark pool configurations in Azure Synapse Analytics in the Azure Synapse Analytics documentation.

Spark pools in an Azure Synapse Analytics Workspace are serverless - they start on-demand and stop when idle.




Introduction - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/understand-big-data-engineering-with-apache-spark-azure-synapse-analytics/1-introduction
Introduction
1 minute

Apache Spark is an open source parallel processing framework for large-scale data processing and analytics. Spark has become extremely popular in "big data" processing scenarios, and is available in multiple platform implementations; including Azure HDInsight, Azure Databricks, and Azure Synapse Analytics.

This module explores how you can use Spark in Azure Synapse Analytics to ingest, process, and analyze data from a data lake. While the core techniques and code described in this module are common to all Spark implementations, the integrated tools and ability to work with Spark in the same environment as other Synapse analytical runtimes are specific to Azure Synapse Analytics.

After completing this module, you'll be able to:

Identify core features and capabilities of Apache Spark.
Configure a Spark pool in Azure Synapse Analytics.
Run code to load, analyze, and visualize data in a Spark notebook.




Analyze data with Apache Spark in Azure Synapse Analytics - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/understand-big-data-engineering-with-apache-spark-azure-synapse-analytics/

Analyze data with Apache Spark in Azure Synapse Analytics
Module
8 Units
Feedback
Intermediate
Data Analyst
Data Engineer
Azure Synapse Analytics

Apache Spark is a core technology for large-scale data analytics. Learn how to use Spark in Azure Synapse Analytics to analyze and visualize data in a data lake.

Learning objectives

After completing this module, you will be able to:

Identify core features and capabilities of Apache Spark.
Configure a Spark pool in Azure Synapse Analytics.
Run code to load, analyze, and visualize data in a Spark notebook.
Add
Prerequisites

If you are not already familiar with Azure Synapse Analytics, consider completing the Introduction to Azure Synapse Analytics module before starting this module.

Introduction
min
Get to know Apache Spark
min
Use Spark in Azure Synapse Analytics
min
Analyze data with Spark
min
Visualize data with Spark
min
Exercise - Analyze data with Spark
min
Knowledge check
min
Summary
min


Summary - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/query-data-lake-using-azure-synapse-serverless-sql-pools/7-summary
Summary
1 minute

Serverless SQL pools enable you to easily query files in data lake. You can query various file formats CSV, JSON, Parquet, and create external database objects to provide a relational abstraction layer over the raw files.

In this module, you've learned how to:

Identify capabilities and use cases for serverless SQL pools in Azure Synapse Analytics
Query CSV, JSON, and Parquet files using a serverless SQL pool
Create external database objects in a serverless SQL pool
Learn more

To learn more about using serverless SQL pools to query files, refer to the Azure Synapse Analytics documentation.




Knowledge check - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/query-data-lake-using-azure-synapse-serverless-sql-pools/6-knowledge-check
Knowledge check
5 minutes
1. 

What function is used to read the data in files stored in a data lake?

 

FORMAT

ROWSET

OPENROWSET

2. 

What character in file path can be used to select all the file/folders that match rest of the path?

 

&

*

/

3. 

Which external database object encapsulates the connection information to a file location in a data lake store?

 

FILE FORMAT

DATA SOURCE

EXTERNAL TABLE

Check your answers




Exercise - Query files using a serverless SQL pool - Training | Microsoft Learn
https://learn.microsoft.com/en-us/training/modules/query-data-lake-using-azure-synapse-serverless-sql-pools/5-exercise-sql
Exercise - Query files using a serverless SQL pool
40 minutes

Now it's your opportunity to try using a serverless SQL pool for yourself. In this exercise, you'll use a provided script to provision an Azure Synapse Analytics workspace in your Azure subscription; and then use a serverless SQL pool to query data files in a data lake.

 Note

To complete this lab, you will need an Azure subscription in which you have administrative access.

Launch the exercise and follow the instructions.




